<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Untitled47</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e193f7c6-911e-4363-8b7c-db375a888eea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">FINAL</span> <span class="n">PROJECT</span><span class="p">:</span> <span class="n">Object</span> <span class="n">Detection</span> <span class="n">Using</span> <span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">Models</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">YOLO</span><span class="p">,</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">)</span>  

<span class="mi">16</span><span class="p">:</span><span class="mi">11</span><span class="p">:</span><span class="mi">2024</span>     <span class="mf">1.</span><span class="n">Balubhai</span> <span class="n">Sukani</span>  
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=66522404-a4b3-4241-9d51-b58ea3533521">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Why We Selected This Project: Object Detection Using Pretrained Models (YOLO/Faster R-CNN)</span>

<span class="mf">1.</span> <span class="n">Real</span><span class="o">-</span><span class="n">World</span> <span class="n">Relevance</span> <span class="n">of</span> <span class="n">Object</span> <span class="n">Detection</span>

<span class="n">Object</span> <span class="n">detection</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">crucial</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">computer</span> <span class="n">vision</span> <span class="k">with</span> <span class="n">extensive</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">applications</span><span class="o">.</span>
<span class="n">This</span> <span class="n">project</span> <span class="n">was</span> <span class="n">selected</span> <span class="n">to</span> <span class="n">explore</span> <span class="n">the</span> <span class="n">practical</span> <span class="n">implementation</span> <span class="n">of</span> <span class="nb">object</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">a</span> 
<span class="n">scenario</span> <span class="n">where</span> <span class="n">video</span><span class="o">-</span><span class="n">based</span> <span class="n">analysis</span> <span class="ow">is</span> <span class="n">required</span><span class="o">.</span> <span class="n">The</span> <span class="n">dataset</span> <span class="n">derived</span> <span class="kn">from</span> <span class="nn">the</span> 
<span class="n">video</span> <span class="n">file</span> <span class="p">(</span><span class="n">C</span><span class="p">:</span>\<span class="n">Users</span>\<span class="n">krna5</span>\<span class="n">Downloads</span>\<span class="mi">30952</span><span class="o">-</span><span class="mi">383991415</span><span class="n">_small</span><span class="o">.</span><span class="n">mp4</span><span class="p">)</span> <span class="n">provides</span> <span class="n">a</span> <span class="n">perfect</span> <span class="n">example</span> 
<span class="n">to</span> <span class="n">demonstrate</span> <span class="n">the</span> <span class="n">potential</span> <span class="ow">and</span> <span class="n">challenges</span> <span class="n">of</span> <span class="nb">object</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">scenarios</span><span class="o">.</span>

<span class="mf">2.</span> <span class="n">Real</span><span class="o">-</span><span class="n">World</span> <span class="n">Applications</span> <span class="n">Related</span> <span class="n">to</span> <span class="n">This</span> <span class="n">Project</span>

<span class="n">Surveillance</span> <span class="ow">and</span> <span class="n">Security</span> <span class="n">Systems</span>

<span class="c1"># Application:</span>
<span class="n">Object</span> <span class="n">detection</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">cornerstone</span> <span class="n">of</span> <span class="n">modern</span> <span class="n">surveillance</span> <span class="n">systems</span><span class="o">.</span> <span class="n">For</span> <span class="n">example</span><span class="p">:</span>
<span class="n">Detecting</span> <span class="n">unauthorized</span> <span class="n">access</span> <span class="ow">or</span> <span class="n">unusual</span> <span class="n">behavior</span> <span class="ow">in</span> <span class="n">public</span> <span class="n">spaces</span><span class="o">.</span>
<span class="n">Identifying</span> <span class="n">objects</span> <span class="n">like</span> <span class="n">bags</span> <span class="n">left</span> <span class="n">unattended</span> <span class="ow">or</span> <span class="n">hazardous</span> <span class="n">items</span> <span class="ow">in</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span><span class="o">.</span>

<span class="c1"># Relevance: </span>
<span class="n">A</span> <span class="n">video</span> <span class="n">dataset</span> <span class="n">simulates</span> <span class="n">security</span> <span class="n">camera</span> <span class="n">footage</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">an</span> <span class="n">ideal</span> <span class="n">use</span> <span class="n">case</span> <span class="k">for</span> <span class="n">testing</span> 
<span class="ow">and</span> <span class="n">deploying</span> <span class="n">models</span> <span class="n">like</span> <span class="n">YOLO</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">monitoring</span><span class="o">.</span>
<span class="n">Traffic</span> <span class="n">Monitoring</span> <span class="ow">and</span> <span class="n">Analysis</span>

<span class="n">Application</span><span class="p">:</span> <span class="n">Detecting</span> <span class="ow">and</span> <span class="n">classifying</span> <span class="n">vehicles</span><span class="p">,</span> <span class="n">pedestrians</span><span class="p">,</span> <span class="ow">or</span> <span class="n">other</span> <span class="n">road</span> <span class="n">users</span> <span class="ow">in</span> <span class="n">traffic</span> 
<span class="n">management</span> <span class="n">systems</span><span class="o">.</span><span class="n">Tracking</span> <span class="n">vehicle</span> <span class="n">counts</span> <span class="n">to</span> <span class="n">optimize</span> <span class="n">traffic</span> <span class="n">flow</span><span class="o">.</span><span class="n">Identifying</span> <span class="n">traffic</span>
<span class="n">violations</span> <span class="ow">or</span> <span class="n">accidents</span> <span class="ow">in</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span><span class="o">.</span>

<span class="c1"># Relevance:</span>
<span class="n">The</span> <span class="n">video</span> <span class="n">dataset</span> <span class="n">can</span> <span class="n">be</span> <span class="n">treated</span> <span class="k">as</span> <span class="n">a</span> <span class="n">proxy</span> <span class="k">for</span> <span class="n">traffic</span> <span class="n">scenes</span><span class="p">,</span> <span class="n">showcasing</span> <span class="n">the</span> <span class="n">potential</span> <span class="n">to</span> 
<span class="n">integrate</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">into</span> <span class="n">smart</span> <span class="n">city</span> <span class="n">solutions</span><span class="o">.</span>
<span class="n">Content</span> <span class="n">Moderation</span> <span class="ow">and</span> <span class="n">Annotation</span>

<span class="c1"># Application:</span>
<span class="n">Automating</span> <span class="n">the</span> <span class="n">tagging</span> <span class="ow">or</span> <span class="n">filtering</span> <span class="n">of</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">multimedia</span> <span class="n">content</span> <span class="k">for</span> <span class="n">platforms</span> <span class="n">like</span>
<span class="n">YouTube</span><span class="p">,</span> <span class="n">TikTok</span><span class="p">,</span> <span class="ow">or</span> <span class="n">Instagram</span><span class="o">.</span>
<span class="n">Identifying</span> <span class="n">restricted</span> <span class="n">items</span> <span class="k">for</span> <span class="n">compliance</span> <span class="n">purposes</span><span class="o">.</span>
<span class="n">Enabling</span> <span class="n">automated</span> <span class="n">video</span> <span class="n">annotation</span> <span class="k">for</span> <span class="n">datasets</span><span class="o">.</span>
                        
<span class="c1"># Relevance: </span>
<span class="n">This</span> <span class="n">project</span> <span class="n">demonstrates</span> <span class="n">the</span> <span class="n">use</span> <span class="n">of</span> <span class="n">pretrained</span> <span class="n">models</span> <span class="n">to</span> <span class="n">analyze</span> <span class="n">video</span> <span class="n">content</span> <span class="n">dynamically</span><span class="o">.</span>
<span class="n">Retail</span> <span class="n">Analytics</span>

<span class="c1"># Application: </span>
<span class="n">Object</span> <span class="n">detection</span> <span class="n">can</span> <span class="n">identify</span> <span class="n">customer</span> <span class="n">behavior</span><span class="p">,</span> <span class="n">track</span> <span class="n">movement</span> <span class="n">patterns</span><span class="p">,</span> <span class="ow">or</span> <span class="n">analyze</span> <span class="n">product</span> 
<span class="n">placements</span><span class="o">.</span>
<span class="n">Detecting</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">customers</span> <span class="n">entering</span> <span class="n">a</span> <span class="n">store</span><span class="o">.</span>
<span class="n">Monitoring</span> <span class="n">shelves</span> <span class="n">to</span> <span class="n">identify</span> <span class="n">when</span> <span class="n">products</span> <span class="n">need</span> <span class="n">restocking</span><span class="o">.</span>
<span class="c1"># Relevance: </span>
<span class="n">By</span> <span class="n">adapting</span> <span class="n">the</span> <span class="n">video</span> <span class="n">dataset</span><span class="p">,</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">models</span> <span class="n">can</span> <span class="n">be</span> <span class="n">trained</span> <span class="n">to</span> <span class="n">mimic</span> <span class="n">retail</span> <span class="n">analytics</span>
<span class="n">applications</span><span class="o">.</span>

<span class="c1"># Autonomous Navigation</span>

<span class="c1"># Application: Object detection is vital for robots and autonomous vehicles to perceive</span>
<span class="n">their</span> <span class="n">environment</span> <span class="ow">and</span> <span class="n">make</span> <span class="n">decisions</span><span class="o">.</span>
    
<span class="c1"># Avoiding obstacles.</span>
<span class="n">Detecting</span> <span class="n">pedestrians</span> <span class="ow">or</span> <span class="n">traffic</span> <span class="n">signals</span><span class="o">.</span>
<span class="n">Relevance</span><span class="p">:</span> <span class="n">The</span> <span class="n">project</span> <span class="n">explores</span> <span class="n">detecting</span> <span class="n">multiple</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">video</span> <span class="n">feed</span><span class="p">,</span> <span class="n">a</span> <span class="n">critical</span> 
<span class="n">capability</span> <span class="k">for</span> <span class="n">autonomous</span> <span class="n">systems</span><span class="o">.</span>
          
<span class="c1"># 3. Educational and Practical Objectives</span>
          
<span class="c1"># Explore State-of-the-Art Models:</span>
<span class="n">This</span> <span class="n">project</span> <span class="n">allows</span> <span class="n">hands</span><span class="o">-</span><span class="n">on</span> <span class="n">experimentation</span> <span class="k">with</span> <span class="n">cutting</span><span class="o">-</span><span class="n">edge</span> <span class="n">pretrained</span> <span class="n">models</span> <span class="n">like</span> 
<span class="n">YOLO</span> <span class="ow">or</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">,</span> <span class="n">enabling</span><span class="p">:</span>

<span class="n">An</span> <span class="n">understanding</span> <span class="n">of</span> <span class="n">how</span> <span class="n">these</span> <span class="n">models</span> <span class="n">process</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">data</span><span class="o">.</span>
    
<span class="n">Insights</span> <span class="n">into</span> <span class="n">their</span> <span class="n">strengths</span><span class="p">,</span> <span class="n">weaknesses</span><span class="p">,</span> <span class="ow">and</span> <span class="n">deployment</span> <span class="n">considerations</span><span class="o">.</span>
    
<span class="c1"># Handle Realistic Data Challenges:</span>
<span class="c1"># Working with a video dataset introduces:</span>

<span class="n">Variability</span> <span class="ow">in</span> <span class="n">lighting</span><span class="p">,</span> <span class="n">angles</span><span class="p">,</span> <span class="ow">and</span> <span class="nb">object</span> <span class="n">appearances</span><span class="o">.</span>
<span class="n">The</span> <span class="n">need</span> <span class="k">for</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">frame</span> <span class="n">extraction</span><span class="p">,</span> <span class="n">augmentation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">normalization</span><span class="o">.</span>
<span class="n">Temporal</span> <span class="n">aspects</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">closer</span> <span class="n">to</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">applications</span><span class="o">.</span>
<span class="n">Bridge</span> <span class="n">Academic</span> <span class="n">Knowledge</span> <span class="ow">and</span> <span class="n">Practical</span> <span class="n">Use</span> <span class="n">Cases</span><span class="p">:</span>
<span class="n">Applying</span> <span class="n">theoretical</span> <span class="n">concepts</span> <span class="ow">in</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">to</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">scenarios</span> <span class="n">enhances</span><span class="p">:</span>

<span class="n">Problem</span><span class="o">-</span><span class="n">solving</span> <span class="n">skills</span><span class="o">.</span>
<span class="n">Knowledge</span> <span class="n">transfer</span> <span class="n">between</span> <span class="n">academia</span> <span class="ow">and</span> <span class="n">industry</span><span class="o">.</span>
<span class="mf">4.</span> <span class="n">Benefits</span> <span class="n">of</span> <span class="n">Using</span> <span class="n">Video</span> <span class="n">Data</span> <span class="k">for</span> <span class="n">Object</span> <span class="n">Detection</span>
<span class="n">Dynamic</span> <span class="n">Scenarios</span><span class="p">:</span>

<span class="n">Video</span> <span class="n">data</span> <span class="n">provides</span> <span class="n">a</span> <span class="n">sequence</span> <span class="n">of</span> <span class="n">frames</span><span class="p">,</span> <span class="n">allowing</span> <span class="n">the</span> <span class="n">detection</span> <span class="n">of</span> <span class="n">moving</span> <span class="n">objects</span><span class="o">.</span>
<span class="n">The</span> <span class="n">dataset</span> <span class="n">enables</span> <span class="n">testing</span> <span class="n">the</span> <span class="n">robustness</span> <span class="n">of</span> <span class="n">models</span> <span class="n">against</span> <span class="n">dynamic</span> <span class="n">changes</span> <span class="ow">in</span> <span class="nb">object</span> 
<span class="n">orientation</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="ow">or</span> <span class="n">lighting</span><span class="o">.</span>
                               
<span class="c1"># Temporal Analysis:</span>

<span class="n">Video</span> <span class="n">frames</span> <span class="n">allow</span> <span class="n">the</span> <span class="n">possibility</span> <span class="n">of</span> <span class="n">tracking</span> <span class="n">objects</span> <span class="n">over</span> <span class="n">time</span><span class="o">.</span>
<span class="n">Enables</span> <span class="n">further</span> <span class="n">expansion</span> <span class="n">of</span> <span class="n">the</span> <span class="n">project</span> <span class="n">into</span> <span class="nb">object</span> <span class="n">tracking</span> <span class="n">applications</span><span class="o">.</span>
<span class="n">Large</span> <span class="n">Volume</span> <span class="n">of</span> <span class="n">Data</span><span class="p">:</span>

<span class="n">Each</span> <span class="n">frame</span> <span class="n">acts</span> <span class="k">as</span> <span class="n">a</span> <span class="n">unique</span> <span class="n">data</span> <span class="n">point</span><span class="p">,</span> <span class="n">offering</span> <span class="n">diverse</span> <span class="n">examples</span> <span class="k">for</span> <span class="n">training</span> <span class="ow">and</span> <span class="n">testing</span><span class="o">.</span>
                                                                 
<span class="mf">5.</span> <span class="n">Why</span> <span class="n">YOLO</span> <span class="k">for</span> <span class="n">This</span> Project<span class="o">?</span>
<span class="n">Real</span><span class="o">-</span><span class="n">Time</span> <span class="n">Performance</span><span class="p">:</span> <span class="n">YOLO</span> <span class="ow">is</span> <span class="n">lightweight</span> <span class="ow">and</span> <span class="n">designed</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="nb">object</span> <span class="n">detection</span><span class="p">,</span>
<span class="n">making</span> <span class="n">it</span> <span class="n">suitable</span> <span class="k">for</span> <span class="n">video</span> <span class="n">analysis</span><span class="o">.</span>
                                                                 
<span class="n">Versatility</span><span class="p">:</span> <span class="n">YOLO</span> <span class="n">can</span> <span class="n">handle</span> <span class="n">multiple</span> <span class="nb">object</span> <span class="n">categories</span> <span class="n">simultaneously</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">ideal</span> <span class="k">for</span> 
<span class="n">diverse</span> <span class="n">datasets</span> <span class="n">like</span> <span class="n">videos</span><span class="o">.</span>
                                                                 
<span class="c1"># Efficient Deployment: </span>
                                                                 
<span class="n">Its</span> <span class="n">speed</span> <span class="ow">and</span> <span class="n">efficiency</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">it</span> <span class="n">can</span> <span class="n">be</span> <span class="n">implemented</span> <span class="ow">in</span> <span class="n">resource</span><span class="o">-</span><span class="n">constrained</span>
<span class="n">environments</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">edge</span> <span class="n">devices</span> <span class="ow">or</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">systems</span><span class="o">.</span>
    
<span class="c1"># 6. Potential Impact of the Project : Improved Efficiency</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=62129d13-ba3a-4889-85a5-52b52553175b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">Creating</span> <span class="n">a</span> <span class="n">robust</span> <span class="n">project</span> <span class="n">titled</span> <span class="s2">"Object Detection Using Pre-trained Models </span>
<span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">YOLO</span><span class="p">,</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">)</span><span class="s2">" involves several structured steps. Below is a comprehensive guide:</span>

<span class="mf">1.</span> <span class="n">Defining</span> <span class="n">the</span> <span class="n">Problem</span>
<span class="n">Objective</span><span class="p">:</span> <span class="n">Clearly</span> <span class="n">state</span> <span class="n">the</span> <span class="n">problem</span> <span class="n">you</span><span class="s1">'re solving with object detection.</span>

<span class="c1"># Example: Detecting cars, pedestrians, and traffic signs in urban environments.</span>
<span class="n">Use</span> <span class="n">Case</span><span class="p">:</span> <span class="n">Define</span> <span class="n">the</span> <span class="n">practical</span> <span class="n">application</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">autonomous</span> <span class="n">vehicles</span><span class="p">,</span> <span class="n">surveillance</span><span class="p">)</span><span class="o">.</span>

<span class="mf">2.</span> <span class="n">Set</span> <span class="n">Up</span> <span class="n">the</span> <span class="n">Environment</span>
<span class="n">Install</span> <span class="n">necessary</span> <span class="n">libraries</span> <span class="ow">and</span> <span class="n">tools</span><span class="p">:</span>
<span class="n">YOLO</span><span class="p">:</span> <span class="n">ultralytics</span><span class="p">,</span> <span class="n">PyTorch</span>
<span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">:</span> <span class="n">torchvision</span> <span class="p">(</span><span class="n">part</span> <span class="n">of</span> <span class="n">PyTorch</span><span class="p">)</span>
<span class="n">General</span> <span class="n">libraries</span><span class="p">:</span> <span class="n">numpy</span><span class="p">,</span> <span class="n">opencv</span><span class="o">-</span><span class="n">python</span><span class="p">,</span> <span class="n">matplotlib</span><span class="p">,</span> <span class="n">pandas</span>
<span class="n">Install</span> <span class="n">GPU</span> <span class="n">drivers</span> <span class="ow">and</span> <span class="n">enable</span> <span class="n">CUDA</span> <span class="k">for</span> <span class="n">faster</span> <span class="n">processing</span> <span class="k">if</span> <span class="n">available</span><span class="o">.</span>
                                   
<span class="n">Recommended</span> <span class="n">tools</span><span class="p">:</span> <span class="n">Google</span> <span class="n">Colab</span><span class="p">,</span> <span class="n">Jupyter</span> <span class="n">Notebook</span><span class="p">,</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">local</span> <span class="n">setup</span> <span class="k">with</span> <span class="n">a</span> <span class="n">GPU</span><span class="o">.</span>

<span class="mf">3.</span> <span class="n">Collect</span> <span class="ow">and</span> <span class="n">Prepare</span> <span class="n">the</span> <span class="n">Dataset</span>
                                   
<span class="c1"># Dataset Selection:</span>
<span class="n">Use</span> <span class="n">a</span> <span class="n">publicly</span> <span class="n">available</span> <span class="n">dataset</span> <span class="n">like</span> <span class="n">COCO</span><span class="p">,</span> <span class="n">Pascal</span> <span class="n">VOC</span><span class="p">,</span> <span class="ow">or</span> <span class="n">custom</span> <span class="n">datasets</span><span class="o">.</span>
<span class="n">If</span> <span class="n">using</span> <span class="n">custom</span> <span class="n">data</span><span class="p">,</span> <span class="n">annotate</span> <span class="n">it</span> <span class="n">using</span> <span class="n">tools</span> <span class="n">like</span> <span class="n">LabelImg</span> <span class="ow">or</span> <span class="n">Roboflow</span><span class="o">.</span>
                                   
<span class="c1"># Preprocessing:</span>
<span class="n">Normalize</span> <span class="ow">and</span> <span class="n">resize</span> <span class="n">images</span><span class="o">.</span>
<span class="n">Split</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">into</span> <span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">test</span> <span class="n">sets</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="mi">70</span><span class="o">%-</span><span class="mi">20</span><span class="o">%-</span><span class="mi">10</span><span class="o">%</span><span class="p">)</span><span class="o">.</span>

<span class="mf">4.</span> <span class="n">Choose</span> <span class="ow">and</span> <span class="n">Understand</span> <span class="n">the</span> <span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">Model</span>
<span class="n">YOLO</span> <span class="p">(</span><span class="n">You</span> <span class="n">Only</span> <span class="n">Look</span> <span class="n">Once</span><span class="p">):</span>
                                   
<span class="n">Lightweight</span> <span class="ow">and</span> <span class="n">suitable</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="o">.</span>
<span class="n">Popular</span> <span class="n">versions</span><span class="p">:</span> <span class="n">YOLOv4</span><span class="p">,</span> <span class="n">YOLOv5</span><span class="p">,</span> <span class="n">YOLOv8</span><span class="o">.</span>
                                   
<span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">:</span>
<span class="n">Higher</span> <span class="n">accuracy</span> <span class="n">but</span> <span class="n">computationally</span> <span class="n">intensive</span><span class="o">.</span>
<span class="n">Suitable</span> <span class="k">for</span> <span class="n">applications</span> <span class="n">where</span> <span class="n">speed</span> <span class="ow">is</span> <span class="n">less</span> <span class="n">critical</span><span class="o">.</span>
<span class="n">Study</span> <span class="n">the</span> <span class="n">architecture</span> <span class="ow">and</span> <span class="n">differences</span> <span class="n">to</span> <span class="n">align</span> <span class="k">with</span> <span class="n">project</span> <span class="n">requirements</span><span class="o">.</span>

<span class="mf">5.</span><span class="n">Load</span> <span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">Weights</span>
                                   
<span class="c1"># Download pre-trained weights for the model:</span>
<span class="n">YOLO</span><span class="p">:</span> <span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">weights</span> <span class="kn">from</span> <span class="nn">Ultralytics</span> <span class="ow">or</span> <span class="n">other</span> <span class="n">sources</span><span class="o">.</span>
    
<span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">:</span> <span class="n">Use</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span> <span class="n">to</span> <span class="n">load</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">models</span><span class="o">.</span>

<span class="mf">6.</span> <span class="n">Implement</span> <span class="n">the</span> <span class="n">Model</span>
<span class="n">YOLO</span><span class="p">:</span>
<span class="n">Install</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">library</span><span class="p">:</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">ultralytics</span>
<span class="n">Load</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">and</span> <span class="n">test</span> <span class="n">on</span> <span class="n">sample</span> <span class="n">images</span><span class="p">:</span>
<span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">'yolov5s.pt'</span><span class="p">)</span>  <span class="c1"># YOLOv5 small model</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s1">'sample_image.jpg'</span><span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">:</span>
<span class="n">Use</span> <span class="n">PyTorch</span> <span class="n">to</span> <span class="n">load</span> <span class="ow">and</span> <span class="n">test</span><span class="p">:</span>
<span class="n">Code</span>
<span class="kn">import</span> <span class="nn">torchvisionfrom</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">detection</span> <span class="kn">import</span> <span class="nn">fasterrcnn_resnet50_fpn</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">fasterrcnn_resnet50_fpn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="mf">7.</span> <span class="n">Train</span> <span class="n">the</span> <span class="n">Model</span> <span class="n">on</span> <span class="n">Custom</span> <span class="n">Data</span>
<span class="n">Fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">model</span> <span class="n">using</span> <span class="n">your</span> <span class="n">dataset</span><span class="p">:</span>
<span class="n">Prepare</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">required</span> <span class="nb">format</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">YOLO</span> <span class="nb">format</span><span class="p">:</span> <span class="o">.</span><span class="n">txt</span> <span class="n">files</span> <span class="k">for</span> <span class="n">annotations</span><span class="p">)</span><span class="o">.</span>
<span class="n">For</span> <span class="n">YOLO</span><span class="p">:</span>
<span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">data</span> <span class="n">custom</span><span class="o">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">cfg</span> <span class="n">yolov5s</span><span class="o">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">weights</span> <span class="n">yolov5s</span><span class="o">.</span><span class="n">pt</span>

<span class="n">For</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">:</span> <span class="n">Use</span> <span class="n">PyTorch</span><span class="s1">'s DataLoader and train the model with custom annotations.</span>

<span class="mf">8.</span> <span class="n">Evaluate</span> <span class="n">the</span> <span class="n">Model</span>
<span class="n">Use</span> <span class="n">metrics</span> <span class="n">like</span><span class="p">:</span>
<span class="n">mAP</span> <span class="p">(</span><span class="n">Mean</span> <span class="n">Average</span> <span class="n">Precision</span><span class="p">):</span> <span class="n">Measures</span> <span class="n">detection</span> <span class="n">accuracy</span><span class="o">.</span>
<span class="n">IoU</span> <span class="p">(</span><span class="n">Intersection</span> <span class="n">over</span> <span class="n">Union</span><span class="p">):</span> <span class="n">Evaluates</span> <span class="n">bounding</span> <span class="n">box</span> <span class="n">quality</span><span class="o">.</span>
<span class="n">Visualize</span> <span class="n">predictions</span> <span class="k">with</span> <span class="n">bounding</span> <span class="n">boxes</span> <span class="ow">and</span> <span class="n">labels</span> <span class="n">using</span> <span class="n">matplotlib</span> <span class="ow">or</span> <span class="n">OpenCV</span><span class="o">.</span>

<span class="mf">9.</span> <span class="n">Optimize</span> <span class="n">the</span> <span class="n">Model</span>
<span class="n">Techniques</span> <span class="n">to</span> <span class="n">improve</span> <span class="n">performance</span><span class="p">:</span>
<span class="n">Data</span> <span class="n">Augmentation</span><span class="p">:</span> <span class="n">Flip</span><span class="p">,</span> <span class="n">rotate</span><span class="p">,</span> <span class="n">crop</span><span class="p">,</span> <span class="ow">and</span> <span class="n">color</span> <span class="n">jitter</span> <span class="n">images</span><span class="o">.</span>
<span class="n">Hyperparameter</span> <span class="n">Tuning</span><span class="p">:</span> <span class="n">Experiment</span> <span class="k">with</span> <span class="n">learning</span> <span class="n">rate</span><span class="p">,</span> <span class="n">batch</span> <span class="n">size</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span>
<span class="n">Pruning</span> <span class="ow">and</span> <span class="n">Quantization</span><span class="p">:</span> <span class="n">Reduce</span> <span class="n">model</span> <span class="n">size</span> <span class="k">for</span> <span class="n">faster</span> <span class="n">inference</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d147313d-9701-431d-a731-afe0b64ddfed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 1 : DEFINING THE PROBLEM:</span>

<span class="c1"># Objective</span>
<span class="n">To</span> <span class="n">develop</span> <span class="n">a</span> <span class="n">system</span> <span class="n">capable</span> <span class="n">of</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">to</span> <span class="n">identify</span> <span class="ow">and</span> <span class="n">classify</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">a</span>
<span class="n">scene</span><span class="o">.</span> <span class="n">This</span> <span class="n">includes</span> <span class="n">detecting</span> <span class="n">multiple</span> <span class="n">objects</span> <span class="n">simultaneously</span> <span class="k">with</span> <span class="n">high</span> <span class="n">accuracy</span> <span class="ow">and</span> <span class="n">annotating</span> 
<span class="n">them</span> <span class="k">with</span> <span class="n">bounding</span> <span class="n">boxes</span><span class="p">,</span> <span class="k">class</span> <span class="nc">labels</span><span class="p">,</span> <span class="ow">and</span> <span class="n">confidence</span> <span class="n">scores</span><span class="o">.</span> <span class="n">The</span> <span class="n">system</span> <span class="n">must</span> <span class="n">be</span> <span class="n">efficient</span> <span class="n">enough</span> 
<span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="o">.</span>

<span class="c1"># Example Objective:</span>
<span class="n">Detecting</span> <span class="n">cars</span><span class="p">,</span> <span class="n">pedestrians</span><span class="p">,</span> <span class="ow">and</span> <span class="n">traffic</span> <span class="n">signs</span> <span class="ow">in</span> <span class="n">urban</span> <span class="n">environments</span><span class="o">.</span>

<span class="c1"># Use Case</span>
<span class="c1"># Practical Application: </span>
<span class="n">Enhance</span> <span class="n">the</span> <span class="n">safety</span> <span class="ow">and</span> <span class="n">functionality</span> <span class="n">of</span> <span class="n">autonomous</span> <span class="n">vehicles</span> <span class="n">by</span> <span class="n">enabling</span> <span class="n">them</span> <span class="n">to</span> <span class="n">perceive</span> <span class="n">their</span> 
<span class="n">surroundings</span> <span class="ow">and</span> <span class="n">make</span> <span class="n">decisions</span> <span class="n">based</span> <span class="n">on</span> <span class="n">detected</span> <span class="n">objects</span><span class="o">.</span>

<span class="c1"># Specific Use Case:</span>

<span class="c1"># Autonomous Vehicles: </span>
<span class="n">Detect</span> <span class="ow">and</span> <span class="n">classify</span><span class="p">:</span>

<span class="n">Cars</span><span class="p">:</span> <span class="n">To</span> <span class="n">avoid</span> <span class="n">collisions</span><span class="o">.</span>
<span class="n">Pedestrians</span><span class="p">:</span> <span class="n">To</span> <span class="n">ensure</span> <span class="n">pedestrian</span> <span class="n">safety</span><span class="o">.</span>
<span class="n">Traffic</span> <span class="n">Signs</span> <span class="ow">and</span> <span class="n">Signals</span><span class="p">:</span> <span class="n">To</span> <span class="n">obey</span> <span class="n">traffic</span> <span class="n">rules</span><span class="o">.</span>
<span class="n">Surveillance</span> <span class="ow">and</span> <span class="n">Security</span><span class="p">:</span>

<span class="n">Monitor</span> <span class="n">public</span> <span class="n">areas</span> <span class="k">for</span> <span class="n">suspicious</span> <span class="n">activities</span> <span class="ow">or</span> <span class="n">unauthorized</span> <span class="n">entries</span><span class="o">.</span>
<span class="n">Real</span><span class="o">-</span><span class="n">time</span> <span class="n">alerts</span> <span class="k">for</span> <span class="n">specific</span> <span class="nb">object</span> <span class="n">detections</span> <span class="n">like</span><span class="p">,</span><span class="n">abandoned</span> <span class="n">luggage</span><span class="o">.</span>
    
<span class="c1"># Retail Analytics:</span>

<span class="n">Monitor</span> <span class="n">customer</span> <span class="n">behavior</span><span class="o">.</span>
<span class="n">Detect</span> <span class="ow">and</span> <span class="n">count</span> <span class="n">foot</span> <span class="n">traffic</span> <span class="ow">and</span> <span class="n">items</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">store</span><span class="o">.</span>
    
<span class="c1"># Smart Cities:</span>

<span class="n">Detect</span> <span class="ow">and</span> <span class="n">manage</span> <span class="n">urban</span> <span class="n">congestion</span><span class="o">.</span>
<span class="n">Monitor</span> <span class="n">public</span> <span class="n">spaces</span> <span class="k">for</span> <span class="n">safety</span> <span class="ow">and</span> <span class="n">cleanliness</span><span class="o">.</span>
<span class="n">Success</span> <span class="n">Criteria</span>
    
<span class="c1"># Accuracy: High precision and recall rates for object detection.</span>
<span class="c1"># Speed: Process at least 30 frames per second for seamless real-time operation.</span>
<span class="c1"># Scalability: Handle various environments, including indoor and outdoor settings.</span>
<span class="c1"># Hardware Compatibility: Optimized for edge devices (low-power CPUs) and GPUs for scalability.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=58904d12-e10c-47a8-a8b9-edb06cb22d4c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 2. Setting Up the Environment for Real-Time Object Detection</span>
<span class="n">To</span> <span class="n">develop</span> <span class="n">a</span> <span class="n">system</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="nb">object</span> <span class="n">detection</span><span class="p">,</span> <span class="n">a</span> <span class="n">properly</span> <span class="n">configured</span> <span class="n">environment</span> <span class="ow">is</span> <span class="n">crucial</span><span class="o">.</span>

<span class="c1">#1: Installing Necessary Libraries and Tools</span>
<span class="n">YOLOv5</span> <span class="p">(</span><span class="n">Ultralytics</span><span class="p">):</span>

<span class="n">YOLOv5</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span><span class="p">,</span> <span class="n">fast</span><span class="p">,</span> <span class="ow">and</span> <span class="n">easy</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">use</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">model</span><span class="o">.</span>
<span class="n">Install</span> <span class="n">via</span> <span class="n">PyTorch</span><span class="err"></span><span class="n">s</span> <span class="n">Hub</span> <span class="n">API</span><span class="p">:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e301e70d-6bf5-45f4-b3c5-48d5c22f13a1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Key Components Installed:</span>
<span class="c1"># PyTorch (torch): Version 2.5.1 is installed, indicating the latest CPU-compatible version.</span>

<span class="c1"># Torchvision: Version 0.20.1 is installed, which is required for handling datasets and image transformations.</span>

<span class="c1"># Torchaudio: Installed, which is part of the PyTorch ecosystem for audio processing.</span>

<span class="c1"># Ultralytics: Version 8.3.33 is installed. This is necessary for using YOLO implementations.</span>

<span class="c1"># General Libraries:1.numpy;2.opencv-python;3.matplotlib;3.pandas;4.pyyaml;5.requests;6.tqdm;7.scipy;8.seaborn</span>
<span class="n">These</span> <span class="n">are</span> <span class="n">essential</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="nb">object</span> <span class="n">detection</span> <span class="ow">and</span> <span class="n">related</span> <span class="n">tasks</span><span class="o">.</span>

<span class="n">Dependencies</span><span class="p">:</span>

<span class="n">Tools</span> <span class="n">like</span> <span class="n">filelock</span><span class="p">,</span> <span class="n">sympy</span><span class="p">,</span> <span class="n">networkx</span><span class="p">,</span> <span class="n">jinja2</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">,</span> <span class="n">are</span> <span class="n">correctly</span> <span class="n">installed</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=b15525b4-cbd7-4549-83e0-b1839082f601">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Testing PyTorch Installation:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bbcae267-d5b8-4ed2-b919-93fce88661a5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>  <span class="c1"># Should return False for CPU installation</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>2.5.1+cpu
False
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=299f8b1a-464b-48fa-8db8-7bb355e32b58">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Testing Ultralytics YOLO Installation</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=15ed3585-94c7-492c-92bf-8b259540751c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolov8n.pt"</span><span class="p">)</span>  <span class="c1"># Load the YOLOv8 nano model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"YOLO installed successfully!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>YOLO installed successfully!
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=3c559a87-ab42-4d88-a230-51b8c440b26f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Verifying Other Libraries: Checking if important libraries are importable</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a19ea0ed-29e5-4386-afb2-6714fdc2aacf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span><span class="o">,</span> <span class="nn">cv2</span><span class="o">,</span> <span class="nn">matplotlib</span><span class="o">,</span> <span class="nn">pandas</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"All libraries are working!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>All libraries are working!
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=bc0d29f3-e0ed-459b-b370-d5aab6a4b114">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 3: Collect and Prepare the Dataset:</span>

<span class="n">The</span> <span class="n">data</span> <span class="n">preparation</span> <span class="n">stage</span> <span class="ow">is</span> <span class="n">crucial</span> <span class="k">for</span> <span class="n">training</span> <span class="n">a</span> <span class="n">high</span><span class="o">-</span><span class="n">performing</span> <span class="n">YOLO</span> <span class="n">model</span><span class="o">.</span>
<span class="n">It</span> <span class="n">involves</span> <span class="n">dataset</span> <span class="n">selection</span><span class="p">,</span> <span class="n">annotation</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="ow">and</span> <span class="n">splitting</span><span class="o">.</span>

<span class="c1">#  Dataset Selection</span>
<span class="n">You</span> <span class="n">can</span> <span class="n">either</span> <span class="n">use</span> <span class="n">a</span> <span class="n">publicly</span> <span class="n">available</span> <span class="n">dataset</span> <span class="ow">or</span> <span class="n">create</span> <span class="n">your</span> <span class="n">custom</span> <span class="n">dataset</span><span class="p">:</span>

<span class="c1"># Publicly Available Datasets:</span>

<span class="c1"># COCO (Common Objects in Context):</span>
<span class="n">Contains</span> <span class="mi">80</span> <span class="nb">object</span> <span class="n">categories</span> <span class="k">with</span> <span class="n">over</span> <span class="mi">200</span><span class="p">,</span><span class="mi">000</span> <span class="n">images</span><span class="o">.</span>
<span class="n">Suitable</span> <span class="k">for</span> <span class="n">general</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">tasks</span><span class="o">.</span>
    
<span class="c1"># Pascal VOC:</span>
<span class="n">Includes</span> <span class="mi">20</span> <span class="nb">object</span> <span class="n">categories</span> <span class="k">with</span> <span class="n">bounding</span> <span class="n">box</span> <span class="n">annotations</span><span class="o">.</span>
<span class="n">Well</span><span class="o">-</span><span class="n">known</span> <span class="k">for</span> <span class="n">benchmarking</span> <span class="ow">in</span> <span class="nb">object</span> <span class="n">detection</span><span class="o">.</span>
    
<span class="n">Open</span> <span class="n">Images</span> <span class="n">Dataset</span><span class="p">:</span>
<span class="n">A</span> <span class="n">large</span> <span class="n">dataset</span> <span class="k">with</span> <span class="n">images</span> <span class="n">annotated</span> <span class="k">with</span> <span class="n">labels</span> <span class="ow">and</span> <span class="n">bounding</span> <span class="n">boxes</span><span class="o">.</span>
<span class="n">Custom</span> <span class="n">Dataset</span><span class="p">:</span>

<span class="c1"># If the  project requires detecting domain-specific objects, use tools like:</span>
    
<span class="n">LabelImg</span><span class="p">:</span> 
<span class="n">A</span> <span class="n">simple</span> <span class="n">tool</span> <span class="n">to</span> <span class="n">annotate</span> <span class="n">images</span> <span class="k">with</span> <span class="n">bounding</span> <span class="n">boxes</span><span class="o">.</span>
<span class="n">Roboflow</span><span class="p">:</span> <span class="n">Provides</span> <span class="n">cloud</span><span class="o">-</span><span class="n">based</span> <span class="n">annotation</span> <span class="ow">and</span> <span class="n">preprocessing</span> <span class="n">capabilities</span><span class="p">,</span> <span class="k">with</span> <span class="n">options</span>
<span class="n">to</span> <span class="n">auto</span><span class="o">-</span><span class="n">generate</span> <span class="n">YOLO</span><span class="o">-</span><span class="n">compatible</span> <span class="n">datasets</span><span class="o">.</span>
    
<span class="c1">#  Custom Dataset:</span>
    
<span class="c1"># Collect Images: </span>
<span class="n">Gather</span> <span class="n">domain</span><span class="o">-</span><span class="n">specific</span> <span class="n">images</span> <span class="p">(</span><span class="kn">from</span> <span class="nn">online</span> <span class="n">sources</span><span class="p">,</span> <span class="n">company</span> <span class="n">data</span><span class="p">,</span> <span class="ow">or</span> <span class="n">captured</span> <span class="n">manually</span><span class="p">)</span><span class="o">.</span>
    
<span class="c1"># Annotate Data:</span>
    
<span class="n">Label</span> <span class="n">objects</span> <span class="n">of</span> <span class="n">interest</span> <span class="k">with</span> <span class="n">bounding</span> <span class="n">boxes</span><span class="o">.</span>
<span class="n">Save</span> <span class="n">annotations</span> <span class="ow">in</span> <span class="n">YOLO</span> <span class="nb">format</span><span class="p">:</span> <span class="n">each</span> <span class="n">line</span> <span class="n">contains</span> <span class="o">&lt;</span><span class="n">class</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">x_center</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">y_center</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">width</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">height</span><span class="o">&gt;</span><span class="p">,</span>
<span class="n">where</span> <span class="n">values</span> <span class="n">are</span> <span class="n">normalized</span> <span class="n">relative</span> <span class="n">to</span> <span class="n">image</span> <span class="n">dimensions</span><span class="mf">.3.2</span> <span class="n">Preprocessing</span>

<span class="c1"># Ensure the dataset is consistent and ready for training:</span>

<span class="c1"># Image Normalization:</span>

<span class="n">Normalize</span> <span class="n">pixel</span> <span class="n">values</span> <span class="n">to</span> <span class="n">a</span> <span class="nb">range</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="n">by</span> <span class="n">dividing</span> <span class="n">by</span> <span class="mf">255.</span>
<span class="n">Ensure</span> <span class="nb">all</span> <span class="n">images</span> <span class="n">have</span> <span class="n">the</span> <span class="n">same</span> <span class="n">color</span> <span class="n">channels</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">RGB</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ac324733-e311-4e16-949e-e558cab01b9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 4. Choosing and Understanding Pre-Trained Models for Object Detection</span>
<span class="n">In</span> <span class="n">the</span> <span class="n">context</span> <span class="n">of</span> <span class="n">your</span> <span class="n">project</span><span class="err"></span><span class="nb">object</span> <span class="n">detection</span> <span class="n">using</span> <span class="n">a</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">model</span><span class="err"></span><span class="n">it</span> <span class="ow">is</span> <span class="n">essential</span>
<span class="n">to</span> <span class="n">carefully</span> <span class="n">select</span> <span class="n">a</span> <span class="n">model</span> <span class="n">that</span> <span class="n">aligns</span> <span class="k">with</span> <span class="n">the</span> <span class="n">requirements</span> <span class="n">of</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">speed</span><span class="p">,</span> <span class="ow">and</span> 
<span class="n">computational</span> <span class="n">resources</span><span class="o">.</span> <span class="n">Below</span><span class="p">,</span> <span class="n">I</span> <span class="n">elaborate</span> <span class="n">on</span> <span class="n">YOLO</span> <span class="ow">and</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">,</span> <span class="n">comparing</span> <span class="n">their</span> 
<span class="n">suitability</span> <span class="k">for</span> <span class="n">the</span> <span class="n">task</span><span class="o">.</span>

<span class="c1"># 1. YOLO (You Only Look Once):</span>
<span class="c1"># Overview</span>
<span class="n">YOLO</span> <span class="ow">is</span> <span class="n">an</span> <span class="n">end</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">end</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">model</span> <span class="n">that</span> <span class="n">predicts</span> <span class="n">bounding</span> <span class="n">boxes</span> <span class="ow">and</span> <span class="k">class</span>
<span class="nc">probabilities</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">single</span> <span class="n">forward</span> <span class="k">pass</span><span class="o">.</span>
<span class="n">Designed</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="p">,</span> <span class="n">it</span> <span class="n">processes</span> <span class="n">an</span> <span class="n">image</span> <span class="k">as</span> <span class="n">a</span> <span class="n">whole</span><span class="p">,</span> <span class="n">ensuring</span> 
<span class="n">speed</span> <span class="ow">and</span> <span class="n">efficiency</span><span class="o">.</span>
<span class="n">Versions</span> <span class="n">like</span> <span class="n">YOLOv4</span><span class="p">,</span> <span class="n">YOLOv5</span><span class="p">,</span> <span class="ow">and</span> <span class="n">YOLOv8</span> <span class="n">bring</span> <span class="n">enhancements</span> <span class="ow">in</span> <span class="n">terms</span> <span class="n">of</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">speed</span><span class="p">,</span>
<span class="ow">and</span> <span class="n">ease</span> <span class="n">of</span> <span class="n">use</span><span class="o">.</span>

<span class="c1"># Advantages for the Project</span>
<span class="n">Real</span><span class="o">-</span><span class="n">time</span> <span class="n">Processing</span><span class="p">:</span> <span class="n">YOLO</span><span class="s1">'s high speed makes it ideal for analyzing large datasets of </span>
<span class="n">frames</span> <span class="n">extracted</span> <span class="kn">from</span> <span class="nn">videos.</span>

<span class="c1"># Pre-trained Models:</span>

<span class="n">Available</span> <span class="n">on</span> <span class="n">datasets</span> <span class="n">like</span> <span class="n">COCO</span><span class="p">,</span> <span class="n">which</span> <span class="n">ensures</span> <span class="n">robustness</span> <span class="ow">in</span> <span class="n">detecting</span> <span class="n">common</span> <span class="n">objects</span><span class="o">.</span>
    
<span class="c1"># Lightweight: </span>
<span class="n">YOLO</span> <span class="n">models</span> <span class="n">are</span> <span class="n">less</span> <span class="n">computationally</span> <span class="n">demanding</span> <span class="n">compared</span> <span class="n">to</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">,</span> <span class="n">making</span> <span class="n">them</span>
<span class="n">suitable</span> <span class="k">for</span> <span class="n">systems</span> <span class="k">with</span> <span class="n">limited</span> <span class="n">resources</span><span class="o">.</span>
    
<span class="c1"># Scalability:</span>
<span class="n">Easily</span> <span class="n">scalable</span> <span class="n">to</span> <span class="n">custom</span> <span class="n">datasets</span> <span class="n">through</span> <span class="n">fine</span><span class="o">-</span><span class="n">tuning</span><span class="o">.</span>
<span class="n">Key</span> <span class="n">Differences</span> <span class="n">Between</span> <span class="n">Versions</span>
<span class="n">YOLOv4</span><span class="p">:</span>
<span class="n">Focuses</span> <span class="n">on</span> <span class="n">efficiency</span> <span class="ow">and</span> <span class="n">accuracy</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">detection</span><span class="o">.</span>
<span class="n">Integrates</span> <span class="n">tricks</span> <span class="n">like</span> <span class="n">Mosaic</span> <span class="n">augmentation</span> <span class="ow">and</span> <span class="n">Self</span><span class="o">-</span><span class="n">Adversarial</span> <span class="n">Training</span> <span class="p">(</span><span class="n">SAT</span><span class="p">)</span><span class="o">.</span>
<span class="n">YOLOv5</span><span class="p">:</span>
<span class="n">Lightweight</span> <span class="k">with</span> <span class="n">native</span> <span class="n">PyTorch</span> <span class="n">implementation</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">easy</span> <span class="n">to</span> <span class="n">integrate</span> <span class="ow">and</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span><span class="o">.</span>
<span class="n">Comes</span> <span class="k">with</span> <span class="n">pre</span><span class="o">-</span><span class="n">built</span> <span class="n">features</span> <span class="n">like</span> <span class="n">model</span> <span class="n">conversion</span> <span class="p">(</span><span class="n">ONNX</span><span class="p">,</span> <span class="n">TensorRT</span><span class="p">)</span> <span class="ow">and</span> <span class="n">scaling</span> <span class="p">(</span><span class="n">small</span><span class="p">,</span> <span class="n">medium</span><span class="p">,</span> <span class="n">large</span><span class="p">)</span><span class="o">.</span>
<span class="n">YOLOv8</span><span class="p">:</span>
<span class="n">The</span> <span class="n">latest</span> <span class="n">version</span> <span class="k">with</span> <span class="n">enhancements</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">speed</span><span class="p">,</span> <span class="ow">and</span> <span class="n">simplicity</span><span class="o">.</span>
<span class="n">Offers</span> <span class="n">better</span> <span class="n">performance</span> <span class="ow">in</span> <span class="n">small</span><span class="o">-</span><span class="nb">object</span> <span class="n">detection</span> <span class="ow">and</span> <span class="n">dynamic</span> <span class="n">scaling</span><span class="o">.</span>
<span class="n">Suitability</span> <span class="k">for</span> <span class="n">Your</span> <span class="n">Project</span>
<span class="n">Use</span> <span class="n">Case</span><span class="p">:</span> <span class="n">Detecting</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">video</span> <span class="n">frames</span> <span class="k">with</span> <span class="n">a</span> <span class="n">focus</span> <span class="n">on</span> <span class="n">speed</span><span class="o">.</span>
<span class="n">Recommended</span> <span class="n">Version</span><span class="p">:</span> <span class="n">YOLOv5</span> <span class="ow">or</span> <span class="n">YOLOv8</span> <span class="k">for</span> <span class="n">their</span> <span class="n">ease</span> <span class="n">of</span> <span class="n">use</span> <span class="ow">and</span> <span class="n">performance</span> <span class="n">improvements</span><span class="o">.</span>
<span class="mf">2.</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span>
<span class="n">Overview</span>
<span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">two</span><span class="o">-</span><span class="n">stage</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">model</span><span class="o">.</span>
<span class="n">In</span> <span class="n">the</span> <span class="n">first</span> <span class="n">stage</span><span class="p">,</span> <span class="n">it</span> <span class="n">identifies</span> <span class="n">regions</span> <span class="n">of</span> <span class="n">interest</span> <span class="p">(</span><span class="n">RoIs</span><span class="p">)</span><span class="o">.</span>
<span class="n">In</span> <span class="n">the</span> <span class="n">second</span> <span class="n">stage</span><span class="p">,</span> <span class="n">it</span> <span class="n">classifies</span> <span class="n">these</span> <span class="n">regions</span> <span class="ow">and</span> <span class="n">refines</span> <span class="n">bounding</span> <span class="n">boxes</span><span class="o">.</span>
<span class="n">Known</span> <span class="k">for</span> <span class="n">its</span> <span class="n">high</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">better</span> <span class="n">suited</span> <span class="k">for</span> <span class="n">applications</span> <span class="n">where</span> <span class="n">precision</span> <span class="ow">is</span> <span class="n">critical</span><span class="o">.</span>
<span class="n">Advantages</span> <span class="k">for</span> <span class="n">Your</span> <span class="n">Project</span>
<span class="n">Higher</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="n">Delivers</span> <span class="n">more</span> <span class="n">precise</span> <span class="n">bounding</span> <span class="n">boxes</span> <span class="ow">and</span> <span class="nb">object</span> <span class="n">classification</span><span class="o">.</span>
<span class="n">Robust</span> <span class="k">for</span> <span class="n">Complex</span> <span class="n">Scenarios</span><span class="p">:</span> <span class="n">Handles</span> <span class="n">overlapping</span> <span class="n">objects</span> <span class="ow">or</span> <span class="n">scenes</span> <span class="k">with</span> <span class="n">clutter</span> <span class="n">effectively</span><span class="o">.</span>
<span class="n">Customizability</span><span class="p">:</span> <span class="n">Suitable</span> <span class="k">for</span> <span class="n">fine</span><span class="o">-</span><span class="n">tuning</span> <span class="n">on</span> <span class="n">custom</span> <span class="n">datasets</span> <span class="n">where</span> <span class="n">accuracy</span> <span class="ow">is</span> <span class="n">prioritized</span> <span class="n">over</span> <span class="n">speed</span><span class="o">.</span>
<span class="n">Disadvantages</span>
<span class="n">Computational</span> <span class="n">Overhead</span><span class="p">:</span> <span class="n">Slower</span> <span class="n">than</span> <span class="n">YOLO</span> <span class="n">due</span> <span class="n">to</span> <span class="n">its</span> <span class="n">two</span><span class="o">-</span><span class="n">stage</span> <span class="n">approach</span><span class="o">.</span>
<span class="n">Real</span><span class="o">-</span><span class="n">Time</span> <span class="n">Limitation</span><span class="p">:</span> <span class="n">Less</span> <span class="n">suitable</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">video</span> <span class="n">processing</span><span class="o">.</span>
<span class="n">Suitability</span> <span class="k">for</span> <span class="n">Your</span> <span class="n">Project</span>
    
<span class="c1">#cUse Case:</span>
<span class="n">Focused</span> <span class="n">on</span> <span class="n">accuracy</span> <span class="n">over</span> <span class="n">speed</span><span class="p">,</span> <span class="n">especially</span> <span class="k">if</span> <span class="n">the</span> <span class="n">project</span> <span class="n">involves</span> <span class="n">detecting</span> <span class="n">small</span> <span class="ow">or</span> 
<span class="n">overlapping</span> <span class="n">objects</span> <span class="ow">in</span> <span class="nb">complex</span> <span class="n">scenes</span><span class="o">.</span>
    
<span class="c1"># Recommended Scenarios: </span>
<span class="n">Use</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="k">if</span> <span class="n">computational</span> <span class="n">resources</span> <span class="n">are</span> <span class="n">abundant</span> <span class="ow">or</span> <span class="k">if</span> <span class="n">the</span> <span class="n">task</span> <span class="n">requires</span> <span class="n">extremely</span> 
<span class="n">accurate</span> <span class="n">detection</span><span class="o">.</span>
    
<span class="c1"># Comparison Between YOLO and Faster R-CNN</span>
    
<span class="n">Feature</span>   	               <span class="n">YOLO</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">YOLOv5</span><span class="o">/</span><span class="n">Yv8</span><span class="p">)</span>	                        <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span>
<span class="n">Speed</span><span class="p">:</span>	                   <span class="n">Very</span> <span class="n">fast</span> <span class="p">(</span><span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">capability</span><span class="p">)</span>	               <span class="n">Slower</span> <span class="p">(</span><span class="ow">not</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span><span class="p">)</span>
<span class="n">Accuracy</span><span class="p">:</span>                  <span class="n">Good</span> <span class="n">but</span> <span class="n">slightly</span> <span class="n">lower</span> <span class="n">than</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span>	    <span class="n">Very</span> <span class="n">high</span><span class="p">,</span> <span class="n">especially</span> <span class="k">for</span> <span class="nb">complex</span> <span class="n">scenes</span>
<span class="n">Ease</span> <span class="n">of</span> <span class="n">Use</span><span class="p">:</span>	          <span class="n">Lightweight</span><span class="p">,</span> <span class="n">easy</span> <span class="n">to</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span>	                <span class="n">More</span> <span class="nb">complex</span> <span class="ow">and</span> <span class="n">resource</span><span class="o">-</span><span class="n">intensive</span>
<span class="n">Dataset</span>	<span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">on</span> <span class="n">COCO</span><span class="p">:</span> <span class="n">easy</span> <span class="n">to</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span>	                            <span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">on</span> <span class="n">COCO</span><span class="p">,</span> <span class="n">harder</span> <span class="n">to</span> <span class="n">train</span>
<span class="n">Resource</span> <span class="n">Requirement</span><span class="p">:</span>     <span class="n">Suitable</span> <span class="k">for</span> <span class="n">low</span><span class="o">-</span><span class="n">resource</span> <span class="n">systems</span>	                <span class="n">Requires</span> <span class="n">high</span> <span class="n">computational</span> <span class="n">power</span>
<span class="n">Applications</span><span class="p">:</span> 	         <span class="n">Real</span><span class="o">-</span><span class="n">time</span> <span class="n">video</span><span class="p">,</span> <span class="n">low</span><span class="o">-</span><span class="n">latency</span> <span class="n">systems</span>	             <span class="n">High</span><span class="o">-</span><span class="n">precision</span> <span class="n">tasks</span>
    
<span class="c1"># Recommendations for Your Project</span>
    
<span class="n">If</span> <span class="n">You</span> <span class="n">Need</span> <span class="n">Speed</span> <span class="ow">and</span> <span class="n">Real</span><span class="o">-</span><span class="n">Time</span> <span class="n">Detection</span><span class="p">:</span>
<span class="n">Use</span> <span class="n">YOLOv5</span> <span class="ow">or</span> <span class="n">YOLOv8</span><span class="o">.</span>
<span class="n">Ideal</span> <span class="k">for</span> <span class="n">video</span> <span class="n">frame</span> <span class="n">processing</span> <span class="ow">and</span> <span class="nb">object</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">lightweight</span> <span class="n">system</span><span class="o">.</span>
    
<span class="n">If</span> <span class="n">Accuracy</span> <span class="n">Is</span> <span class="n">Critical</span><span class="p">:</span>
<span class="n">Use</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="k">for</span> <span class="n">scenarios</span> <span class="n">where</span> <span class="n">detecting</span> <span class="n">small</span> <span class="n">objects</span> <span class="ow">or</span> <span class="n">handling</span> <span class="nb">complex</span> <span class="n">scenes</span> 
<span class="ow">is</span> <span class="n">crucial</span><span class="o">.</span>
    
<span class="n">Steps</span> <span class="n">to</span> <span class="n">Move</span> <span class="n">Forward</span>
    
<span class="n">Select</span> <span class="n">the</span> <span class="n">Model</span><span class="p">:</span>

<span class="n">YOLOv8</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">detection</span> <span class="ow">and</span> <span class="n">speed</span><span class="o">.</span>
<span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="k">for</span> <span class="n">accuracy</span> <span class="ow">and</span> <span class="nb">complex</span> <span class="n">scenarios</span><span class="o">.</span>
    
<span class="n">Prepare</span> <span class="n">the</span> <span class="n">Dataset</span><span class="p">:</span>

<span class="n">Fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">model</span> <span class="n">using</span> <span class="n">the</span> <span class="n">frames</span> <span class="n">extracted</span> <span class="kn">from</span> <span class="nn">your</span> <span class="n">video</span><span class="o">.</span>
<span class="n">Annotate</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">dataset</span> <span class="k">if</span> <span class="n">necessary</span> <span class="n">using</span> <span class="n">tools</span> <span class="n">like</span> <span class="n">LabelImg</span> <span class="ow">or</span> <span class="n">Roboflow</span><span class="o">.</span>
    
<span class="c1"># Model Training:</span>

<span class="n">Use</span> <span class="n">transfer</span> <span class="n">learning</span> <span class="n">to</span> <span class="n">leverage</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">weights</span><span class="o">.</span>
<span class="n">Experiment</span> <span class="k">with</span> <span class="n">hyperparameters</span> <span class="n">to</span> <span class="n">achieve</span> <span class="n">a</span> <span class="n">balance</span> <span class="n">between</span> <span class="n">speed</span> <span class="ow">and</span> <span class="n">accuracy</span><span class="o">.</span>
<span class="n">Evaluate</span> <span class="n">Performance</span><span class="p">:</span>

<span class="n">Test</span> <span class="n">the</span> <span class="n">model</span> <span class="n">on</span> <span class="n">the</span> <span class="n">test</span> <span class="n">dataset</span> <span class="p">(</span><span class="mi">30</span><span class="o">%</span> <span class="n">split</span><span class="p">)</span><span class="o">.</span>
<span class="n">Use</span> <span class="n">metrics</span> <span class="n">like</span> <span class="n">mAP</span> <span class="p">(</span><span class="n">mean</span> <span class="n">Average</span> <span class="n">Precision</span><span class="p">)</span> <span class="n">to</span> <span class="n">compare</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s performance.</span>
<span class="n">Deployment</span><span class="p">:</span>

<span class="n">For</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="p">,</span> <span class="n">deploy</span> <span class="n">YOLO</span> <span class="n">models</span> <span class="n">on</span> <span class="n">edge</span> <span class="n">devices</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">NVIDIA</span> <span class="n">Jetson</span><span class="p">)</span><span class="o">.</span>
<span class="n">For</span> <span class="n">desktop</span><span class="o">/</span><span class="n">server</span> <span class="n">systems</span><span class="p">,</span> <span class="n">integrate</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="k">for</span> <span class="n">detailed</span> <span class="n">analysis</span><span class="o">.</span>
<span class="n">By</span> <span class="n">aligning</span> <span class="n">the</span> <span class="n">choice</span> <span class="n">of</span> <span class="n">model</span> <span class="k">with</span> <span class="n">your</span> <span class="n">project</span> <span class="n">requirements</span><span class="p">,</span><span class="n">we</span> <span class="n">can</span> <span class="n">ensure</span> <span class="n">a</span> <span class="n">robust</span> 
<span class="ow">and</span> <span class="n">efficient</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">pipeline</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=32feaa51-8d11-409b-aa4f-6986d74b66c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Working with video data, the preprocessing, data augmentation, and dataset preparation</span>
<span class="n">steps</span> <span class="n">need</span> <span class="n">to</span> <span class="n">include</span> <span class="n">extracting</span> <span class="n">frames</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">video</span><span class="o">.</span>
<span class="c1"># YOLO with Video Input: </span>
<span class="n">Preprocessing</span><span class="p">:</span> <span class="n">Extract</span> <span class="n">Frames</span> <span class="kn">from</span> <span class="nn">Video</span> <span class="n">Extract</span> <span class="n">frames</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">video</span> <span class="k">for</span> <span class="n">annotation</span> <span class="ow">and</span> 
<span class="n">further</span> <span class="n">processing</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bec939ad-bc13-435a-a32b-a2bd8b597c57">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Set the correct video path</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\30952-383991415_small.mp4"</span>

<span class="c1"># Open the video using OpenCV</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>

<span class="c1"># Check if the video was opened successfully</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Error: Cannot open video file."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Video loaded successfully!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Video loaded successfully!
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=a2bb9ea3-d513-43be-845b-030a65cc965a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#  Extracting frames from the video and saving them in a folder:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=93220ade-ebab-4736-b780-4143136a1b50">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Set the path to save frames</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>

<span class="c1"># Create the folder if it doesn't exist</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_folder</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_folder</span><span class="p">)</span>

<span class="c1"># Open the video</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>

<span class="n">frame_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ret</span><span class="p">:</span>
        <span class="c1"># Save frame as an image file</span>
        <span class="n">frame_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"frame_</span><span class="si">{</span><span class="n">frame_count</span><span class="si">}</span><span class="s2">.jpg"</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">frame_filename</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
        <span class="n">frame_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Extracted </span><span class="si">{</span><span class="n">frame_count</span><span class="si">}</span><span class="s2"> frames to </span><span class="si">{</span><span class="n">output_folder</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Extracted 650 frames to C:\Users\krna5\Downloads\frames
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=71ca833f-f0b5-4aca-9b04-edcc9dd130b1">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># successfully extracted 650 frames from the video to the folder C:\Users\krna5\Downloads\frames. </span>
<span class="n">Now</span> <span class="n">that</span> <span class="n">we</span> <span class="n">have</span> <span class="n">the</span> <span class="n">frames</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">proceed</span> <span class="k">with</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">steps</span> <span class="n">of</span> <span class="n">your</span> <span class="n">project</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">preprocessing</span><span class="p">,</span>
<span class="n">data</span> <span class="n">augmentation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">training</span> <span class="n">the</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=38d8ef77-cfd8-43a5-986c-0754e3883422">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Resize: You may need to resize the images to a consistent size (e.g., 224x224 for use in deep learning models).</span>
<span class="c1"># Normalization: Normalize pixel values to a range of 0 to 1 (or -1 to 1 depending on the model you plan to use).</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=329f4eac-cdd4-4db2-84e6-cd3f7aafd260">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">frame_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\processed_frames"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_folder</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_folder</span><span class="p">)</span>

<span class="c1"># Resize and normalize frames</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">):</span>
    <span class="n">frame_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">frame_path</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".jpg"</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">frame_path</span><span class="p">)</span>
        <span class="n">img_resized</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>  <span class="c1"># Resize to 224x224</span>
        <span class="n">img_normalized</span> <span class="o">=</span> <span class="n">img_resized</span> <span class="o">/</span> <span class="mf">255.0</span>  <span class="c1"># Normalize to [0, 1]</span>
        <span class="n">output_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">img_normalized</span> <span class="o">*</span> <span class="mi">255</span><span class="p">)</span>  <span class="c1"># Save back as an image with normalized values</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=916ca8c2-029e-4226-937e-7aabbe3b9210">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Check the folder existence:</span>
<span class="n">Make</span> <span class="n">sure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">frames</span> <span class="n">folder</span> <span class="n">contains</span> <span class="o">.</span><span class="n">jpg</span> <span class="n">images</span> <span class="ow">and</span> <span class="n">that</span> <span class="n">the</span> <span class="n">processed_frames</span> <span class="n">folder</span> <span class="ow">is</span> <span class="n">created</span><span class="o">.</span> 
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c085b58d-a8bf-47bf-8b5f-3fd223cf32c4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">frame_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\processed_frames"</span>

<span class="c1"># Check if the 'frames' folder exists and contains files</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: The folder </span><span class="si">{</span><span class="n">frame_folder</span><span class="si">}</span><span class="s2"> does not exist."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">frame_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".jpg"</span><span class="p">)]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">frame_files</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: No image files found in </span><span class="si">{</span><span class="n">frame_folder</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">frame_files</span><span class="p">)</span><span class="si">}</span><span class="s2"> image files in </span><span class="si">{</span><span class="n">frame_folder</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
        
<span class="c1"># Check if 'processed_frames' folder exists, create it if not</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_folder</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_folder</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Found 650 image files in C:\Users\krna5\Downloads\frames.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c701535b-6d34-4d5d-a9e5-b13a829f46e6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#  Resizing and Normalizing Frames:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1f79b5dc-73cf-4a33-bdcd-02b2c33d8759">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Resize and normalize frames</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">frame_files</span><span class="p">:</span>
    <span class="n">frame_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">frame_path</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">img</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: Unable to read image </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
        <span class="k">continue</span>

    <span class="c1"># Resize to 224x224</span>
    <span class="n">img_resized</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
    
    <span class="c1"># Normalize the pixel values to [0, 1]</span>
    <span class="n">img_normalized</span> <span class="o">=</span> <span class="n">img_resized</span> <span class="o">/</span> <span class="mf">255.0</span>
    
    <span class="c1"># Save the processed image</span>
    <span class="n">output_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">img_normalized</span> <span class="o">*</span> <span class="mi">255</span><span class="p">)</span>  <span class="c1"># Saving with original pixel values</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Processed </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2"> and saved to </span><span class="si">{</span><span class="n">output_folder</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Processed frame_0.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_1.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_10.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_100.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_101.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_102.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_103.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_104.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_105.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_106.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_107.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_108.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_109.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_11.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_110.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_111.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_112.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_113.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_114.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_115.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_116.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_117.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_118.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_119.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_12.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_120.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_121.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_122.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_123.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_124.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_125.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_126.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_127.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_128.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_129.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_13.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_130.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_131.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_132.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_133.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_134.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_135.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_136.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_137.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_138.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_139.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_14.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_140.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_141.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_142.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_143.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_144.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_145.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_146.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_147.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_148.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_149.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_15.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_150.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_151.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_152.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_153.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_154.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_155.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_156.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_157.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_158.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_159.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_16.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_160.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_161.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_162.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_163.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_164.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_165.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_166.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_167.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_168.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_169.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_17.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_170.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_171.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_172.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_173.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_174.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_175.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_176.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_177.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_178.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_179.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_18.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_180.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_181.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_182.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_183.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_184.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_185.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_186.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_187.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_188.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_189.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_19.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_190.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_191.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_192.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_193.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_194.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_195.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_196.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_197.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_198.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_199.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_2.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_20.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_200.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_201.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_202.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_203.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_204.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_205.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_206.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_207.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_208.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_209.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_21.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_210.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_211.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_212.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_213.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_214.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_215.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_216.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_217.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_218.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_219.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_22.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_220.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_221.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_222.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_223.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_224.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_225.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_226.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_227.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_228.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_229.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_23.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_230.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_231.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_232.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_233.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_234.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_235.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_236.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_237.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_238.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_239.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_24.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_240.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_241.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_242.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_243.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_244.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_245.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_246.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_247.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_248.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_249.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_25.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_250.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_251.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_252.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_253.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_254.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_255.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_256.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_257.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_258.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_259.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_26.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_260.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_261.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_262.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_263.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_264.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_265.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_266.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_267.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_268.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_269.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_27.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_270.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_271.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_272.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_273.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_274.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_275.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_276.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_277.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_278.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_279.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_28.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_280.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_281.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_282.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_283.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_284.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_285.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_286.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_287.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_288.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_289.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_29.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_290.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_291.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_292.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_293.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_294.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_295.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_296.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_297.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_298.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_299.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_3.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_30.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_300.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_301.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_302.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_303.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_304.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_305.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_306.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_307.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_308.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_309.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_31.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_310.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_311.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_312.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_313.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_314.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_315.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_316.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_317.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_318.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_319.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_32.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_320.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_321.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_322.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_323.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_324.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_325.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_326.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_327.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_328.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_329.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_33.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_330.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_331.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_332.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_333.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_334.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_335.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_336.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_337.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_338.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_339.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_34.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_340.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_341.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_342.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_343.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_344.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_345.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_346.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_347.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_348.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_349.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_35.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_350.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_351.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_352.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_353.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_354.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_355.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_356.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_357.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_358.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_359.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_36.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_360.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_361.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_362.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_363.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_364.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_365.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_366.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_367.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_368.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_369.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_37.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_370.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_371.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_372.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_373.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_374.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_375.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_376.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_377.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_378.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_379.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_38.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_380.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_381.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_382.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_383.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_384.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_385.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_386.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_387.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_388.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_389.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_39.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_390.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_391.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_392.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_393.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_394.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_395.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_396.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_397.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_398.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_399.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_4.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_40.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_400.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_401.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_402.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_403.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_404.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_405.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_406.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_407.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_408.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_409.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_41.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_410.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_411.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_412.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_413.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_414.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_415.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_416.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_417.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_418.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_419.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_42.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_420.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_421.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_422.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_423.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_424.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_425.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_426.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_427.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_428.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_429.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_43.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_430.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_431.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_432.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_433.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_434.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_435.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_436.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_437.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_438.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_439.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_44.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_440.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_441.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_442.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_443.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_444.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_445.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_446.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_447.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_448.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_449.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_45.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_450.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_451.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_452.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_453.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_454.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_455.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_456.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_457.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_458.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_459.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_46.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_460.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_461.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_462.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_463.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_464.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_465.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_466.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_467.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_468.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_469.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_47.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_470.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_471.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_472.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_473.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_474.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_475.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_476.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_477.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_478.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_479.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_48.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_480.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_481.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_482.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_483.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_484.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_485.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_486.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_487.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_488.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_489.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_49.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_490.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_491.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_492.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_493.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_494.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_495.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_496.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_497.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_498.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_499.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_5.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_50.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_500.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_501.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_502.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_503.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_504.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_505.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_506.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_507.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_508.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_509.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_51.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_510.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_511.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_512.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_513.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_514.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_515.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_516.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_517.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_518.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_519.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_52.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_520.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_521.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_522.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_523.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_524.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_525.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_526.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_527.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_528.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_529.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_53.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_530.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_531.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_532.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_533.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_534.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_535.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_536.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_537.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_538.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_539.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_54.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_540.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_541.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_542.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_543.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_544.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_545.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_546.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_547.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_548.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_549.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_55.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_550.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_551.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_552.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_553.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_554.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_555.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_556.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_557.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_558.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_559.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_56.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_560.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_561.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_562.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_563.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_564.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_565.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_566.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_567.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_568.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_569.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_57.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_570.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_571.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_572.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_573.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_574.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_575.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_576.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_577.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_578.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_579.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_58.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_580.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_581.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_582.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_583.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_584.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_585.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_586.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_587.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_588.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_589.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_59.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_590.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_591.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_592.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_593.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_594.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_595.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_596.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_597.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_598.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_599.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_6.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_60.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_600.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_601.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_602.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_603.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_604.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_605.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_606.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_607.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_608.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_609.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_61.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_610.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_611.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_612.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_613.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_614.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_615.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_616.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_617.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_618.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_619.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_62.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_620.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_621.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_622.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_623.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_624.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_625.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_626.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_627.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_628.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_629.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_63.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_630.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_631.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_632.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_633.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_634.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_635.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_636.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_637.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_638.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_639.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_64.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_640.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_641.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_642.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_643.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_644.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_645.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_646.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_647.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_648.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_649.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_65.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_66.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_67.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_68.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_69.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_7.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_70.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_71.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_72.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_73.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_74.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_75.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_76.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_77.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_78.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_79.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_8.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_80.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_81.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_82.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_83.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_84.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_85.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_86.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_87.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_88.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_89.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_9.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_90.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_91.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_92.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_93.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_94.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_95.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_96.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_97.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_98.jpg and saved to C:\Users\krna5\Downloads\processed_frames
Processed frame_99.jpg and saved to C:\Users\krna5\Downloads\processed_frames
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f49bf30e-7908-47ab-b96a-b1d56fcdc060">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">Explanation</span> <span class="p">:</span>
<span class="s2">"Found 650 image files in C:\Users\krna5\Downloads</span><span class="se">\f</span><span class="s2">rames"</span><span class="p">:</span>

<span class="n">This</span> <span class="n">means</span> <span class="n">that</span> <span class="n">the</span> <span class="n">code</span> <span class="n">successfully</span> <span class="n">found</span> <span class="ow">and</span> <span class="n">read</span> <span class="nb">all</span> <span class="mi">650</span> <span class="o">.</span><span class="n">jpg</span> <span class="n">images</span> <span class="n">that</span> <span class="n">were</span>
<span class="n">extracted</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">video</span> <span class="n">frames</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">correct</span> <span class="n">number</span> <span class="n">of</span> <span class="n">frames</span> <span class="k">if</span> <span class="n">you</span> <span class="n">previously</span>
<span class="n">extracted</span> <span class="n">them</span><span class="p">,</span> <span class="n">confirming</span> <span class="n">that</span> <span class="n">your</span> <span class="n">frame</span> <span class="n">extraction</span> <span class="n">was</span> <span class="n">successful</span><span class="o">.</span>
<span class="s2">"Processed frame_96.jpg and saved to C:\Users\krna5\Downloads\processed_frames"</span> 
<span class="p">(</span><span class="ow">and</span> <span class="n">similar</span> <span class="k">for</span> <span class="n">frames</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">):</span>

<span class="n">For</span> <span class="n">each</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">frames</span> <span class="n">folder</span><span class="p">,</span> <span class="n">the</span> <span class="n">code</span> <span class="n">performed</span> <span class="n">the</span> <span class="n">following</span> <span class="n">steps</span><span class="p">:</span>
<span class="n">Resized</span> <span class="n">the</span> <span class="n">image</span> <span class="n">to</span> <span class="mi">224</span><span class="n">x224</span> <span class="n">pixels</span><span class="o">.</span>
<span class="n">Normalized</span> <span class="n">the</span> <span class="n">pixel</span> <span class="n">values</span> <span class="n">to</span> <span class="n">the</span> <span class="nb">range</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="n">by</span> <span class="n">dividing</span> <span class="n">the</span> <span class="n">pixel</span> <span class="n">values</span> <span class="n">by</span> 
<span class="mf">255.0</span> <span class="p">(</span><span class="n">this</span> <span class="ow">is</span> <span class="n">common</span> <span class="ow">in</span> <span class="n">deep</span> <span class="n">learning</span> <span class="n">to</span> <span class="n">make</span> <span class="n">data</span> <span class="n">fit</span> <span class="n">within</span> <span class="n">a</span> <span class="n">certain</span> <span class="nb">range</span> <span class="ow">and</span> 
<span class="n">improve</span> <span class="n">model</span> <span class="n">training</span><span class="p">)</span><span class="o">.</span>
<span class="n">Saved</span> <span class="n">the</span> <span class="n">processed</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">processed_frames</span> <span class="n">folder</span><span class="o">.</span>
<span class="n">The</span> <span class="n">code</span> <span class="n">processed</span> <span class="n">the</span> <span class="n">images</span> <span class="n">one</span> <span class="n">by</span> <span class="n">one</span> <span class="ow">and</span> <span class="n">saved</span> <span class="n">the</span> <span class="n">resized</span> <span class="ow">and</span> <span class="n">normalized</span> <span class="n">versions</span> <span class="n">to</span> 
<span class="n">the</span> <span class="n">output</span> <span class="n">folder</span><span class="o">.</span> <span class="n">The</span> <span class="n">printed</span> <span class="n">messages</span> <span class="n">indicate</span> <span class="n">that</span> <span class="n">the</span> <span class="n">processing</span> <span class="n">steps</span> <span class="n">were</span> <span class="n">applied</span> 
<span class="n">successfully</span> <span class="n">to</span> <span class="n">the</span> <span class="n">frames</span> <span class="ow">and</span> <span class="n">saved</span> <span class="n">to</span> <span class="n">the</span> <span class="n">designated</span> <span class="n">folder</span><span class="o">.</span>

<span class="c1"># Interpretation and Insights:</span>

<span class="c1"># Frame Preprocessing for Model Training:</span>
<span class="n">The</span> <span class="n">resizing</span> <span class="n">to</span> <span class="mi">224</span><span class="n">x224</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">standard</span> <span class="n">practice</span> <span class="k">for</span> <span class="n">models</span> <span class="n">like</span> <span class="n">Convolutional</span> <span class="n">Neural</span> <span class="n">Networks</span> 
<span class="p">(</span><span class="n">CNNs</span><span class="p">)</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">VGG</span><span class="p">,</span> <span class="n">ResNet</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">),</span> <span class="n">which</span> <span class="n">often</span> <span class="n">require</span> <span class="n">a</span> <span class="n">fixed</span> <span class="n">image</span> <span class="n">size</span> <span class="k">as</span> <span class="nb">input</span><span class="o">.</span> <span class="n">By</span> <span class="n">resizing</span>
<span class="nb">all</span> <span class="n">frames</span> <span class="n">to</span> <span class="n">the</span> <span class="n">same</span> <span class="n">size</span><span class="p">,</span> <span class="n">you</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="n">consistent</span> <span class="ow">and</span> <span class="n">ready</span> <span class="k">for</span> <span class="n">training</span><span class="o">.</span>

<span class="c1"># Normalization:</span>
<span class="n">Normalizing</span> <span class="n">the</span> <span class="n">pixel</span> <span class="n">values</span> <span class="n">to</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="n">helps</span> <span class="n">the</span> <span class="n">model</span> <span class="n">during</span> <span class="n">training</span> <span class="n">by</span> <span class="n">reducing</span> <span class="n">the</span> <span class="n">potential</span>
<span class="k">for</span> <span class="n">large</span> <span class="n">value</span> <span class="n">disparities</span> <span class="n">between</span> <span class="n">different</span> <span class="n">image</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">which</span> <span class="n">can</span> <span class="n">improve</span> <span class="n">convergence</span> <span class="n">during</span> 
<span class="n">training</span> <span class="ow">and</span> <span class="n">lead</span> <span class="n">to</span> <span class="n">better</span> <span class="n">results</span><span class="o">.</span>
    
<span class="n">Next</span> <span class="n">Steps</span><span class="p">:</span>
<span class="c1"># Data Augmentation:</span>
<span class="n">Since</span> <span class="n">you</span><span class="s1">'re preparing video data for training a model, you can apply data</span>
<span class="n">augmentation</span> <span class="n">techniques</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">rotation</span><span class="p">,</span> <span class="n">flipping</span><span class="p">,</span> <span class="n">zooming</span><span class="p">)</span> <span class="n">to</span> <span class="n">generate</span> <span class="n">more</span> <span class="n">variations</span> <span class="n">of</span> <span class="n">your</span> 
<span class="n">frames</span> <span class="ow">and</span> <span class="n">help</span> <span class="n">improve</span> <span class="n">the</span> <span class="n">robustness</span> <span class="n">of</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span>

<span class="c1"># Splitting the Data:</span>
<span class="n">Once</span> <span class="n">you</span> <span class="n">have</span> <span class="n">your</span> <span class="n">processed</span> <span class="n">frames</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">split</span> <span class="n">the</span> <span class="n">data</span> <span class="n">into</span> <span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">test</span>
<span class="n">sets</span> <span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="mi">70</span><span class="o">%-</span><span class="mi">20</span><span class="o">%-</span><span class="mi">10</span><span class="o">%</span><span class="p">)</span> <span class="n">to</span> <span class="n">evaluate</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s performance on unseen data. This split can be </span>
<span class="n">done</span> <span class="n">randomly</span><span class="p">,</span> <span class="n">but</span> <span class="n">you</span> <span class="n">might</span> <span class="n">want</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">frames</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">same</span> <span class="n">video</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">split</span> <span class="n">across</span> <span class="n">training</span>
<span class="ow">and</span> <span class="n">test</span> <span class="n">sets</span> <span class="n">to</span> <span class="n">avoid</span> <span class="n">data</span> <span class="n">leakage</span><span class="o">.</span>
<span class="c1"># Model Training: </span>
<span class="n">With</span> <span class="n">the</span> <span class="n">processed</span> <span class="ow">and</span> <span class="n">split</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">proceed</span> <span class="k">with</span> <span class="n">training</span> <span class="n">a</span> <span class="n">machine</span> <span class="n">learning</span> <span class="ow">or</span> <span class="n">deep</span>
<span class="n">learning</span> <span class="n">model</span><span class="p">,</span> <span class="n">depending</span> <span class="n">on</span> <span class="n">your</span> <span class="n">project</span> <span class="n">requirements</span><span class="o">.</span>
<span class="c1"># Summary:</span>
<span class="mi">650</span> <span class="n">frames</span> <span class="n">have</span> <span class="n">been</span> <span class="n">processed</span> <span class="n">successfully</span> <span class="ow">and</span> <span class="n">are</span> <span class="n">now</span> <span class="n">resized</span> <span class="ow">and</span> <span class="n">normalized</span><span class="p">,</span> <span class="n">ready</span> <span class="k">for</span> <span class="n">further</span>
<span class="n">steps</span> <span class="n">such</span> <span class="k">as</span> <span class="n">data</span> <span class="n">augmentation</span><span class="p">,</span> <span class="n">model</span> <span class="n">training</span><span class="p">,</span> <span class="ow">and</span> <span class="n">evaluation</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">critical</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">preparing</span>
<span class="n">the</span> <span class="n">video</span> <span class="n">data</span> <span class="k">for</span> <span class="n">a</span> <span class="n">deep</span> <span class="n">learning</span> <span class="n">model</span><span class="o">.</span> <span class="n">If</span> <span class="n">you</span> <span class="n">plan</span> <span class="n">to</span> <span class="n">use</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">models</span> <span class="ow">or</span> <span class="n">train</span> <span class="n">your</span> <span class="n">own</span> <span class="n">model</span> 
<span class="kn">from</span> <span class="nn">scratch</span><span class="p">,</span> <span class="n">this</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="n">now</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">suitable</span> <span class="nb">format</span> <span class="k">for</span> <span class="n">such</span> <span class="n">tasks</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e0bea577-52ef-47f8-8c9f-8f0cae302bca">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#. Data Augmentation</span>
<span class="n">Now</span> <span class="n">apply</span> <span class="n">data</span> <span class="n">augmentation</span> <span class="n">techniques</span> <span class="n">to</span> <span class="n">artificially</span> <span class="n">increase</span> <span class="n">the</span> <span class="n">size</span> <span class="n">of</span> <span class="n">your</span> <span class="n">dataset</span> <span class="ow">and</span>
<span class="n">improve</span> <span class="n">the</span> <span class="n">robustness</span> <span class="n">of</span> <span class="n">your</span> <span class="n">model</span><span class="o">.</span>

<span class="c1"># The basic data augmentation using ImageDataGenerator:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=072343fd-c3c3-4353-8cbe-9c78b0e8dbba">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Paths</span>
<span class="n">input_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>  <span class="c1"># Folder containing extracted frames</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\processed_frames"</span>  <span class="c1"># Folder to save processed frames</span>

<span class="c1"># Ensure output folder exists</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_folder</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_folder</span><span class="p">)</span>

<span class="c1"># Initialize ImageDataGenerator for augmentation</span>
<span class="n">datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>         <span class="c1"># Randomly rotate images by up to 30 degrees</span>
    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>     <span class="c1"># Randomly shift images horizontally by 20%</span>
    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>    <span class="c1"># Randomly shift images vertically by 20%</span>
    <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>           <span class="c1"># Shear images by 20%</span>
    <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>            <span class="c1"># Zoom in and out by 20%</span>
    <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>      <span class="c1"># Randomly flip images horizontally</span>
    <span class="n">fill_mode</span><span class="o">=</span><span class="s1">'nearest'</span>        <span class="c1"># Fill any new pixels after transformation</span>
<span class="p">)</span>

<span class="c1"># Read images from input folder and apply augmentations</span>
<span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">input_folder</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">'.jpg'</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">image_file</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">image_files</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Processing frames"</span><span class="p">):</span>
    <span class="c1"># Load image</span>
    <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">input_folder</span><span class="p">,</span> <span class="n">image_file</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>

    <span class="c1"># Check if the image is loaded properly</span>
    <span class="k">if</span> <span class="n">img</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error loading image </span><span class="si">{</span><span class="n">image_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">continue</span>
    
    <span class="c1"># Resize image to a fixed size (224x224)</span>
    <span class="n">img_resized</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>

    <span class="c1"># Convert image to 4D array (batch size, height, width, channels)</span>
    <span class="n">img_resized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">img_resized</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Apply augmentation</span>
    <span class="n">augmented_images</span> <span class="o">=</span> <span class="n">datagen</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">img_resized</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">save_to_dir</span><span class="o">=</span><span class="n">output_folder</span><span class="p">,</span>
                                    <span class="n">save_prefix</span><span class="o">=</span><span class="s1">'aug'</span><span class="p">,</span> <span class="n">save_format</span><span class="o">=</span><span class="s1">'jpg'</span><span class="p">)</span>

    <span class="c1"># Save augmented images</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>  <span class="c1"># Generate 5 augmented images per frame</span>
        <span class="nb">next</span><span class="p">(</span><span class="n">augmented_images</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Data augmentation complete!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Processing frames: 100%|| 650/650 [01:25&lt;00:00,  7.64it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Data augmentation complete!
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=5ab90249-682a-41b7-8952-cf30a9fe8a69">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="s2">"Data augmentation complete!"</span> <span class="n">indicates</span> <span class="n">that</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">frames</span> <span class="n">were</span> <span class="n">successfully</span> <span class="n">processed</span> <span class="ow">and</span> <span class="n">augmented</span><span class="o">.</span>

<span class="c1"># Explanation of Augmentation Process:</span>
<span class="n">Image</span> <span class="n">Augmentation</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">technique</span> <span class="n">used</span> <span class="n">to</span> <span class="n">artificially</span> <span class="n">increase</span> <span class="n">the</span> <span class="n">size</span> <span class="n">of</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">by</span> 
<span class="n">generating</span> <span class="n">modified</span> <span class="n">versions</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">This</span> <span class="n">process</span> <span class="n">helps</span> <span class="n">to</span> <span class="n">create</span> <span class="n">more</span> <span class="n">variability</span> <span class="ow">in</span> 
<span class="n">the</span> <span class="n">data</span><span class="p">,</span> <span class="n">allowing</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">learn</span> <span class="n">more</span> <span class="n">diverse</span> <span class="n">patterns</span> <span class="ow">and</span> <span class="n">thus</span> <span class="n">generalize</span> <span class="n">better</span><span class="o">.</span>

<span class="n">In</span> <span class="n">the</span> <span class="n">current</span> <span class="n">scenario</span><span class="p">,</span> <span class="n">augmented</span> <span class="n">the</span> <span class="n">video</span> <span class="n">frames</span> <span class="n">extracted</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">video</span> <span class="n">using</span> <span class="n">the</span> 
<span class="n">following</span> <span class="n">transformations</span><span class="p">:</span>

<span class="c1"># Rotation:</span>
<span class="n">Randomly</span> <span class="n">rotated</span> <span class="n">images</span> <span class="n">by</span> <span class="n">up</span> <span class="n">to</span> <span class="mi">30</span> <span class="n">degrees</span><span class="o">.</span> <span class="n">This</span> <span class="n">helps</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">recognize</span> <span class="n">objects</span> <span class="ow">or</span>
<span class="n">features</span> <span class="ow">in</span> <span class="n">different</span> <span class="n">orientations</span><span class="o">.</span>

<span class="c1"># Width and Height Shifting: </span>
<span class="n">Each</span> <span class="n">image</span> <span class="n">was</span> <span class="n">randomly</span> <span class="n">shifted</span> <span class="n">horizontally</span> <span class="ow">and</span> <span class="n">vertically</span> <span class="n">by</span> <span class="n">up</span> <span class="n">to</span> <span class="mi">20</span><span class="o">%.</span> <span class="n">This</span> <span class="n">simulates</span> 
<span class="n">various</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">scenarios</span> <span class="n">where</span> <span class="n">the</span> <span class="nb">object</span> <span class="n">might</span> <span class="n">be</span> <span class="n">at</span> <span class="n">different</span> <span class="n">positions</span> <span class="n">within</span> <span class="n">the</span> <span class="n">frame</span><span class="o">.</span>

<span class="c1"># Zooming: </span>
<span class="n">The</span> <span class="n">images</span> <span class="n">were</span> <span class="n">zoomed</span> <span class="ow">in</span> <span class="ow">or</span> <span class="n">out</span> <span class="n">by</span> <span class="n">up</span> <span class="n">to</span> <span class="mi">20</span><span class="o">%.</span> <span class="n">This</span> <span class="n">adds</span> <span class="n">variety</span> <span class="n">by</span> <span class="n">simulating</span> <span class="n">different</span> 
<span class="n">distances</span> <span class="n">between</span> <span class="n">the</span> <span class="n">camera</span> <span class="ow">and</span> <span class="n">the</span> <span class="nb">object</span><span class="o">.</span>

<span class="c1"># Horizontal Flipping: </span>
<span class="n">Random</span> <span class="n">horizontal</span> <span class="n">flipping</span> <span class="n">of</span> <span class="n">the</span> <span class="n">images</span><span class="o">.</span> <span class="n">This</span> <span class="n">allows</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">recognize</span> <span class="n">objects</span> 
<span class="kn">from</span> <span class="nn">both</span> <span class="n">sides</span> <span class="p">(</span><span class="n">mirroring</span> <span class="n">the</span> <span class="n">images</span><span class="p">)</span><span class="o">.</span>

<span class="c1"># Shear: A random shear transformation applied to the images. This mimics slight perspective</span>
<span class="n">shifts</span><span class="p">,</span> <span class="k">as</span> <span class="k">if</span> <span class="n">the</span> <span class="nb">object</span> <span class="ow">is</span> <span class="n">viewed</span> <span class="kn">from</span> <span class="nn">a</span> <span class="n">different</span> <span class="n">angle</span><span class="o">.</span>

<span class="c1"># How Augmentation Was Applied:</span>

<span class="n">For</span> <span class="n">each</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">650</span> <span class="n">frames</span><span class="p">,</span> <span class="n">you</span> <span class="n">generated</span> <span class="mi">5</span> <span class="n">augmented</span> <span class="n">versions</span><span class="o">.</span>
<span class="n">These</span> <span class="n">augmented</span> <span class="n">versions</span> <span class="n">have</span> <span class="n">been</span> <span class="n">saved</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">output</span> <span class="n">folder</span> <span class="p">(</span><span class="n">processed_frames</span><span class="p">)</span> <span class="k">with</span> <span class="n">the</span> 
<span class="n">prefix</span> <span class="n">aug_</span> <span class="p">(</span><span class="n">such</span> <span class="k">as</span><span class="p">,</span> <span class="n">aug_frame_96</span><span class="o">.</span><span class="n">jpg</span><span class="p">,</span> <span class="n">aug_frame_96_1</span><span class="o">.</span><span class="n">jpg</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">)</span><span class="o">.</span>

<span class="c1"># Insights and Interpretation:</span>

<span class="c1"># Increased Variability: </span>
<span class="n">By</span> <span class="n">applying</span> <span class="n">these</span> <span class="n">transformations</span><span class="p">,</span> <span class="n">we</span> <span class="n">are</span>  <span class="n">generating</span> <span class="n">new</span> <span class="n">training</span> <span class="n">data</span> <span class="n">that</span> <span class="n">captures</span> <span class="n">a</span> 
<span class="n">broader</span> <span class="n">variety</span> <span class="n">of</span> <span class="n">scenarios</span> <span class="n">than</span> <span class="n">what</span> <span class="n">was</span> <span class="n">present</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">original</span> <span class="n">frames</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">beneficial</span> 
<span class="n">because</span> <span class="n">it</span> <span class="n">helps</span> <span class="n">the</span> <span class="n">model</span> <span class="n">learn</span> <span class="n">to</span> <span class="n">recognize</span> <span class="n">patterns</span> <span class="ow">and</span> <span class="n">features</span> <span class="ow">in</span> <span class="n">different</span> <span class="n">conditions</span> 
<span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="n">rotated</span><span class="p">,</span> <span class="n">shifted</span><span class="p">,</span> <span class="n">zoomed</span><span class="p">)</span><span class="o">.</span>

<span class="c1"># Improved Generalization: </span>
<span class="n">Since</span> <span class="n">deep</span> <span class="n">learning</span> <span class="n">models</span> <span class="n">can</span> <span class="n">easily</span> <span class="n">overfit</span> <span class="n">to</span> <span class="n">training</span> <span class="n">data</span><span class="p">,</span> <span class="n">these</span> <span class="n">augmentations</span> <span class="n">help</span> 
<span class="n">mitigate</span> <span class="n">that</span> <span class="n">issue</span><span class="o">.</span><span class="n">The</span> <span class="n">model</span> <span class="n">will</span> <span class="n">see</span> <span class="n">different</span> <span class="n">variations</span> <span class="n">of</span> <span class="n">the</span> <span class="n">same</span> <span class="nb">object</span><span class="p">,</span> <span class="n">improving</span> 
<span class="n">its</span> <span class="n">ability</span> <span class="n">to</span> <span class="n">generalize</span> <span class="ow">and</span> <span class="n">recognize</span> <span class="n">the</span> <span class="nb">object</span> <span class="n">even</span> <span class="n">when</span> <span class="n">it</span> <span class="n">appears</span> <span class="ow">in</span> <span class="n">new</span> <span class="ow">or</span> <span class="n">unseen</span> <span class="n">ways</span>
<span class="n">during</span> <span class="n">inference</span><span class="o">.</span>

<span class="c1"># Data for Training: </span>
<span class="n">If</span> <span class="n">we</span> <span class="n">originally</span> <span class="n">had</span> <span class="mi">650</span> <span class="n">frames</span><span class="p">,</span> <span class="n">after</span> <span class="n">augmentation</span><span class="p">,</span> <span class="n">we</span> <span class="n">wll</span> <span class="n">have</span> <span class="mi">650</span> <span class="n">x</span> <span class="mi">5</span> <span class="o">=</span> <span class="mi">3250</span> <span class="n">frames</span><span class="o">.</span> 
<span class="n">This</span> <span class="n">larger</span><span class="p">,</span> <span class="n">augmented</span> <span class="n">dataset</span> <span class="n">gives</span> <span class="n">your</span> <span class="n">model</span> <span class="n">more</span> <span class="n">examples</span> <span class="n">to</span> <span class="n">learn</span> <span class="n">from</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">less</span> 
<span class="n">likely</span> <span class="n">to</span> <span class="n">memorize</span> <span class="p">(</span><span class="n">overfit</span><span class="p">)</span> <span class="n">the</span> <span class="n">training</span> <span class="n">data</span><span class="o">.</span>

<span class="n">Next</span> <span class="n">Steps</span><span class="p">:</span>
<span class="c1"># Verify Output:</span>
<span class="n">Check</span> <span class="n">the</span> <span class="n">processed_frames</span> <span class="n">folder</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">augmented</span> <span class="n">images</span> <span class="n">are</span> <span class="n">saved</span> <span class="n">correctly</span><span class="o">.</span>

<span class="n">Model</span> <span class="n">Training</span><span class="p">:</span> <span class="n">Now</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">move</span> <span class="n">on</span> <span class="n">to</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">step</span><span class="err"></span><span class="n">training</span> <span class="n">your</span> <span class="n">model</span> <span class="n">using</span> <span class="n">these</span> <span class="n">augmented</span> 
<span class="n">images</span><span class="o">.</span> <span class="n">The</span> <span class="n">model</span> <span class="n">should</span> <span class="n">be</span> <span class="n">able</span> <span class="n">to</span> <span class="n">learn</span> <span class="n">more</span> <span class="n">effectively</span> <span class="kn">from</span> <span class="nn">this</span> <span class="n">augmented</span> <span class="n">data</span><span class="p">,</span> <span class="n">especially</span> 
<span class="k">if</span> <span class="n">you</span><span class="s1">'re training a deep learning model.</span>

<span class="c1"># Monitor Performance: </span>
<span class="n">As</span> <span class="n">you</span> <span class="n">start</span> <span class="n">training</span><span class="p">,</span> <span class="n">keep</span> <span class="n">an</span> <span class="n">eye</span> <span class="n">on</span> <span class="n">performance</span> <span class="n">metrics</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> 
<span class="n">augmentation</span> <span class="ow">is</span> <span class="n">having</span> <span class="n">a</span> <span class="n">positive</span> <span class="n">impact</span> <span class="n">on</span> <span class="n">model</span> <span class="n">performance</span><span class="o">.</span>

<span class="c1"># In summary, the augmentation process worked successfully, and this will help in training a more</span>
<span class="n">robust</span> <span class="n">model</span> <span class="n">by</span> <span class="n">exposing</span> <span class="n">it</span> <span class="n">to</span> <span class="n">various</span> <span class="n">transformations</span> <span class="n">of</span> <span class="n">the</span> <span class="n">same</span> <span class="n">data</span><span class="o">.</span> 
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=8fbb493d-eb3d-4204-b32d-6da5c908298d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 3. Data Splitting</span>
<span class="n">To</span> <span class="n">split</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">into</span> <span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">test</span> <span class="n">sets</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">use</span> <span class="n">tools</span> <span class="n">like</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="ow">or</span> <span class="n">manually</span> <span class="n">organize</span> <span class="n">them</span> <span class="n">into</span> <span class="n">different</span> <span class="n">folders</span><span class="o">.</span>
 <span class="n">using</span> <span class="n">train_test_split</span><span class="p">:</span>
<span class="n">Split</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">into</span> <span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">test</span> <span class="n">sets</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="mi">70</span><span class="o">%-</span><span class="mi">20</span><span class="o">%-</span><span class="mi">10</span><span class="o">%</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fc389eca-0618-4a52-a679-575ab8cd3ca2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Define the folder containing frames</span>
<span class="n">frame_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>  <span class="c1"># Update with your actual path</span>

<span class="c1"># Ensure the folder exists</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The folder '</span><span class="si">{</span><span class="n">frame_folder</span><span class="si">}</span><span class="s2">' does not exist."</span><span class="p">)</span>

<span class="c1"># List all image file paths in the folder</span>
<span class="n">frame_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">frame_folder</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">'.jpg'</span><span class="p">)]</span>

<span class="c1"># Print the number of frames found</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">frame_paths</span><span class="p">)</span><span class="si">}</span><span class="s2"> image files in '</span><span class="si">{</span><span class="n">frame_folder</span><span class="si">}</span><span class="s2">'."</span><span class="p">)</span>

<span class="c1"># For demonstration, show a few example frame paths</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Example frame paths:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">frame_paths</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="c1"># Assuming binary labels for now (0 for all)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">frame_paths</span><span class="p">)</span>  <span class="c1"># Replace with actual labels if available</span>

<span class="c1"># Split into training and test sets (e.g., 70%-30%)</span>
<span class="n">train_paths</span><span class="p">,</span> <span class="n">test_paths</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">frame_paths</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Print the split details</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training set size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_paths</span><span class="p">)</span><span class="si">}</span><span class="s2"> frames"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test set size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_paths</span><span class="p">)</span><span class="si">}</span><span class="s2"> frames"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Found 650 image files in 'C:\Users\krna5\Downloads\frames'.
Example frame paths:
C:\Users\krna5\Downloads\frames\frame_0.jpg
C:\Users\krna5\Downloads\frames\frame_1.jpg
C:\Users\krna5\Downloads\frames\frame_10.jpg
C:\Users\krna5\Downloads\frames\frame_100.jpg
C:\Users\krna5\Downloads\frames\frame_101.jpg
Training set size: 455 frames
Test set size: 195 frames
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=fcec581d-9ea1-4681-acf2-b2a9d25f8006">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Analysising the Result:</span>

<span class="c1"># Identified Frames: </span>
<span class="n">Located</span> <span class="mi">650</span> <span class="n">image</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">folder</span> <span class="n">C</span><span class="p">:</span>\<span class="n">Users</span>\<span class="n">krna5</span>\<span class="n">Downloads</span>\<span class="n">frames</span><span class="o">.</span>
<span class="c1"># isplayed Example Paths:</span>
<span class="n">Printed</span> <span class="n">the</span> <span class="n">paths</span> <span class="n">of</span> <span class="n">five</span> <span class="n">example</span> <span class="n">frames</span><span class="p">,</span> <span class="n">showing</span> <span class="n">that</span> <span class="n">the</span> <span class="n">files</span> <span class="n">are</span> <span class="n">named</span> <span class="ow">in</span>
<span class="n">a</span> <span class="n">predictable</span> <span class="n">pattern</span><span class="o">.</span>
<span class="c1"># Split Data: </span>
<span class="n">Divided</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">into</span> <span class="n">a</span> <span class="n">training</span> <span class="nb">set</span> <span class="p">(</span><span class="mi">70</span><span class="o">%</span><span class="p">,</span> <span class="mi">455</span> <span class="n">frames</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">test</span> <span class="nb">set</span> <span class="p">(</span><span class="mi">30</span><span class="o">%</span><span class="p">,</span> <span class="mi">195</span> <span class="n">frames</span><span class="p">)</span> 
<span class="n">using</span> <span class="n">the</span> <span class="n">train_test_split</span> <span class="n">function</span><span class="o">.</span>
    
<span class="c1"># The result demonstrates that the dataset has been prepared correctly for the next steps</span>
<span class="ow">in</span> <span class="n">model</span> <span class="n">training</span> <span class="ow">and</span> <span class="n">evaluation</span><span class="o">.</span>

<span class="c1"># Insights:</span>
<span class="c1"># Frame Count:</span>
<span class="n">The</span> <span class="n">dataset</span> <span class="n">contains</span> <span class="mi">650</span> <span class="n">frames</span><span class="p">,</span> <span class="n">which</span> <span class="ow">is</span> <span class="n">sufficient</span> <span class="k">for</span> <span class="n">training</span> <span class="n">simple</span> <span class="n">models</span>
<span class="ow">or</span> <span class="n">conducting</span> <span class="n">experiments</span><span class="o">.</span> <span class="n">For</span> <span class="n">deep</span> <span class="n">learning</span> <span class="n">models</span><span class="p">,</span> <span class="n">augmenting</span> <span class="n">the</span> <span class="n">data</span> <span class="n">might</span> <span class="n">be</span> 
<span class="n">necessary</span> <span class="n">to</span> <span class="n">improve</span> <span class="n">diversity</span> <span class="ow">and</span> <span class="n">performance</span><span class="o">.</span>

<span class="c1"># Data Split:</span>

<span class="n">The</span> <span class="n">training</span> <span class="nb">set</span> <span class="n">includes</span> <span class="mi">455</span> <span class="n">frames</span><span class="p">,</span> <span class="n">which</span> <span class="n">will</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">train</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span>
<span class="n">The</span> <span class="n">test</span> <span class="nb">set</span> <span class="n">includes</span> <span class="mi">195</span> <span class="n">frames</span><span class="p">,</span> <span class="n">which</span> <span class="n">will</span> <span class="n">be</span> <span class="n">reserved</span> <span class="k">for</span> <span class="n">evaluating</span> <span class="n">the</span> <span class="n">model</span><span class="err"></span><span class="n">s</span> 
<span class="n">performance</span> <span class="n">on</span> <span class="n">unseen</span> <span class="n">data</span><span class="o">.</span><span class="n">A</span> <span class="mi">70</span><span class="o">%-</span><span class="mi">30</span><span class="o">%</span> <span class="n">split</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">common</span> <span class="n">practice</span> <span class="ow">and</span> <span class="ow">is</span> <span class="n">appropriate</span> <span class="ow">in</span> <span class="n">this</span> <span class="n">case</span><span class="o">.</span>
    
<span class="c1"># Frame File Names: The filenames (frame_0.jpg, frame_1.jpg, etc.) appear consistent, suggesting </span>
<span class="n">no</span> <span class="n">data</span> <span class="n">corruption</span> <span class="ow">or</span> <span class="n">missing</span> <span class="n">frames</span><span class="o">.</span>

<span class="c1"># Suggestions for Improvement:</span>
<span class="n">To</span> <span class="n">make</span> <span class="n">the</span> <span class="n">model</span> <span class="n">robust</span> <span class="ow">and</span> <span class="n">outstanding</span><span class="p">,</span> <span class="n">consider</span> <span class="n">the</span> <span class="n">following</span><span class="p">:</span>

<span class="c1"># 1. Data Augmentation</span>
<span class="n">Perform</span> <span class="n">data</span> <span class="n">augmentation</span> <span class="n">on</span> <span class="n">the</span> <span class="n">training</span> <span class="nb">set</span> <span class="n">to</span> <span class="n">improve</span> <span class="n">model</span> <span class="n">generalization</span><span class="o">.</span> <span class="n">Common</span> <span class="n">techniques</span> <span class="n">include</span><span class="p">:</span>
<span class="n">Horizontal</span> <span class="ow">and</span> <span class="n">vertical</span> <span class="n">flips</span>
<span class="n">Random</span> <span class="n">rotations</span> <span class="ow">and</span> <span class="n">cropping</span>
<span class="n">Brightness</span><span class="p">,</span> <span class="n">contrast</span><span class="p">,</span> <span class="ow">and</span> <span class="n">saturation</span> <span class="n">adjustments</span>

<span class="n">Adding</span> <span class="n">noise</span> <span class="n">Example</span> <span class="n">augmentation</span> <span class="n">techniques</span> <span class="n">can</span> <span class="n">be</span> <span class="n">implemented</span> <span class="n">using</span> <span class="n">libraries</span> <span class="n">like</span> <span class="n">imgaug</span><span class="p">,</span> <span class="n">albumentations</span><span class="p">,</span>
<span class="ow">or</span> <span class="n">TensorFlow</span><span class="o">/</span><span class="n">Keras</span><span class="o">.</span>

<span class="c1"># 2. Check for Imbalanced Data</span>
<span class="n">Ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">has</span> <span class="n">balanced</span> <span class="n">labels</span> <span class="k">if</span> <span class="n">there</span> <span class="n">are</span> <span class="n">multiple</span> <span class="n">classes</span><span class="o">.</span>
<span class="n">If</span> <span class="n">the</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="n">imbalanced</span><span class="p">,</span> <span class="n">use</span> <span class="n">techniques</span> <span class="n">such</span> <span class="k">as</span> <span class="n">oversampling</span><span class="p">,</span> <span class="n">undersampling</span><span class="p">,</span> <span class="ow">or</span> <span class="n">class</span><span class="o">-</span><span class="n">weight</span> 
<span class="n">adjustments</span> <span class="n">during</span> <span class="n">training</span><span class="o">.</span>

<span class="c1"># 3. Explore Transfer Learning</span>
<span class="n">Utilize</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">models</span> <span class="n">like</span> <span class="n">ResNet</span><span class="p">,</span> <span class="n">VGG</span><span class="p">,</span> <span class="ow">or</span> <span class="n">EfficientNet</span> <span class="k">for</span> <span class="n">feature</span> <span class="n">extraction</span> <span class="ow">or</span> <span class="n">fine</span><span class="o">-</span><span class="n">tuning</span><span class="o">.</span> 
<span class="n">Transfer</span> <span class="n">learning</span> <span class="n">can</span> <span class="n">significantly</span> <span class="n">boost</span> <span class="n">performance</span> <span class="n">on</span> <span class="n">small</span> <span class="n">datasets</span><span class="o">.</span>
<span class="mf">4.</span> <span class="n">Cross</span><span class="o">-</span><span class="n">Validation</span>
<span class="n">Instead</span> <span class="n">of</span> <span class="n">relying</span> <span class="n">solely</span> <span class="n">on</span> <span class="n">a</span> <span class="n">single</span> <span class="n">train</span><span class="o">-</span><span class="n">test</span> <span class="n">split</span><span class="p">,</span> <span class="n">perform</span> <span class="n">k</span><span class="o">-</span><span class="n">fold</span> <span class="n">cross</span><span class="o">-</span><span class="n">validation</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s robustness and reduce variability in performance metrics.</span>
<span class="mf">5.</span> <span class="n">Preprocessing</span>
<span class="n">Normalize</span> <span class="n">image</span> <span class="n">pixel</span> <span class="n">values</span> <span class="n">to</span> <span class="n">a</span> <span class="nb">range</span> <span class="n">of</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span>
<span class="n">Resize</span> <span class="nb">all</span> <span class="n">frames</span> <span class="n">to</span> <span class="n">a</span> <span class="n">consistent</span> <span class="n">size</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="mi">224</span><span class="n">x224</span> <span class="k">for</span> <span class="n">most</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">models</span><span class="p">)</span><span class="o">.</span>
<span class="mf">6.</span> <span class="n">Feature</span> <span class="n">Engineering</span>
<span class="n">If</span> <span class="n">working</span> <span class="k">with</span> <span class="n">multiple</span> <span class="n">sources</span> <span class="n">of</span> <span class="n">data</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">video</span> <span class="n">metadata</span> <span class="ow">or</span> <span class="n">audio</span> <span class="n">features</span><span class="p">),</span> <span class="n">consider</span> <span class="n">incorporating</span> <span class="n">additional</span> <span class="n">features</span> <span class="n">into</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span>
<span class="mf">7.</span> <span class="n">Model</span> <span class="n">Selection</span>
<span class="n">Experiment</span> <span class="k">with</span> <span class="n">different</span> <span class="n">architectures</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">CNNs</span> <span class="k">for</span> <span class="n">image</span><span class="o">-</span><span class="n">based</span> <span class="n">tasks</span><span class="p">)</span><span class="o">.</span>
<span class="n">Evaluate</span> <span class="n">performance</span> <span class="n">metrics</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">F1</span><span class="o">-</span><span class="n">score</span><span class="p">)</span> <span class="n">to</span> <span class="n">choose</span> <span class="n">the</span> <span class="n">best</span> <span class="n">model</span><span class="o">.</span>
<span class="mf">8.</span> <span class="n">Model</span> <span class="n">Validation</span> <span class="ow">and</span> <span class="n">Testing</span>
<span class="n">Use</span> <span class="n">the</span> <span class="n">reserved</span> <span class="n">test</span> <span class="nb">set</span> <span class="n">to</span> <span class="n">evaluate</span> <span class="n">the</span> <span class="n">model</span> <span class="n">only</span> <span class="n">after</span> <span class="n">hyperparameter</span> <span class="n">tuning</span><span class="o">.</span>
<span class="n">Create</span> <span class="n">a</span> <span class="n">validation</span> <span class="nb">set</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">training</span> <span class="n">data</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">an</span> <span class="mi">80</span><span class="o">%-</span><span class="mi">20</span><span class="o">%</span> <span class="n">split</span> <span class="n">of</span> <span class="n">the</span> <span class="n">training</span> <span class="nb">set</span><span class="p">)</span> <span class="k">for</span> <span class="n">tuning</span> <span class="n">hyperparameters</span><span class="o">.</span>
<span class="mf">9.</span> <span class="n">Monitor</span> <span class="n">Overfitting</span>
<span class="n">Regularize</span> <span class="n">the</span> <span class="n">model</span> <span class="n">using</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">weight</span> <span class="n">decay</span><span class="p">,</span> <span class="ow">or</span> <span class="n">early</span> <span class="n">stopping</span><span class="o">.</span>
<span class="n">Monitor</span> <span class="n">training</span> <span class="ow">and</span> <span class="n">validation</span> <span class="n">loss</span> <span class="n">to</span> <span class="n">detect</span> <span class="n">overfitting</span><span class="o">.</span>
<span class="mf">10.</span> <span class="n">Interpretability</span>
<span class="n">Use</span> <span class="n">techniques</span> <span class="n">like</span> <span class="n">Grad</span><span class="o">-</span><span class="n">CAM</span> <span class="ow">or</span> <span class="n">SHAP</span> <span class="n">to</span> <span class="n">visualize</span> <span class="ow">and</span> <span class="n">interpret</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s predictions, especially for image-based tasks.</span>
<span class="n">Next</span> <span class="n">Steps</span><span class="p">:</span>
<span class="n">Perform</span> <span class="n">data</span> <span class="n">augmentation</span> <span class="n">on</span> <span class="n">the</span> <span class="n">training</span> <span class="nb">set</span><span class="o">.</span>
<span class="n">Normalize</span> <span class="ow">and</span> <span class="n">resize</span> <span class="nb">all</span> <span class="n">images</span> <span class="n">to</span> <span class="n">a</span> <span class="n">consistent</span> <span class="n">size</span><span class="o">.</span>
<span class="n">Define</span> <span class="n">the</span> <span class="n">model</span> <span class="n">architecture</span> <span class="ow">and</span> <span class="n">start</span> <span class="n">training</span><span class="o">.</span>
<span class="n">Regularly</span> <span class="n">evaluate</span> <span class="n">the</span> <span class="n">model</span> <span class="n">on</span> <span class="n">the</span> <span class="n">validation</span> <span class="nb">set</span> <span class="n">to</span> <span class="n">adjust</span> <span class="n">hyperparameters</span><span class="o">.</span>
<span class="n">Once</span> <span class="n">satisfied</span><span class="p">,</span> <span class="n">test</span> <span class="n">the</span> <span class="n">model</span> <span class="n">on</span> <span class="n">the</span> <span class="n">test</span> <span class="nb">set</span> <span class="ow">and</span> <span class="n">report</span> <span class="n">metrics</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=729fec8b-2c5a-4050-addb-2ae517505faa">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Splitting the Data</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6dbc6fc0-20d8-4722-96a9-917a4c09f23a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Paths</span>
<span class="n">input_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>  <span class="c1"># Folder containing extracted frames</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\processed_frames"</span>  <span class="c1"># Folder to save processed frames</span>

<span class="c1"># Output directories for splits</span>
<span class="n">train_folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">)</span>
<span class="n">val_folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="s2">"val"</span><span class="p">)</span>
<span class="n">test_folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="s2">"test"</span><span class="p">)</span>

<span class="c1"># Create directories if they don't exist</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">train_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">val_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get a list of all image files in the input folder</span>
<span class="n">frame_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">input_folder</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">'.jpg'</span><span class="p">)]</span>

<span class="c1"># Split data into training, validation, and test sets</span>
<span class="n">train_files</span><span class="p">,</span> <span class="n">temp_files</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">frame_files</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">val_files</span><span class="p">,</span> <span class="n">test_files</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">temp_files</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># 0.33 x 30% = 10%</span>

<span class="c1"># Function to copy files to designated folders</span>
<span class="k">def</span> <span class="nf">copy_files</span><span class="p">(</span><span class="n">file_list</span><span class="p">,</span> <span class="n">source_folder</span><span class="p">,</span> <span class="n">destination_folder</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">file_list</span><span class="p">:</span>
        <span class="n">src_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">source_folder</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
        <span class="n">dst_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">destination_folder</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">src_path</span><span class="p">,</span> <span class="n">dst_path</span><span class="p">)</span>

<span class="c1"># Copy files to respective folders</span>
<span class="n">copy_files</span><span class="p">(</span><span class="n">train_files</span><span class="p">,</span> <span class="n">input_folder</span><span class="p">,</span> <span class="n">train_folder</span><span class="p">)</span>
<span class="n">copy_files</span><span class="p">(</span><span class="n">val_files</span><span class="p">,</span> <span class="n">input_folder</span><span class="p">,</span> <span class="n">val_folder</span><span class="p">)</span>
<span class="n">copy_files</span><span class="p">(</span><span class="n">test_files</span><span class="p">,</span> <span class="n">input_folder</span><span class="p">,</span> <span class="n">test_folder</span><span class="p">)</span>

<span class="c1"># Print summary</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset split completed:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training set size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_files</span><span class="p">)</span><span class="si">}</span><span class="s2"> frames"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Validation set size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">val_files</span><span class="p">)</span><span class="si">}</span><span class="s2"> frames"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test set size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_files</span><span class="p">)</span><span class="si">}</span><span class="s2"> frames"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Files saved in:</span><span class="se">\n</span><span class="s2">Train folder: </span><span class="si">{</span><span class="n">train_folder</span><span class="si">}</span><span class="se">\n</span><span class="s2">Validation folder: </span><span class="si">{</span><span class="n">val_folder</span><span class="si">}</span><span class="se">\n</span><span class="s2">Test folder: </span><span class="si">{</span><span class="n">test_folder</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Dataset split completed:
Training set size: 455 frames
Validation set size: 130 frames
Test set size: 65 frames
Files saved in:
Train folder: C:\Users\krna5\Downloads\processed_frames\train
Validation folder: C:\Users\krna5\Downloads\processed_frames\val
Test folder: C:\Users\krna5\Downloads\processed_frames\test
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6400bad4-245f-47e5-b029-ec01801eae1a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">N</span><span class="o">.</span><span class="n">B</span><span class="o">.</span><span class="p">:</span><span class="n">This</span> <span class="n">organized</span> <span class="n">split</span> <span class="n">will</span> <span class="n">help</span> <span class="n">ensure</span> <span class="n">the</span> <span class="n">data</span> <span class="ow">is</span> <span class="n">ready</span> <span class="k">for</span> <span class="n">training</span> <span class="ow">and</span> <span class="n">evaluation</span> <span class="n">phases</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">project</span>

<span class="c1"># Dataset Split Overview:</span>

<span class="c1"># Training Set (455 frames):</span>

<span class="n">The</span> <span class="n">largest</span> <span class="n">portion</span> <span class="n">of</span> <span class="n">the</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">representing</span> <span class="mi">70</span><span class="o">%</span> <span class="n">of</span> <span class="n">the</span> <span class="n">total</span> <span class="n">frames</span><span class="o">.</span>
<span class="n">Used</span> <span class="n">to</span> <span class="n">train</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">to</span> <span class="n">learn</span> <span class="n">features</span> <span class="ow">and</span> <span class="n">detect</span> <span class="n">objects</span> <span class="n">effectively</span><span class="o">.</span>
    
<span class="c1"># Validation Set (130 frames):</span>
<span class="n">Represents</span> <span class="mi">20</span><span class="o">%</span> <span class="n">of</span> <span class="n">the</span> <span class="n">dataset</span><span class="o">.</span>
<span class="n">Used</span> <span class="n">to</span> <span class="n">tune</span> <span class="n">hyperparameters</span> <span class="ow">and</span> <span class="n">monitor</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s performance during training, </span>
<span class="n">ensuring</span> <span class="n">that</span> <span class="n">it</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">overfitting</span><span class="o">.</span>

<span class="c1"># Test Set (65 frames):</span>
<span class="n">Represents</span> <span class="mi">10</span><span class="o">%</span> <span class="n">of</span> <span class="n">the</span> <span class="n">dataset</span><span class="o">.</span>
<span class="n">Reserved</span> <span class="k">for</span> <span class="n">final</span> <span class="n">evaluation</span> <span class="n">of</span> <span class="n">the</span> <span class="n">trained</span> <span class="n">model</span><span class="s1">'s performance on unseen data, </span>
<span class="n">providing</span> <span class="n">an</span> <span class="n">unbiased</span> <span class="n">metric</span><span class="o">.</span>
    
<span class="c1"># File Organization:</span>

<span class="n">Frames</span> <span class="n">are</span> <span class="n">neatly</span> <span class="n">organized</span> <span class="n">into</span> <span class="n">folders</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span> <span class="k">for</span> <span class="n">seamless</span> <span class="n">integration</span> 
<span class="k">with</span> <span class="n">YOLO</span> <span class="n">training</span> <span class="n">pipelines</span> <span class="ow">or</span> <span class="n">other</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">frameworks</span><span class="o">.</span><span class="n">This</span> <span class="n">structure</span> <span class="ow">is</span> <span class="n">compatible</span>
<span class="k">with</span> <span class="n">most</span> <span class="n">deep</span> <span class="n">learning</span> <span class="n">libraries</span> <span class="n">like</span> <span class="n">TensorFlow</span><span class="p">,</span> <span class="n">PyTorch</span><span class="p">,</span> <span class="ow">or</span> <span class="n">Darknet</span><span class="o">.</span>

<span class="c1"># Insights</span>

<span class="c1"># Balanced Split:</span>

<span class="n">The</span> <span class="n">dataset</span> <span class="n">split</span> <span class="n">ratios</span> <span class="p">(</span><span class="mi">70</span><span class="o">%-</span><span class="mi">20</span><span class="o">%-</span><span class="mi">10</span><span class="o">%</span><span class="p">)</span> <span class="n">are</span> <span class="n">standard</span> <span class="ow">in</span> <span class="n">machine</span> <span class="n">learning</span><span class="p">,</span> <span class="n">ensuring</span> <span class="n">that</span>
<span class="n">the</span> <span class="n">model</span> <span class="n">has</span> <span class="n">enough</span> <span class="n">data</span> <span class="k">for</span> <span class="n">training</span> <span class="k">while</span> <span class="n">leaving</span> <span class="n">sufficient</span> <span class="n">data</span> <span class="k">for</span> <span class="n">validation</span> <span class="ow">and</span> <span class="n">testing</span><span class="o">.</span>
    
<span class="c1"># Adequate Data Volume:</span>

<span class="n">With</span> <span class="mi">455</span> <span class="n">training</span> <span class="n">frames</span><span class="p">,</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">should</span> <span class="n">provide</span> <span class="n">a</span> <span class="n">solid</span> <span class="n">foundation</span> <span class="k">for</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">to</span> 
<span class="n">learn</span> <span class="nb">object</span> <span class="n">detection</span><span class="o">.</span><span class="n">However</span><span class="p">,</span> <span class="n">the</span> <span class="n">actual</span> <span class="n">effectiveness</span> <span class="n">depends</span> <span class="n">on</span> <span class="n">the</span> <span class="n">diversity</span> <span class="n">of</span> <span class="n">objects</span> 
<span class="ow">and</span> <span class="n">scenarios</span> <span class="n">captured</span> <span class="ow">in</span> <span class="n">these</span> <span class="n">frames</span><span class="o">.</span>
    
<span class="c1"># Potential Risks:</span>

<span class="n">If</span> <span class="n">frames</span> <span class="n">lack</span> <span class="n">diversity</span> <span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="n">similar</span> <span class="n">backgrounds</span><span class="p">,</span> <span class="n">lighting</span><span class="p">,</span> <span class="ow">or</span> <span class="nb">object</span> <span class="n">orientations</span><span class="p">),</span> 
<span class="n">the</span> <span class="n">model</span> <span class="n">might</span> <span class="ow">not</span> <span class="n">generalize</span> <span class="n">well</span><span class="o">.</span>

<span class="n">An</span> <span class="n">imbalanced</span> <span class="n">dataset</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">certain</span> <span class="n">objects</span> <span class="n">appearing</span> <span class="n">more</span> <span class="n">frequently</span> <span class="n">than</span> <span class="n">others</span><span class="p">)</span> <span class="n">could</span>
<span class="n">lead</span> <span class="n">to</span> <span class="n">biased</span> <span class="n">predictions</span><span class="o">.</span>
    
<span class="c1"># Suggestions to Improve Model Robustness</span>
    
<span class="c1"># Data Augmentation:</span>

<span class="n">Further</span> <span class="n">enhance</span> <span class="n">the</span> <span class="n">training</span> <span class="n">data</span> <span class="n">by</span> <span class="n">applying</span> <span class="n">advanced</span> <span class="n">augmentation</span> <span class="n">techniques</span> <span class="n">like</span><span class="p">:</span>
<span class="n">Random</span> <span class="n">rotation</span><span class="p">,</span> <span class="n">flipping</span><span class="p">,</span> <span class="ow">and</span> <span class="n">cropping</span><span class="p">:</span> <span class="n">Simulate</span> <span class="n">different</span> <span class="nb">object</span> <span class="n">orientations</span><span class="o">.</span>
    
<span class="c1"># Color jittering: </span>
<span class="n">Add</span> <span class="n">variations</span> <span class="ow">in</span> <span class="n">brightness</span><span class="p">,</span> <span class="n">contrast</span><span class="p">,</span> <span class="ow">and</span> <span class="n">saturation</span> <span class="n">to</span> <span class="n">mimic</span> <span class="n">diverse</span>
<span class="n">lighting</span> <span class="n">conditions</span><span class="o">.</span>
    
<span class="c1"># Gaussian noise: </span>
<span class="n">Introduce</span> <span class="n">random</span> <span class="n">noise</span> <span class="n">to</span> <span class="n">improve</span> <span class="n">robustness</span> <span class="n">against</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">imperfections</span><span class="o">.</span>

<span class="c1"># Class Balancing:</span>

<span class="n">Ensure</span> <span class="n">that</span> <span class="nb">all</span> <span class="nb">object</span> <span class="n">classes</span> <span class="n">are</span> <span class="n">equally</span> <span class="n">represented</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">dataset</span><span class="o">.</span> <span class="n">If</span> <span class="n">some</span> <span class="n">classes</span> <span class="n">are</span> 
<span class="n">underrepresented</span><span class="p">:</span>
<span class="n">Augment</span> <span class="n">underrepresented</span> <span class="n">classes</span><span class="o">.</span><span class="n">Collect</span> <span class="n">additional</span> <span class="n">data</span> <span class="k">for</span> <span class="n">those</span> <span class="n">classes</span><span class="o">.</span>

<span class="c1"># Data Validation:</span>

<span class="c1"># Manually inspect frames in each split to confirm:</span>
<span class="n">Frames</span> <span class="n">contain</span> <span class="n">the</span> <span class="n">objects</span> <span class="n">of</span> <span class="n">interest</span><span class="o">.</span>
<span class="n">Frames</span> <span class="n">are</span> <span class="n">clear</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">corrupted</span><span class="o">.</span>

<span class="c1"># Transfer Learning:</span>

<span class="n">Fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">a</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="p">(</span><span class="n">such</span> <span class="k">as</span><span class="p">,</span> <span class="n">YOLOv5</span> <span class="ow">or</span> <span class="n">YOLOv8</span><span class="p">)</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">training</span> <span class="kn">from</span> <span class="nn">scratch.</span> 
<span class="n">This</span> <span class="n">leverages</span> <span class="n">prior</span> <span class="n">knowledge</span> <span class="kn">from</span> <span class="nn">larger</span> <span class="n">datasets</span> <span class="n">like</span> <span class="n">COCO</span><span class="p">,</span><span class="n">reducing</span> <span class="n">the</span> <span class="n">need</span> <span class="k">for</span> <span class="n">extensive</span> <span class="n">data</span><span class="o">.</span>
                                                                               
<span class="c1"># Hyperparameter Tuning:</span>

<span class="n">Experiment</span> <span class="k">with</span><span class="p">:</span>
<span class="n">Learning</span> <span class="n">rate</span> <span class="n">adjustments</span><span class="o">.</span>
<span class="n">Batch</span> <span class="n">size</span> <span class="n">optimization</span><span class="o">.</span>
<span class="n">Anchor</span> <span class="n">box</span> <span class="n">scaling</span> <span class="k">for</span> <span class="n">better</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">performance</span><span class="o">.</span>
          
<span class="c1"># Evaluation Metrics:</span>

<span class="n">Use</span> <span class="n">metrics</span> <span class="n">like</span> <span class="n">mean</span> <span class="n">Average</span> <span class="n">Precision</span> <span class="p">(</span><span class="n">mAP</span><span class="p">)</span> <span class="ow">and</span> <span class="n">Intersection</span> <span class="n">over</span> <span class="n">Union</span> <span class="p">(</span><span class="n">IoU</span><span class="p">)</span> <span class="n">to</span> <span class="n">gauge</span> <span class="n">performance</span><span class="o">.</span>
<span class="n">Monitor</span> <span class="n">validation</span> <span class="n">loss</span> <span class="n">to</span> <span class="n">detect</span> <span class="n">overfitting</span><span class="o">.</span>
                                                                               
<span class="c1"># Cross-validation:</span>
<span class="n">Perform</span> <span class="n">k</span><span class="o">-</span><span class="n">fold</span> <span class="n">cross</span><span class="o">-</span><span class="n">validation</span> <span class="n">on</span> <span class="n">the</span> <span class="n">training</span> <span class="ow">and</span> <span class="n">validation</span> <span class="n">sets</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span>
<span class="n">robust</span> <span class="n">across</span> <span class="n">various</span> <span class="n">data</span> <span class="n">splits</span><span class="o">.</span>
                                                                               
<span class="c1"># Test Set Diversity:</span>

<span class="n">Ensure</span> <span class="n">the</span> <span class="n">test</span> <span class="nb">set</span> <span class="n">includes</span> <span class="n">challenging</span> <span class="n">frames</span> <span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="n">occlusions</span><span class="p">,</span> <span class="n">multiple</span> <span class="n">objects</span><span class="p">)</span> <span class="n">to</span> <span class="n">rigorously</span> 
<span class="n">evaluate</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s capabilities.</span>
<span class="c1"># Conclusion</span>
                                                                               
<span class="c1"># By following these strategies, the model will:</span>
<span class="n">Learn</span> <span class="n">more</span> <span class="n">robust</span> <span class="ow">and</span> <span class="n">generalized</span> <span class="n">features</span><span class="o">.</span>
<span class="n">Handle</span> <span class="n">diverse</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">scenarios</span> <span class="n">effectively</span><span class="o">.</span>
<span class="n">Avoid</span> <span class="n">overfitting</span> <span class="ow">or</span> <span class="n">underfitting</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">an</span> <span class="n">outstanding</span> <span class="n">solution</span> <span class="k">for</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">tasks</span><span class="o">.</span>
<span class="n">If</span> <span class="n">additional</span> <span class="n">data</span> <span class="n">can</span> <span class="n">be</span> <span class="n">collected</span> <span class="ow">or</span> <span class="n">simulated</span><span class="p">,</span> <span class="n">incorporating</span> <span class="n">it</span> <span class="n">into</span> <span class="n">the</span> <span class="n">training</span> <span class="n">pipeline</span> <span class="n">would</span>
<span class="n">further</span> <span class="n">strengthen</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s performance.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=57f79add-6a34-40a7-9133-03c751f231ce">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Advanced data augmentation using the albumentations library. It applies techniques like horizontal </span>
<span class="n">flips</span><span class="p">,</span> <span class="n">random</span> <span class="n">rotations</span><span class="p">,</span> <span class="n">cropping</span><span class="p">,</span> <span class="n">brightness</span><span class="o">/</span><span class="n">contrast</span> <span class="n">adjustments</span><span class="p">,</span> <span class="ow">and</span> <span class="n">adding</span> <span class="n">noise</span><span class="o">.</span> <span class="n">This</span> <span class="n">script</span> 
<span class="n">processes</span> <span class="n">frames</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">training</span> <span class="nb">set</span> <span class="ow">and</span> <span class="n">saves</span> <span class="n">the</span> <span class="n">augmented</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">specified</span> <span class="n">output</span> <span class="n">folder</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e606590c-4098-438e-90e1-cbc8bbad7515">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">albumentations</span> <span class="k">as</span> <span class="nn">A</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Paths</span>
<span class="n">input_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>  <span class="c1"># Folder containing extracted frames</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\augmented_frames"</span>  <span class="c1"># Folder to save augmented frames</span>

<span class="c1"># Create output folder if it doesn't exist</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Define the augmentation pipeline</span>
<span class="n">augmentations</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">A</span><span class="o">.</span><span class="n">Flip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Random flip (horizontal or vertical)</span>
    <span class="n">A</span><span class="o">.</span><span class="n">Rotate</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Random rotation within 30 degrees</span>
    <span class="n">A</span><span class="o">.</span><span class="n">RandomBrightnessContrast</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Random adjustment of brightness and contrast</span>
    <span class="n">A</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">blur_limit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>  <span class="c1"># Add random Gaussian blur</span>
<span class="p">])</span>

<span class="c1"># Apply augmentations</span>
<span class="k">for</span> <span class="n">frame_name</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">input_folder</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Augmenting frames"</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">frame_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".jpg"</span><span class="p">):</span>
        <span class="n">frame_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">input_folder</span><span class="p">,</span> <span class="n">frame_name</span><span class="p">)</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">frame_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">frame</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Apply augmentations</span>
            <span class="n">augmented</span> <span class="o">=</span> <span class="n">augmentations</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">frame</span><span class="p">)[</span><span class="s2">"image"</span><span class="p">]</span>

            <span class="c1"># Save the augmented frame</span>
            <span class="n">output_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"aug_</span><span class="si">{</span><span class="n">frame_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">augmented</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Augmentation completed! Augmented frames saved to </span><span class="si">{</span><span class="n">output_folder</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\krna5\AppData\Local\Temp\ipykernel_3372\637270041.py:15: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.
  A.Flip(p=0.5),  # Random flip (horizontal or vertical)
C:\Users\krna5\anaconda3\Lib\site-packages\pydantic\main.py:214: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.
  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
Augmenting frames: 100%|| 650/650 [00:33&lt;00:00, 19.46it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Augmentation completed! Augmented frames saved to C:\Users\krna5\Downloads\augmented_frames.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=417f419e-3d0f-49c9-91d0-af00c18a7288">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The data augmentation process completed successfully, with 650 augmented frames saved to the </span>
<span class="n">specified</span> <span class="n">directory</span><span class="o">.</span><span class="n">Analyzing</span> <span class="n">the</span> <span class="n">results</span><span class="p">,</span> <span class="n">explain</span> <span class="n">their</span> <span class="n">significance</span><span class="p">,</span> <span class="ow">and</span> <span class="n">provide</span> <span class="n">insights</span> <span class="k">for</span> <span class="n">further</span> 
<span class="n">improvement</span><span class="o">.</span>

<span class="mf">1.</span> <span class="n">Output</span> <span class="n">Analysis</span>
<span class="n">Warnings</span> <span class="n">Encountered</span><span class="p">:</span>

<span class="n">Deprecation</span> <span class="ne">Warning</span><span class="p">:</span>
<span class="n">The</span> <span class="n">Flip</span> <span class="n">augmentation</span> <span class="ow">is</span> <span class="n">deprecated</span><span class="o">.</span><span class="n">The</span> <span class="n">warning</span> <span class="n">suggests</span> <span class="n">using</span> <span class="n">specific</span> <span class="n">augmentations</span> <span class="n">like</span> <span class="n">HorizontalFlip</span><span class="p">,</span>
<span class="n">VerticalFlip</span><span class="p">,</span> <span class="ow">or</span> <span class="n">others</span> <span class="n">instead</span><span class="o">.</span> <span class="n">While</span> <span class="n">the</span> <span class="n">generic</span> <span class="n">Flip</span> <span class="n">worked</span><span class="p">,</span> <span class="n">switching</span> <span class="n">to</span> <span class="n">specific</span> <span class="n">augmentations</span> <span class="n">like</span> 
<span class="n">HorizontalFlip</span> <span class="ow">and</span> <span class="n">VerticalFlip</span> <span class="n">will</span> <span class="n">align</span> <span class="k">with</span> <span class="n">best</span> <span class="n">practices</span><span class="o">.</span>

<span class="c1"># Blur Limit Warning:</span>
<span class="n">The</span> <span class="n">GaussianBlur</span> <span class="n">default</span> <span class="n">settings</span> <span class="n">had</span> <span class="n">a</span> <span class="n">minor</span> <span class="n">conflict</span><span class="p">,</span> <span class="ow">and</span> <span class="n">Albumentations</span> <span class="n">adjusted</span> <span class="n">the</span> <span class="n">blur_limit</span> <span class="n">to</span> 
<span class="mi">3</span> <span class="n">automatically</span><span class="o">.</span> <span class="n">This</span> <span class="n">did</span> <span class="ow">not</span> <span class="n">impact</span> <span class="n">the</span> <span class="n">output</span> <span class="n">but</span> <span class="n">highlights</span> <span class="n">a</span> <span class="n">need</span> <span class="n">to</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">augmentation</span> <span class="n">settings</span><span class="o">.</span>

<span class="c1"># Performance:</span>

<span class="n">The</span> <span class="n">augmentation</span> <span class="n">pipeline</span> <span class="n">processed</span> <span class="nb">all</span> <span class="mi">650</span> <span class="n">frames</span> <span class="n">efficiently</span><span class="p">,</span> <span class="n">running</span> <span class="n">at</span> <span class="o">~</span><span class="mf">19.46</span> <span class="n">frames</span> <span class="n">per</span> 
<span class="n">second</span><span class="p">,</span> <span class="n">completing</span> <span class="ow">in</span> <span class="o">~</span><span class="mi">33</span> <span class="n">seconds</span><span class="o">.</span>
    
<span class="c1"># Diversity in Augmented Frames:</span>

<span class="n">Random</span> <span class="n">flipping</span><span class="p">,</span> <span class="n">rotation</span><span class="p">,</span> <span class="n">brightness</span><span class="o">/</span><span class="n">contrast</span> <span class="n">adjustments</span><span class="p">,</span> <span class="ow">and</span> <span class="n">Gaussian</span> <span class="n">blur</span> <span class="n">were</span> <span class="n">applied</span><span class="o">.</span>
<span class="n">These</span> <span class="n">augmentations</span> <span class="n">simulate</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">variations</span> <span class="n">like</span> <span class="n">different</span> <span class="n">lighting</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">orientations</span><span class="p">,</span> 
<span class="ow">and</span> <span class="n">minor</span> <span class="n">image</span> <span class="n">distortions</span><span class="p">,</span> <span class="n">making</span> <span class="n">the</span> <span class="n">model</span> <span class="n">robust</span> <span class="n">to</span> <span class="n">unseen</span> <span class="n">data</span><span class="o">.</span>

<span class="c1"># Significance of Data Augmentation</span>
<span class="n">Data</span> <span class="n">augmentation</span> <span class="ow">is</span> <span class="n">crucial</span> <span class="k">for</span> <span class="n">training</span> <span class="n">robust</span> <span class="n">models</span><span class="p">,</span> <span class="n">especially</span> <span class="n">when</span> <span class="n">the</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="n">limited</span> 
<span class="ow">or</span> <span class="n">lacks</span> <span class="n">diversity</span><span class="p">:</span>

<span class="c1"># Simulating Variations:</span>
<span class="n">The</span> <span class="n">augmentations</span> <span class="n">applied</span> <span class="p">(</span><span class="n">flips</span><span class="p">,</span> <span class="n">rotations</span><span class="p">,</span> <span class="n">brightness</span> <span class="n">changes</span><span class="p">)</span> <span class="n">introduce</span> <span class="n">variability</span><span class="p">,</span> <span class="n">which</span> <span class="n">helps</span>
<span class="n">the</span> <span class="n">model</span> <span class="n">generalize</span> <span class="n">better</span> <span class="n">to</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">scenarios</span><span class="o">.</span>
    
<span class="c1"># Mitigating Overfitting:</span>
<span class="n">By</span> <span class="n">artificially</span> <span class="n">increasing</span> <span class="n">the</span> <span class="n">dataset</span> <span class="n">size</span><span class="p">,</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">less</span> <span class="n">likely</span> <span class="n">to</span> <span class="n">memorize</span> <span class="n">the</span> <span class="n">training</span> 
<span class="n">data</span> <span class="ow">and</span> <span class="n">more</span> <span class="n">likely</span> <span class="n">to</span> <span class="n">learn</span> <span class="n">general</span> <span class="n">features</span><span class="o">.</span>
    
<span class="c1"># Suggestions for Improvement</span>
<span class="c1"># To make the model even more robust and outstanding:</span>

<span class="c1"># Advanced Augmentation Techniques:</span>

<span class="n">Use</span> <span class="n">augmentations</span> <span class="n">like</span><span class="p">:</span>
<span class="c1"># RandomCrop: Extract random sub-regions to simulate partial object visibility.</span>
<span class="c1"># CutOut: Mask random parts of the image to enhance robustness.</span>
<span class="c1"># ElasticTransform: Apply non-linear transformations for natural distortions.</span>
<span class="c1"># Color Jitter: Randomly change hue and saturation to simulate different environments.</span>
<span class="c1"># Update the pipeline:</span>
<span class="n">python</span> <span class="n">code</span>
<span class="n">augmentations</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">A</span><span class="o">.</span><span class="n">HorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="n">A</span><span class="o">.</span><span class="n">VerticalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="n">A</span><span class="o">.</span><span class="n">RandomRotate90</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">A</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">A</span><span class="o">.</span><span class="n">CutOut</span><span class="p">(</span><span class="n">num_holes</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_h_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_w_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">A</span><span class="o">.</span><span class="n">ElasticTransform</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">50.0</span><span class="p">,</span> <span class="n">alpha_affine</span><span class="o">=</span><span class="mf">50.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
<span class="p">])</span>
             
<span class="c1"># Balance Training Data:</span>

<span class="n">If</span> <span class="n">using</span> <span class="n">labels</span> <span class="k">for</span> <span class="nb">object</span> <span class="n">detection</span><span class="p">,</span> <span class="n">ensure</span> <span class="n">a</span> <span class="n">balanced</span> <span class="n">representation</span> <span class="n">of</span> <span class="nb">all</span> <span class="n">classes</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">training</span> <span class="nb">set</span><span class="o">.</span>
               
<span class="c1"># Add Synthetic Data:</span>

<span class="n">Use</span> <span class="n">Generative</span> <span class="n">Adversarial</span> <span class="n">Networks</span> <span class="p">(</span><span class="n">GANs</span><span class="p">)</span> <span class="ow">or</span> <span class="n">other</span> <span class="n">generative</span> <span class="n">models</span> <span class="n">to</span> <span class="n">synthesize</span> <span class="n">new</span> <span class="n">data</span> <span class="k">for</span>
<span class="n">rare</span> <span class="n">scenarios</span><span class="o">.</span>
<span class="c1"># Apply Preprocessing in Batches:</span>

<span class="n">For</span> <span class="n">large</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">process</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">batches</span> <span class="n">to</span> <span class="n">optimize</span> <span class="n">memory</span> <span class="n">usage</span> <span class="ow">and</span> <span class="n">ensure</span> <span class="n">scalability</span><span class="o">.</span>
<span class="n">Validation</span> <span class="n">of</span> <span class="n">Augmentation</span><span class="p">:</span>

<span class="n">Visualize</span> <span class="n">a</span> <span class="n">subset</span> <span class="n">of</span> <span class="n">augmented</span> <span class="n">images</span> <span class="n">to</span> <span class="n">confirm</span> <span class="n">that</span> <span class="n">the</span> <span class="n">transformations</span> <span class="n">are</span> <span class="n">meaningful</span> <span class="ow">and</span> <span class="ow">not</span>
<span class="n">overly</span> <span class="n">distorted</span><span class="o">.</span>
    
<span class="c1"># Next Steps</span>
<span class="c1"># Model Training:</span>
<span class="n">Proceed</span> <span class="n">to</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">using</span> <span class="n">the</span> <span class="n">augmented</span> <span class="n">training</span> <span class="nb">set</span><span class="o">.</span>
    
<span class="c1"># Evaluate Model Performance:</span>
<span class="n">Use</span> <span class="n">the</span> <span class="n">validation</span> <span class="ow">and</span> <span class="n">test</span> <span class="n">sets</span> <span class="n">to</span> <span class="n">assess</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s robustness and generalization capabilities.</span>
    
<span class="c1"># Iterate:</span>
<span class="n">Based</span> <span class="n">on</span> <span class="n">evaluation</span> <span class="n">results</span><span class="p">,</span> <span class="n">tweak</span> <span class="n">augmentations</span> <span class="ow">or</span> <span class="n">add</span> <span class="n">more</span> <span class="n">data</span> <span class="n">diversity</span> <span class="k">for</span> <span class="n">further</span> <span class="n">improvement</span><span class="o">.</span>
                                                                           
<span class="c1"># Conclusion</span>
<span class="n">The</span> <span class="n">data</span> <span class="n">augmentation</span> <span class="n">step</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">significant</span> <span class="n">milestone</span> <span class="ow">in</span> <span class="n">your</span> <span class="n">project</span><span class="o">.</span><span class="n">The</span> <span class="n">current</span> <span class="n">approach</span> <span class="n">introduces</span>
<span class="n">meaningful</span> <span class="n">diversity</span> <span class="n">to</span> <span class="n">the</span> <span class="n">training</span> <span class="n">data</span><span class="p">,</span> <span class="n">making</span> <span class="n">the</span> <span class="n">model</span> <span class="n">robust</span><span class="o">.</span><span class="n">Implementing</span> <span class="n">the</span> <span class="n">suggested</span> <span class="n">improvements</span>
<span class="n">will</span> <span class="n">further</span> <span class="n">enhance</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s ability to handle complex and real-world scenarios,aligning with the</span>
<span class="n">objective</span> <span class="n">of</span> <span class="n">building</span> <span class="n">an</span> <span class="n">outstanding</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">system</span> <span class="n">using</span> <span class="n">YOLO</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=43242568-0dd9-4f7d-90ac-d0a62a9ccc41">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>albumentations
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Collecting albumentations
  Downloading albumentations-1.4.22-py3-none-any.whl.metadata (33 kB)
Requirement already satisfied: numpy&gt;=1.24.4 in c:\users\krna5\anaconda3\lib\site-packages (from albumentations) (1.26.4)
Requirement already satisfied: scipy&gt;=1.10.0 in c:\users\krna5\anaconda3\lib\site-packages (from albumentations) (1.14.1)
Requirement already satisfied: PyYAML in c:\users\krna5\anaconda3\lib\site-packages (from albumentations) (6.0.2)
Collecting pydantic&gt;=2.9.2 (from albumentations)
  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)
Collecting albucore==0.0.21 (from albumentations)
  Downloading albucore-0.0.21-py3-none-any.whl.metadata (5.3 kB)
Collecting eval-type-backport (from albumentations)
  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)
Requirement already satisfied: opencv-python-headless&gt;=4.9.0.80 in c:\users\krna5\anaconda3\lib\site-packages (from albumentations) (4.10.0.84)
Collecting stringzilla&gt;=3.10.4 (from albucore==0.0.21-&gt;albumentations)
  Downloading stringzilla-3.11.0-cp312-cp312-win_amd64.whl.metadata (81 kB)
Collecting simsimd&gt;=5.9.2 (from albucore==0.0.21-&gt;albumentations)
  Downloading simsimd-6.2.1-cp312-cp312-win_amd64.whl.metadata (67 kB)
Requirement already satisfied: annotated-types&gt;=0.6.0 in c:\users\krna5\anaconda3\lib\site-packages (from pydantic&gt;=2.9.2-&gt;albumentations) (0.6.0)
Collecting pydantic-core==2.27.1 (from pydantic&gt;=2.9.2-&gt;albumentations)
  Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl.metadata (6.7 kB)
Collecting typing-extensions&gt;=4.12.2 (from pydantic&gt;=2.9.2-&gt;albumentations)
  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Downloading albumentations-1.4.22-py3-none-any.whl (258 kB)
Downloading albucore-0.0.21-py3-none-any.whl (12 kB)
Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)
Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl (2.0 MB)
   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--
   --------------------- ------------------ 1.0/2.0 MB 5.6 MB/s eta 0:00:01
   ---------------------------------------- 2.0/2.0 MB 5.2 MB/s eta 0:00:00
Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)
Downloading simsimd-6.2.1-cp312-cp312-win_amd64.whl (87 kB)
Downloading stringzilla-3.11.0-cp312-cp312-win_amd64.whl (79 kB)
Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Installing collected packages: stringzilla, simsimd, typing-extensions, eval-type-backport, pydantic-core, albucore, pydantic, albumentations
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.11.0
    Uninstalling typing_extensions-4.11.0:
      Successfully uninstalled typing_extensions-4.11.0
  Attempting uninstall: pydantic-core
    Found existing installation: pydantic_core 2.20.1
    Uninstalling pydantic_core-2.20.1:
      Successfully uninstalled pydantic_core-2.20.1
  Attempting uninstall: pydantic
    Found existing installation: pydantic 2.8.2
    Uninstalling pydantic-2.8.2:
      Successfully uninstalled pydantic-2.8.2
Successfully installed albucore-0.0.21 albumentations-1.4.22 eval-type-backport-0.2.0 pydantic-2.10.3 pydantic-core-2.27.1 simsimd-6.2.1 stringzilla-3.11.0 typing-extensions-4.12.2
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=b8b1f51b-b62b-4598-8b6a-b77d5392feb7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Advanced augmentation techniques will further improve the model's robustness by simulating real-world </span>
<span class="n">variability</span> <span class="ow">and</span> <span class="n">enhancing</span> <span class="n">the</span> <span class="n">model</span><span class="err"></span><span class="n">s</span> <span class="n">ability</span> <span class="n">to</span> <span class="n">generalize</span><span class="o">.</span> <span class="n">The</span> <span class="n">advanced</span> <span class="n">techniques</span> <span class="n">like</span> <span class="n">RandomCrop</span><span class="p">,</span><span class="n">CourseDropout</span> <span class="p">,</span> 
<span class="n">ElasticTransform</span><span class="p">,</span> <span class="ow">and</span> <span class="n">ColorJitter</span> <span class="n">will</span> <span class="n">make</span> <span class="n">your</span> <span class="n">model</span> <span class="n">more</span> <span class="n">resilient</span> <span class="n">to</span> <span class="n">various</span> <span class="n">distortions</span><span class="p">,</span> <span class="n">lighting</span> 
<span class="n">conditions</span><span class="p">,</span> <span class="ow">and</span> <span class="n">partial</span> <span class="nb">object</span> <span class="n">visibility</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6d3ec553-cf74-4e99-8085-ccbcc814c6ce">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">albumentations</span> <span class="k">as</span> <span class="nn">A</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Define your paths</span>
<span class="n">input_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\frames"</span>  <span class="c1"># Folder containing extracted frames</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\augmented_frames"</span>  <span class="c1"># Folder to save augmented frames</span>

<span class="c1"># Ensure output folder exists</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Define the augmentation pipeline with advanced techniques</span>
<span class="n">augmentations</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">A</span><span class="o">.</span><span class="n">HorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>           <span class="c1"># 30% chance to horizontally flip</span>
    <span class="n">A</span><span class="o">.</span><span class="n">VerticalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>             <span class="c1"># 30% chance to vertically flip</span>
    <span class="n">A</span><span class="o">.</span><span class="n">RandomRotate90</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>           <span class="c1"># 20% chance to randomly rotate 90 degrees</span>
    <span class="n">A</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>              <span class="c1"># 20% chance to apply color jitter (adjust brightness, contrast, saturation)</span>
    <span class="n">A</span><span class="o">.</span><span class="n">CoarseDropout</span><span class="p">(</span><span class="n">min_holes</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_holes</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">min_height</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">min_width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_height</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>  <span class="c1"># CoarseDropout</span>
    <span class="n">A</span><span class="o">.</span><span class="n">ElasticTransform</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">50.0</span><span class="p">,</span> <span class="n">alpha_affine</span><span class="o">=</span><span class="mf">50.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>  <span class="c1"># 20% chance to apply elastic transform</span>
    <span class="n">A</span><span class="o">.</span><span class="n">RandomBrightnessContrast</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>  <span class="c1"># Adjust brightness and contrast</span>
    <span class="n">A</span><span class="o">.</span><span class="n">RandomGamma</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>               <span class="c1"># Randomly adjust gamma</span>
<span class="p">])</span>

<span class="c1"># Process and augment frames</span>
<span class="n">frame_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">input_folder</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">input_folder</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">'.jpg'</span><span class="p">)]</span>

<span class="k">for</span> <span class="n">frame_path</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">frame_paths</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Augmenting frames"</span><span class="p">):</span>
    <span class="c1"># Read the image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">frame_path</span><span class="p">)</span>
    
    <span class="c1"># Apply augmentation</span>
    <span class="n">augmented</span> <span class="o">=</span> <span class="n">augmentations</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
    <span class="n">augmented_image</span> <span class="o">=</span> <span class="n">augmented</span><span class="p">[</span><span class="s1">'image'</span><span class="p">]</span>
    
    <span class="c1"># Save the augmented image</span>
    <span class="n">output_frame_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">frame_path</span><span class="p">))</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">output_frame_path</span><span class="p">,</span> <span class="n">augmented_image</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Augmentation completed! Augmented frames saved to:"</span><span class="p">,</span> <span class="n">output_folder</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\krna5\AppData\Local\Temp\ipykernel_3372\2586853078.py:20: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.
  A.ElasticTransform(alpha=1.0, sigma=50.0, alpha_affine=50.0, p=0.2),  # 20% chance to apply elastic transform
Augmenting frames: 100%|| 650/650 [00:43&lt;00:00, 14.78it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Augmentation completed! Augmented frames saved to: C:\Users\krna5\Downloads\augmented_frames
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=bd70adf9-0c58-407c-9ef7-9c03c306aefc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Interpretation and Insights:</span>
<span class="c1"># Augmentation Outcome:</span>
<span class="c1"># Augmented Data: The augmentation pipeline applied transformations such as horizontal/vertical </span>
<span class="n">flipping</span><span class="p">,</span> <span class="n">random</span> <span class="n">rotations</span><span class="p">,</span> <span class="n">color</span> <span class="n">jitter</span><span class="p">,</span> <span class="n">coarse</span> <span class="n">dropout</span> <span class="p">(</span><span class="n">masking</span> <span class="n">parts</span> <span class="n">of</span> <span class="n">the</span> <span class="n">image</span><span class="p">),</span> <span class="n">elastic</span>
<span class="n">transformations</span><span class="p">,</span> <span class="ow">and</span> <span class="n">random</span> <span class="n">brightness</span><span class="o">/</span><span class="n">contrast</span> <span class="n">adjustments</span><span class="o">.</span> <span class="n">This</span> <span class="n">process</span> <span class="n">generated</span> <span class="n">new</span> <span class="n">versions</span> 
<span class="n">of</span> <span class="n">the</span> <span class="n">training</span> <span class="n">images</span><span class="p">,</span> <span class="n">increasing</span> <span class="n">data</span> <span class="n">diversity</span><span class="o">.</span>
    
<span class="c1"># Improved Generalization:</span>
<span class="n">By</span> <span class="n">applying</span> <span class="n">various</span> <span class="n">augmentation</span> <span class="n">techniques</span><span class="p">,</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">exposed</span> <span class="n">to</span> <span class="n">more</span> <span class="n">varied</span> <span class="n">training</span> <span class="n">examples</span><span class="p">,</span> 
<span class="n">which</span> <span class="n">helps</span> <span class="n">it</span> <span class="n">generalize</span> <span class="n">better</span> <span class="n">to</span> <span class="n">unseen</span> <span class="n">data</span><span class="o">.</span> <span class="n">It</span> <span class="n">can</span> <span class="n">learn</span> <span class="n">to</span> <span class="n">detect</span> <span class="n">objects</span> <span class="n">under</span> <span class="n">different</span> 
<span class="n">orientations</span><span class="p">,</span> <span class="n">lighting</span> <span class="n">conditions</span><span class="p">,</span> <span class="ow">and</span> <span class="n">partial</span> <span class="n">occlusions</span><span class="p">,</span> <span class="n">making</span> <span class="n">it</span> <span class="n">more</span> <span class="n">robust</span><span class="o">.</span>
    
<span class="c1"># Key Benefits:</span>
<span class="c1"># Increased Robustness: </span>
<span class="n">The</span> <span class="n">model</span> <span class="n">will</span> <span class="n">be</span> <span class="n">more</span> <span class="n">robust</span> <span class="ow">in</span> <span class="n">recognizing</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">real</span><span class="o">-</span><span class="n">world</span> <span class="n">scenarios</span><span class="p">,</span> <span class="n">where</span> <span class="n">variations</span> 
<span class="n">like</span> <span class="n">partial</span> <span class="nb">object</span> <span class="n">visibility</span><span class="p">,</span> <span class="n">lighting</span> <span class="n">changes</span><span class="p">,</span> <span class="ow">and</span> <span class="n">rotations</span> <span class="n">are</span> <span class="n">common</span><span class="o">.</span>
    
<span class="c1"># Synthetic Data Generation:</span>
<span class="n">Augmentation</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">form</span> <span class="n">of</span> <span class="n">creating</span> <span class="n">synthetic</span> <span class="n">data</span><span class="p">,</span> <span class="n">allowing</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">learn</span> <span class="n">better</span> <span class="kn">from</span> <span class="nn">limited</span> 
<span class="n">original</span> <span class="n">frames</span><span class="p">,</span> <span class="n">preventing</span> <span class="n">overfitting</span><span class="o">.</span>
<span class="c1"># Suggestions for Improvement:</span>
<span class="n">Incorporate</span> <span class="n">More</span> <span class="n">Complex</span> <span class="n">Augmentations</span><span class="p">:</span>

<span class="n">You</span> <span class="n">can</span> <span class="n">further</span> <span class="n">experiment</span> <span class="k">with</span> <span class="n">more</span> <span class="n">sophisticated</span> <span class="n">augmentations</span> <span class="n">such</span> <span class="k">as</span> <span class="n">RandomCrop</span><span class="p">,</span> <span class="n">RandomSizedCrop</span><span class="p">,</span>
<span class="ow">and</span> <span class="n">GaussianBlur</span> <span class="n">to</span> <span class="n">simulate</span> <span class="n">more</span> <span class="n">realistic</span> <span class="ow">and</span> <span class="n">challenging</span> <span class="n">scenarios</span><span class="o">.</span><span class="n">Mixup</span> <span class="ow">or</span> <span class="n">CutMix</span> <span class="n">can</span> <span class="n">also</span> <span class="n">be</span> <span class="n">explored</span><span class="o">.</span> 
<span class="n">These</span> <span class="n">augmentations</span> <span class="n">blend</span> <span class="n">two</span> <span class="n">images</span> <span class="ow">and</span> <span class="n">their</span> <span class="n">labels</span><span class="p">,</span> <span class="n">forcing</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">learn</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">combined</span> <span class="n">information</span><span class="o">.</span>
<span class="c1"># Fine-Tuning Pretrained Models:</span>

<span class="n">For</span> <span class="n">YOLO</span> <span class="ow">or</span> <span class="n">Faster</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span><span class="p">,</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">weights</span> <span class="n">on</span> <span class="n">your</span> <span class="n">dataset</span><span class="o">.</span><span class="n">This</span> <span class="n">can</span> <span class="n">help</span> <span class="n">the</span> <span class="n">model</span> <span class="n">adapt</span> 
<span class="n">better</span> <span class="n">to</span> <span class="n">your</span> <span class="n">specific</span> <span class="n">domain</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="nb">object</span> <span class="n">types</span> <span class="ow">or</span> <span class="n">video</span> <span class="n">frames</span><span class="p">)</span> <span class="n">by</span> <span class="n">transferring</span> <span class="n">learned</span> <span class="n">features</span> <span class="kn">from</span> <span class="nn">a</span>
<span class="n">larger</span> <span class="n">dataset</span><span class="o">.</span>
<span class="c1"># Hyperparameter Tuning:</span>

<span class="n">After</span> <span class="n">training</span><span class="p">,</span> <span class="n">experiment</span> <span class="k">with</span> <span class="n">learning</span> <span class="n">rate</span> <span class="n">schedules</span><span class="p">,</span> <span class="n">batch</span> <span class="n">sizes</span><span class="p">,</span> <span class="ow">and</span> <span class="n">other</span> <span class="n">model</span><span class="o">-</span><span class="n">specific</span> <span class="n">hyperparameters</span><span class="o">.</span>
<span class="n">For</span> <span class="n">YOLO</span><span class="p">,</span> <span class="n">consider</span> <span class="n">experimenting</span> <span class="k">with</span> <span class="n">the</span> <span class="n">confidence</span> <span class="n">threshold</span> <span class="ow">and</span> <span class="n">non</span><span class="o">-</span><span class="nb">max</span> <span class="n">suppression</span> <span class="n">parameters</span> <span class="n">to</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span> 
<span class="nb">object</span> <span class="n">detection</span> <span class="n">performance</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0084e8c9-99da-42ed-ad8e-2d2ab4a6634e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 5.Loading Pre-trained Weights Download pre-trained weights for the model:</span>
<span class="n">YOLO</span><span class="p">:</span> <span class="n">Pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">weights</span> <span class="kn">from</span> <span class="nn">Ultralytics</span> <span class="ow">or</span> <span class="n">other</span> <span class="n">sources</span>
<span class="c1"># loading a YOLO pre-trained model, processes the video (30952-383991415_small.mp4), and outputs an</span>
<span class="n">annotated</span> <span class="n">video</span> <span class="k">with</span> <span class="n">detected</span> <span class="n">objects</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0dceff96-62f3-4157-b728-fa2d4a9293ec">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Paths</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\30952-383991415_small.mp4"</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\detected_video.mp4"</span>

<span class="c1"># Load YOLO model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">'yolov5s.pt'</span><span class="p">)</span>  <span class="c1"># Pre-trained YOLOv5 small model</span>

<span class="c1"># Initialize video capture</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>

<span class="c1"># Video properties</span>
<span class="n">width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">CAP_PROP_FRAME_WIDTH</span><span class="p">))</span>
<span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">CAP_PROP_FRAME_HEIGHT</span><span class="p">))</span>
<span class="n">fps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">CAP_PROP_FPS</span><span class="p">))</span>
<span class="n">total_frames</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">CAP_PROP_FRAME_COUNT</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Video Properties - Width: </span><span class="si">{</span><span class="n">width</span><span class="si">}</span><span class="s2">, Height: </span><span class="si">{</span><span class="n">height</span><span class="si">}</span><span class="s2">, FPS: </span><span class="si">{</span><span class="n">fps</span><span class="si">}</span><span class="s2">, Total Frames: </span><span class="si">{</span><span class="n">total_frames</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Initialize video writer</span>
<span class="n">fourcc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoWriter_fourcc</span><span class="p">(</span><span class="o">*</span><span class="s1">'mp4v'</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoWriter</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">fourcc</span><span class="p">,</span> <span class="n">fps</span><span class="p">,</span> <span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">))</span>

<span class="c1"># Process video frames</span>
<span class="n">frame_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Perform object detection</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>  <span class="c1"># Confidence threshold at 40%</span>

    <span class="c1"># Render the results on the frame</span>
    <span class="n">annotated_frame</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

    <span class="c1"># Write the annotated frame to the output video</span>
    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">annotated_frame</span><span class="p">)</span>

    <span class="c1"># Optional: Display progress in the console</span>
    <span class="n">frame_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Processed frame </span><span class="si">{</span><span class="n">frame_count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total_frames</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="c1"># Optional: Display the frame in real-time (for testing purposes)</span>
    <span class="c1"># cv2.imshow('Object Detection', annotated_frame)</span>
    <span class="c1"># if cv2.waitKey(1) &amp; 0xFF == ord('q'):</span>
    <span class="c1">#     break</span>

<span class="c1"># Release resources</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Object detection completed. Annotated video saved at </span><span class="si">{</span><span class="n">output_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.
YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.

Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt to 'yolov5su.pt'...
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>100%|| 17.7M/17.7M [00:02&lt;00:00, 7.05MB/s]
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Video Properties - Width: 960, Height: 540, FPS: 25, Total Frames: 650

0: 384x640 10 persons, 3 cars, 1 truck, 1 backpack, 2 bottles, 511.9ms
Speed: 17.7ms preprocess, 511.9ms inference, 26.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 1/650

0: 384x640 10 persons, 3 cars, 1 truck, 2 bottles, 362.1ms
Speed: 11.0ms preprocess, 362.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 2/650

0: 384x640 10 persons, 4 cars, 1 truck, 2 bottles, 305.9ms
Speed: 5.0ms preprocess, 305.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 3/650

0: 384x640 10 persons, 5 cars, 1 truck, 2 bottles, 282.9ms
Speed: 6.1ms preprocess, 282.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 4/650

0: 384x640 11 persons, 5 cars, 1 truck, 1 backpack, 2 bottles, 308.0ms
Speed: 6.0ms preprocess, 308.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 5/650

0: 384x640 11 persons, 4 cars, 1 truck, 2 bottles, 308.6ms
Speed: 5.0ms preprocess, 308.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 6/650

0: 384x640 11 persons, 4 cars, 1 truck, 2 bottles, 320.5ms
Speed: 5.0ms preprocess, 320.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 7/650

0: 384x640 10 persons, 4 cars, 1 truck, 2 bottles, 299.3ms
Speed: 7.2ms preprocess, 299.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 8/650

0: 384x640 9 persons, 5 cars, 1 truck, 2 bottles, 311.9ms
Speed: 8.0ms preprocess, 311.9ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 9/650

0: 384x640 9 persons, 4 cars, 2 bottles, 312.7ms
Speed: 6.2ms preprocess, 312.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 10/650

0: 384x640 9 persons, 5 cars, 2 bottles, 310.6ms
Speed: 5.6ms preprocess, 310.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 11/650

0: 384x640 9 persons, 5 cars, 2 bottles, 385.1ms
Speed: 11.4ms preprocess, 385.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 12/650

0: 384x640 7 persons, 5 cars, 2 bottles, 418.3ms
Speed: 5.0ms preprocess, 418.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 13/650

0: 384x640 7 persons, 4 cars, 1 truck, 2 bottles, 385.3ms
Speed: 7.0ms preprocess, 385.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 14/650

0: 384x640 7 persons, 4 cars, 1 truck, 2 bottles, 399.6ms
Speed: 5.0ms preprocess, 399.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 15/650

0: 384x640 7 persons, 4 cars, 1 truck, 2 bottles, 541.1ms
Speed: 17.0ms preprocess, 541.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 16/650

0: 384x640 8 persons, 4 cars, 1 truck, 2 bottles, 319.4ms
Speed: 7.1ms preprocess, 319.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 17/650

0: 384x640 8 persons, 4 cars, 1 truck, 2 bottles, 384.6ms
Speed: 7.6ms preprocess, 384.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 18/650

0: 384x640 7 persons, 4 cars, 1 truck, 2 bottles, 414.0ms
Speed: 8.3ms preprocess, 414.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 19/650

0: 384x640 7 persons, 4 cars, 1 truck, 1 bottle, 430.5ms
Speed: 7.0ms preprocess, 430.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 20/650

0: 384x640 7 persons, 4 cars, 1 bus, 2 bottles, 362.0ms
Speed: 7.0ms preprocess, 362.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 21/650

0: 384x640 7 persons, 4 cars, 1 truck, 340.1ms
Speed: 8.6ms preprocess, 340.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 22/650

0: 384x640 8 persons, 4 cars, 1 truck, 382.4ms
Speed: 5.0ms preprocess, 382.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 23/650

0: 384x640 8 persons, 3 cars, 1 motorcycle, 1 truck, 1 bottle, 382.7ms
Speed: 5.6ms preprocess, 382.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 24/650

0: 384x640 7 persons, 3 cars, 1 bus, 1 bottle, 1 tv, 403.4ms
Speed: 5.0ms preprocess, 403.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 25/650

0: 384x640 7 persons, 3 cars, 1 truck, 1 tv, 408.2ms
Speed: 4.0ms preprocess, 408.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 26/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 bottle, 1 tv, 336.3ms
Speed: 5.0ms preprocess, 336.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 27/650

0: 384x640 8 persons, 3 cars, 1 motorcycle, 1 bus, 1 backpack, 1 tv, 298.6ms
Speed: 5.2ms preprocess, 298.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 28/650

0: 384x640 8 persons, 3 cars, 1 bus, 1 truck, 1 backpack, 1 tv, 283.9ms
Speed: 4.6ms preprocess, 283.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 29/650

0: 384x640 8 persons, 4 cars, 1 truck, 1 backpack, 1 tv, 280.8ms
Speed: 5.0ms preprocess, 280.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 30/650

0: 384x640 8 persons, 4 cars, 1 motorcycle, 1 bus, 1 truck, 1 backpack, 1 tv, 262.9ms
Speed: 5.0ms preprocess, 262.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 31/650

0: 384x640 8 persons, 3 cars, 1 bus, 1 backpack, 1 tv, 297.0ms
Speed: 5.0ms preprocess, 297.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 32/650

0: 384x640 8 persons, 3 cars, 1 bus, 1 truck, 1 tv, 277.7ms
Speed: 4.0ms preprocess, 277.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 33/650

0: 384x640 8 persons, 3 cars, 1 bus, 1 truck, 1 tv, 287.8ms
Speed: 5.0ms preprocess, 287.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 34/650

0: 384x640 8 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 1 tv, 283.1ms
Speed: 5.0ms preprocess, 283.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 35/650

0: 384x640 8 persons, 3 cars, 1 bus, 1 truck, 1 tv, 262.6ms
Speed: 5.0ms preprocess, 262.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 36/650

0: 384x640 8 persons, 5 cars, 1 bus, 1 tv, 300.8ms
Speed: 5.0ms preprocess, 300.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 37/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 tv, 282.7ms
Speed: 5.0ms preprocess, 282.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 38/650

0: 384x640 10 persons, 4 cars, 1 bus, 1 tv, 331.0ms
Speed: 5.0ms preprocess, 331.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 39/650

0: 384x640 10 persons, 4 cars, 1 bus, 1 tv, 304.2ms
Speed: 5.0ms preprocess, 304.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 40/650

0: 384x640 9 persons, 5 cars, 1 bus, 1 truck, 1 tv, 281.7ms
Speed: 5.0ms preprocess, 281.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 41/650

0: 384x640 11 persons, 4 cars, 1 bus, 1 tv, 328.9ms
Speed: 5.0ms preprocess, 328.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 42/650

0: 384x640 11 persons, 4 cars, 1 bus, 1 tv, 289.2ms
Speed: 5.0ms preprocess, 289.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 43/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 tv, 336.2ms
Speed: 5.0ms preprocess, 336.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 44/650

0: 384x640 10 persons, 3 cars, 1 bus, 1 tv, 353.8ms
Speed: 6.0ms preprocess, 353.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 45/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 tv, 354.7ms
Speed: 5.0ms preprocess, 354.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 46/650

0: 384x640 8 persons, 1 car, 1 bus, 1 tv, 335.2ms
Speed: 6.0ms preprocess, 335.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 47/650

0: 384x640 8 persons, 1 car, 1 bus, 1 tv, 324.7ms
Speed: 4.0ms preprocess, 324.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 48/650

0: 384x640 8 persons, 1 car, 1 bus, 1 tv, 297.2ms
Speed: 5.0ms preprocess, 297.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 49/650

0: 384x640 9 persons, 1 car, 1 bus, 1 tv, 282.8ms
Speed: 5.0ms preprocess, 282.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 50/650

0: 384x640 8 persons, 1 car, 1 bus, 1 tv, 287.6ms
Speed: 4.0ms preprocess, 287.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 51/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 tv, 325.3ms
Speed: 5.0ms preprocess, 325.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 52/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 tv, 321.4ms
Speed: 5.5ms preprocess, 321.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 53/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 tv, 303.6ms
Speed: 5.0ms preprocess, 303.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 54/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 292.7ms
Speed: 5.0ms preprocess, 292.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 55/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 331.4ms
Speed: 5.0ms preprocess, 331.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 56/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 326.6ms
Speed: 4.0ms preprocess, 326.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 57/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 308.6ms
Speed: 6.0ms preprocess, 308.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 58/650

0: 384x640 8 persons, 1 car, 1 bus, 1 backpack, 1 tv, 484.1ms
Speed: 4.1ms preprocess, 484.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 59/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 327.8ms
Speed: 6.0ms preprocess, 327.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 60/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 319.6ms
Speed: 5.0ms preprocess, 319.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 61/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 334.6ms
Speed: 5.0ms preprocess, 334.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 62/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 302.8ms
Speed: 5.0ms preprocess, 302.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 63/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 281.1ms
Speed: 5.4ms preprocess, 281.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 64/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 312.6ms
Speed: 6.0ms preprocess, 312.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 65/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 tv, 310.7ms
Speed: 4.0ms preprocess, 310.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 66/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 314.0ms
Speed: 5.0ms preprocess, 314.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 67/650

0: 384x640 9 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 324.0ms
Speed: 5.0ms preprocess, 324.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 68/650

0: 384x640 10 persons, 2 cars, 1 bus, 1 backpack, 1 tv, 323.6ms
Speed: 4.0ms preprocess, 323.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 69/650

0: 384x640 10 persons, 2 cars, 1 bus, 333.5ms
Speed: 5.0ms preprocess, 333.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 70/650

0: 384x640 10 persons, 2 cars, 1 bus, 1 handbag, 338.2ms
Speed: 6.0ms preprocess, 338.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 71/650

0: 384x640 9 persons, 1 car, 1 bus, 2 handbags, 292.8ms
Speed: 5.0ms preprocess, 292.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 72/650

0: 384x640 9 persons, 1 car, 1 bus, 1 handbag, 310.4ms
Speed: 4.0ms preprocess, 310.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 73/650

0: 384x640 9 persons, 1 car, 1 bus, 2 handbags, 326.9ms
Speed: 4.0ms preprocess, 326.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 74/650

0: 384x640 9 persons, 1 bus, 1 handbag, 349.0ms
Speed: 6.4ms preprocess, 349.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 75/650

0: 384x640 8 persons, 1 bus, 1 handbag, 371.1ms
Speed: 4.3ms preprocess, 371.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 76/650

0: 384x640 10 persons, 1 bus, 1 handbag, 321.9ms
Speed: 5.1ms preprocess, 321.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 77/650

0: 384x640 9 persons, 1 bus, 1 handbag, 293.3ms
Speed: 5.0ms preprocess, 293.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 78/650

0: 384x640 9 persons, 1 bus, 1 handbag, 325.0ms
Speed: 5.0ms preprocess, 325.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 79/650

0: 384x640 9 persons, 1 bus, 1 handbag, 296.5ms
Speed: 4.0ms preprocess, 296.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 80/650

0: 384x640 9 persons, 1 car, 1 bus, 1 handbag, 302.1ms
Speed: 4.0ms preprocess, 302.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 81/650

0: 384x640 8 persons, 2 cars, 1 bus, 1 handbag, 316.4ms
Speed: 5.0ms preprocess, 316.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 82/650

0: 384x640 9 persons, 1 car, 1 bus, 1 handbag, 284.2ms
Speed: 6.1ms preprocess, 284.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 83/650

0: 384x640 9 persons, 1 car, 1 bus, 1 handbag, 304.4ms
Speed: 4.0ms preprocess, 304.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 84/650

0: 384x640 9 persons, 1 car, 1 bus, 1 handbag, 322.3ms
Speed: 6.1ms preprocess, 322.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 85/650

0: 384x640 9 persons, 1 car, 1 bus, 1 handbag, 302.1ms
Speed: 7.0ms preprocess, 302.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 86/650

0: 384x640 10 persons, 2 cars, 1 bus, 1 handbag, 290.3ms
Speed: 5.0ms preprocess, 290.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 87/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 305.8ms
Speed: 5.0ms preprocess, 305.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 88/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 331.7ms
Speed: 5.0ms preprocess, 331.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 89/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 354.6ms
Speed: 4.0ms preprocess, 354.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 90/650

0: 384x640 10 persons, 4 cars, 1 bus, 355.9ms
Speed: 4.0ms preprocess, 355.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 91/650

0: 384x640 9 persons, 5 cars, 1 bus, 2 handbags, 357.4ms
Speed: 4.9ms preprocess, 357.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 92/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 362.0ms
Speed: 6.2ms preprocess, 362.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 93/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 336.0ms
Speed: 5.0ms preprocess, 336.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 94/650

0: 384x640 10 persons, 4 cars, 1 bus, 2 handbags, 294.9ms
Speed: 5.0ms preprocess, 294.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 95/650

0: 384x640 10 persons, 4 cars, 1 bus, 2 handbags, 442.4ms
Speed: 12.0ms preprocess, 442.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 96/650

0: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 329.8ms
Speed: 5.6ms preprocess, 329.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 97/650

0: 384x640 11 persons, 6 cars, 1 bus, 2 handbags, 324.7ms
Speed: 4.0ms preprocess, 324.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 98/650

0: 384x640 11 persons, 6 cars, 1 bus, 2 handbags, 314.5ms
Speed: 5.0ms preprocess, 314.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 99/650

0: 384x640 11 persons, 4 cars, 1 bus, 2 handbags, 326.2ms
Speed: 5.0ms preprocess, 326.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 100/650

0: 384x640 10 persons, 5 cars, 1 bus, 1 handbag, 326.8ms
Speed: 4.0ms preprocess, 326.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 101/650

0: 384x640 10 persons, 4 cars, 1 bus, 2 handbags, 344.7ms
Speed: 5.7ms preprocess, 344.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 102/650

0: 384x640 10 persons, 5 cars, 1 bus, 2 handbags, 307.3ms
Speed: 6.0ms preprocess, 307.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 103/650

0: 384x640 10 persons, 5 cars, 1 bus, 3 handbags, 343.3ms
Speed: 5.0ms preprocess, 343.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 104/650

0: 384x640 11 persons, 5 cars, 1 bus, 1 handbag, 382.4ms
Speed: 5.5ms preprocess, 382.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 105/650

0: 384x640 12 persons, 7 cars, 1 bus, 1 handbag, 366.3ms
Speed: 6.0ms preprocess, 366.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 106/650

0: 384x640 10 persons, 5 cars, 1 bus, 2 handbags, 305.3ms
Speed: 5.0ms preprocess, 305.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 107/650

0: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 326.0ms
Speed: 5.0ms preprocess, 326.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 108/650

0: 384x640 12 persons, 5 cars, 1 bus, 1 handbag, 326.0ms
Speed: 5.0ms preprocess, 326.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 109/650

0: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 328.2ms
Speed: 6.0ms preprocess, 328.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 110/650

0: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 319.5ms
Speed: 5.0ms preprocess, 319.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 111/650

0: 384x640 12 persons, 5 cars, 1 bus, 1 handbag, 338.5ms
Speed: 5.0ms preprocess, 338.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 112/650

0: 384x640 12 persons, 5 cars, 1 bus, 1 handbag, 333.4ms
Speed: 5.0ms preprocess, 333.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 113/650

0: 384x640 11 persons, 5 cars, 1 bus, 1 handbag, 327.6ms
Speed: 5.0ms preprocess, 327.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 114/650

0: 384x640 10 persons, 5 cars, 1 bus, 2 handbags, 305.9ms
Speed: 5.0ms preprocess, 305.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 115/650

0: 384x640 10 persons, 5 cars, 1 bus, 1 handbag, 320.9ms
Speed: 8.1ms preprocess, 320.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 116/650

0: 384x640 9 persons, 5 cars, 1 bus, 1 handbag, 300.8ms
Speed: 4.5ms preprocess, 300.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 117/650

0: 384x640 9 persons, 5 cars, 1 bus, 1 handbag, 343.6ms
Speed: 4.0ms preprocess, 343.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 118/650

0: 384x640 9 persons, 5 cars, 1 bus, 1 handbag, 340.2ms
Speed: 5.0ms preprocess, 340.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 119/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 342.0ms
Speed: 4.0ms preprocess, 342.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 120/650

0: 384x640 9 persons, 4 cars, 1 bus, 1 handbag, 301.2ms
Speed: 5.0ms preprocess, 301.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 121/650

0: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 324.3ms
Speed: 5.0ms preprocess, 324.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 122/650

0: 384x640 11 persons, 3 cars, 1 bus, 1 handbag, 478.8ms
Speed: 4.0ms preprocess, 478.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 123/650

0: 384x640 11 persons, 3 cars, 1 bus, 1 handbag, 301.6ms
Speed: 6.0ms preprocess, 301.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 124/650

0: 384x640 12 persons, 3 cars, 1 motorcycle, 1 bus, 1 handbag, 309.1ms
Speed: 5.0ms preprocess, 309.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 125/650

0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 316.6ms
Speed: 6.0ms preprocess, 316.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 126/650

0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 317.0ms
Speed: 5.0ms preprocess, 317.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 127/650

0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 312.7ms
Speed: 4.0ms preprocess, 312.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 128/650

0: 384x640 13 persons, 5 cars, 1 motorcycle, 1 bus, 1 handbag, 341.7ms
Speed: 7.1ms preprocess, 341.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 129/650

0: 384x640 14 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 342.0ms
Speed: 4.0ms preprocess, 342.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 130/650

0: 384x640 14 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 313.1ms
Speed: 5.0ms preprocess, 313.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 131/650

0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 327.1ms
Speed: 5.0ms preprocess, 327.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 132/650

0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 319.4ms
Speed: 5.0ms preprocess, 319.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 133/650

0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 351.3ms
Speed: 5.6ms preprocess, 351.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 134/650

0: 384x640 13 persons, 3 cars, 1 motorcycle, 1 bus, 1 handbag, 362.2ms
Speed: 5.0ms preprocess, 362.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 135/650

0: 384x640 12 persons, 3 cars, 1 motorcycle, 1 bus, 1 handbag, 333.7ms
Speed: 4.0ms preprocess, 333.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 136/650

0: 384x640 11 persons, 3 cars, 1 bus, 1 truck, 1 handbag, 334.1ms
Speed: 4.0ms preprocess, 334.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 137/650

0: 384x640 12 persons, 3 cars, 1 bus, 1 truck, 1 handbag, 300.1ms
Speed: 4.0ms preprocess, 300.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 138/650

0: 384x640 12 persons, 3 cars, 1 bus, 1 truck, 1 handbag, 326.0ms
Speed: 5.0ms preprocess, 326.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 139/650

0: 384x640 10 persons, 3 cars, 1 motorcycle, 1 bus, 1 truck, 1 handbag, 318.2ms
Speed: 4.9ms preprocess, 318.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 140/650

0: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 1 handbag, 337.5ms
Speed: 5.0ms preprocess, 337.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 141/650

0: 384x640 12 persons, 5 cars, 1 bus, 1 handbag, 314.7ms
Speed: 5.0ms preprocess, 314.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 142/650

0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 truck, 1 handbag, 347.7ms
Speed: 4.0ms preprocess, 347.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 143/650

0: 384x640 12 persons, 4 cars, 1 motorcycle, 2 trucks, 1 handbag, 329.7ms
Speed: 6.0ms preprocess, 329.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 144/650

0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 handbag, 488.7ms
Speed: 5.0ms preprocess, 488.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 145/650

0: 384x640 10 persons, 5 cars, 1 motorcycle, 1 bus, 1 truck, 1 handbag, 298.3ms
Speed: 5.0ms preprocess, 298.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 146/650

0: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 1 handbag, 315.6ms
Speed: 4.0ms preprocess, 315.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 147/650

0: 384x640 9 persons, 5 cars, 1 bus, 1 truck, 1 handbag, 331.0ms
Speed: 4.0ms preprocess, 331.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 148/650

0: 384x640 8 persons, 4 cars, 1 motorcycle, 1 bus, 1 truck, 1 handbag, 327.0ms
Speed: 5.0ms preprocess, 327.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 149/650

0: 384x640 9 persons, 5 cars, 1 bus, 1 truck, 1 handbag, 332.9ms
Speed: 4.0ms preprocess, 332.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 150/650

0: 384x640 8 persons, 5 cars, 1 bus, 1 truck, 1 handbag, 319.7ms
Speed: 5.2ms preprocess, 319.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 151/650

0: 384x640 10 persons, 5 cars, 1 bus, 1 truck, 331.2ms
Speed: 5.0ms preprocess, 331.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 152/650

0: 384x640 10 persons, 5 cars, 1 bus, 1 truck, 1 handbag, 334.1ms
Speed: 5.0ms preprocess, 334.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 153/650

0: 384x640 10 persons, 5 cars, 1 bus, 1 handbag, 352.4ms
Speed: 6.0ms preprocess, 352.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 154/650

0: 384x640 10 persons, 6 cars, 1 bus, 335.1ms
Speed: 4.9ms preprocess, 335.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 155/650

0: 384x640 10 persons, 6 cars, 1 bus, 407.9ms
Speed: 5.0ms preprocess, 407.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 156/650

0: 384x640 11 persons, 6 cars, 1 bus, 350.2ms
Speed: 5.1ms preprocess, 350.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 157/650

0: 384x640 11 persons, 7 cars, 1 bus, 317.0ms
Speed: 4.0ms preprocess, 317.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 158/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 1 bus, 332.1ms
Speed: 4.0ms preprocess, 332.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 159/650

0: 384x640 10 persons, 5 cars, 1 bus, 308.9ms
Speed: 5.0ms preprocess, 308.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 160/650

0: 384x640 9 persons, 5 cars, 1 motorcycle, 1 bus, 334.2ms
Speed: 4.0ms preprocess, 334.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 161/650

0: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 350.1ms
Speed: 4.0ms preprocess, 350.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 162/650

0: 384x640 10 persons, 5 cars, 1 bus, 378.0ms
Speed: 3.9ms preprocess, 378.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 163/650

0: 384x640 9 persons, 6 cars, 1 bus, 583.3ms
Speed: 5.0ms preprocess, 583.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 164/650

0: 384x640 10 persons, 7 cars, 1 bus, 384.1ms
Speed: 7.1ms preprocess, 384.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 165/650

0: 384x640 10 persons, 7 cars, 1 bus, 315.2ms
Speed: 5.0ms preprocess, 315.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 166/650

0: 384x640 10 persons, 7 cars, 1 bus, 314.4ms
Speed: 5.0ms preprocess, 314.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 167/650

0: 384x640 10 persons, 6 cars, 1 bus, 307.0ms
Speed: 5.0ms preprocess, 307.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 168/650

0: 384x640 10 persons, 8 cars, 1 bus, 305.9ms
Speed: 4.0ms preprocess, 305.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 169/650

0: 384x640 10 persons, 8 cars, 1 bus, 332.4ms
Speed: 5.0ms preprocess, 332.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 170/650

0: 384x640 9 persons, 8 cars, 309.0ms
Speed: 4.0ms preprocess, 309.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 171/650

0: 384x640 10 persons, 8 cars, 1 bus, 330.9ms
Speed: 5.0ms preprocess, 330.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 172/650

0: 384x640 9 persons, 9 cars, 1 bus, 331.1ms
Speed: 5.0ms preprocess, 331.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 173/650

0: 384x640 9 persons, 8 cars, 322.0ms
Speed: 4.0ms preprocess, 322.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 174/650

0: 384x640 9 persons, 8 cars, 323.0ms
Speed: 5.0ms preprocess, 323.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 175/650

0: 384x640 10 persons, 8 cars, 321.1ms
Speed: 5.0ms preprocess, 321.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 176/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 306.9ms
Speed: 4.0ms preprocess, 306.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 177/650

0: 384x640 9 persons, 7 cars, 1 motorcycle, 297.1ms
Speed: 4.0ms preprocess, 297.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 178/650

0: 384x640 9 persons, 7 cars, 323.6ms
Speed: 4.0ms preprocess, 323.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 179/650

0: 384x640 9 persons, 8 cars, 291.2ms
Speed: 4.0ms preprocess, 291.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 180/650

0: 384x640 9 persons, 8 cars, 376.0ms
Speed: 4.0ms preprocess, 376.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 181/650

0: 384x640 8 persons, 8 cars, 372.2ms
Speed: 15.0ms preprocess, 372.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 182/650

0: 384x640 8 persons, 8 cars, 347.3ms
Speed: 4.0ms preprocess, 347.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 183/650

0: 384x640 9 persons, 8 cars, 334.8ms
Speed: 5.0ms preprocess, 334.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 184/650

0: 384x640 9 persons, 8 cars, 296.9ms
Speed: 4.0ms preprocess, 296.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 185/650

0: 384x640 9 persons, 8 cars, 308.4ms
Speed: 4.0ms preprocess, 308.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 186/650

0: 384x640 8 persons, 8 cars, 308.1ms
Speed: 5.0ms preprocess, 308.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 187/650

0: 384x640 8 persons, 8 cars, 333.7ms
Speed: 5.0ms preprocess, 333.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 188/650

0: 384x640 8 persons, 7 cars, 333.1ms
Speed: 4.0ms preprocess, 333.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 189/650

0: 384x640 8 persons, 8 cars, 330.5ms
Speed: 5.0ms preprocess, 330.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 190/650

0: 384x640 7 persons, 8 cars, 328.6ms
Speed: 4.0ms preprocess, 328.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 191/650

0: 384x640 10 persons, 8 cars, 361.7ms
Speed: 4.0ms preprocess, 361.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 192/650

0: 384x640 10 persons, 8 cars, 378.1ms
Speed: 4.0ms preprocess, 378.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 193/650

0: 384x640 10 persons, 8 cars, 385.5ms
Speed: 5.0ms preprocess, 385.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 194/650

0: 384x640 11 persons, 8 cars, 341.1ms
Speed: 5.5ms preprocess, 341.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 195/650

0: 384x640 10 persons, 8 cars, 319.7ms
Speed: 5.0ms preprocess, 319.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 196/650

0: 384x640 9 persons, 8 cars, 317.3ms
Speed: 5.0ms preprocess, 317.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 197/650

0: 384x640 10 persons, 8 cars, 336.0ms
Speed: 4.0ms preprocess, 336.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 198/650

0: 384x640 11 persons, 8 cars, 468.5ms
Speed: 4.0ms preprocess, 468.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 199/650

0: 384x640 11 persons, 8 cars, 308.5ms
Speed: 4.0ms preprocess, 308.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 200/650

0: 384x640 10 persons, 8 cars, 331.9ms
Speed: 5.0ms preprocess, 331.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 201/650

0: 384x640 9 persons, 9 cars, 316.3ms
Speed: 4.0ms preprocess, 316.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 202/650

0: 384x640 9 persons, 8 cars, 1 truck, 294.2ms
Speed: 4.0ms preprocess, 294.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 203/650

0: 384x640 9 persons, 8 cars, 315.2ms
Speed: 5.0ms preprocess, 315.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 204/650

0: 384x640 7 persons, 8 cars, 345.4ms
Speed: 4.0ms preprocess, 345.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 205/650

0: 384x640 8 persons, 8 cars, 340.4ms
Speed: 5.0ms preprocess, 340.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 206/650

0: 384x640 8 persons, 8 cars, 1 truck, 328.9ms
Speed: 5.0ms preprocess, 328.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 207/650

0: 384x640 8 persons, 8 cars, 1 truck, 335.6ms
Speed: 5.0ms preprocess, 335.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 208/650

0: 384x640 8 persons, 9 cars, 337.5ms
Speed: 4.0ms preprocess, 337.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 209/650

0: 384x640 8 persons, 8 cars, 330.7ms
Speed: 4.0ms preprocess, 330.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 210/650

0: 384x640 9 persons, 8 cars, 325.8ms
Speed: 4.0ms preprocess, 325.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 211/650

0: 384x640 9 persons, 8 cars, 331.6ms
Speed: 5.0ms preprocess, 331.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 212/650

0: 384x640 9 persons, 9 cars, 316.2ms
Speed: 5.0ms preprocess, 316.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 213/650

0: 384x640 8 persons, 9 cars, 310.2ms
Speed: 5.0ms preprocess, 310.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 214/650

0: 384x640 10 persons, 9 cars, 487.5ms
Speed: 5.0ms preprocess, 487.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 215/650

0: 384x640 8 persons, 9 cars, 334.8ms
Speed: 5.0ms preprocess, 334.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 216/650

0: 384x640 9 persons, 8 cars, 315.9ms
Speed: 4.2ms preprocess, 315.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 217/650

0: 384x640 9 persons, 8 cars, 337.0ms
Speed: 4.0ms preprocess, 337.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 218/650

0: 384x640 9 persons, 9 cars, 340.0ms
Speed: 4.0ms preprocess, 340.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 219/650

0: 384x640 9 persons, 9 cars, 360.5ms
Speed: 4.0ms preprocess, 360.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 220/650

0: 384x640 10 persons, 9 cars, 333.2ms
Speed: 5.0ms preprocess, 333.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 221/650

0: 384x640 9 persons, 8 cars, 378.7ms
Speed: 5.0ms preprocess, 378.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 222/650

0: 384x640 9 persons, 9 cars, 417.9ms
Speed: 4.0ms preprocess, 417.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 223/650

0: 384x640 10 persons, 9 cars, 364.9ms
Speed: 5.0ms preprocess, 364.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 224/650

0: 384x640 10 persons, 9 cars, 347.1ms
Speed: 5.0ms preprocess, 347.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 225/650

0: 384x640 10 persons, 9 cars, 352.6ms
Speed: 4.5ms preprocess, 352.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 226/650

0: 384x640 10 persons, 8 cars, 349.6ms
Speed: 5.0ms preprocess, 349.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 227/650

0: 384x640 10 persons, 8 cars, 348.7ms
Speed: 4.0ms preprocess, 348.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 228/650

0: 384x640 9 persons, 8 cars, 539.4ms
Speed: 4.0ms preprocess, 539.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 229/650

0: 384x640 10 persons, 8 cars, 372.0ms
Speed: 4.0ms preprocess, 372.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 230/650

0: 384x640 11 persons, 8 cars, 352.3ms
Speed: 6.0ms preprocess, 352.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 231/650

0: 384x640 11 persons, 9 cars, 375.6ms
Speed: 7.0ms preprocess, 375.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 232/650

0: 384x640 11 persons, 9 cars, 365.4ms
Speed: 5.0ms preprocess, 365.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 233/650

0: 384x640 11 persons, 9 cars, 409.1ms
Speed: 6.0ms preprocess, 409.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 234/650

0: 384x640 11 persons, 9 cars, 393.0ms
Speed: 6.0ms preprocess, 393.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 235/650

0: 384x640 12 persons, 9 cars, 378.1ms
Speed: 5.0ms preprocess, 378.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 236/650

0: 384x640 12 persons, 8 cars, 344.2ms
Speed: 5.3ms preprocess, 344.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 237/650

0: 384x640 12 persons, 9 cars, 378.9ms
Speed: 4.0ms preprocess, 378.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 238/650

0: 384x640 13 persons, 9 cars, 448.0ms
Speed: 5.0ms preprocess, 448.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 239/650

0: 384x640 12 persons, 9 cars, 591.2ms
Speed: 4.3ms preprocess, 591.2ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 240/650

0: 384x640 14 persons, 10 cars, 449.4ms
Speed: 10.2ms preprocess, 449.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 241/650

0: 384x640 14 persons, 10 cars, 336.6ms
Speed: 5.0ms preprocess, 336.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 242/650

0: 384x640 14 persons, 9 cars, 338.2ms
Speed: 6.0ms preprocess, 338.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 243/650

0: 384x640 14 persons, 9 cars, 338.9ms
Speed: 5.0ms preprocess, 338.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 244/650

0: 384x640 14 persons, 9 cars, 328.6ms
Speed: 4.1ms preprocess, 328.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 245/650

0: 384x640 14 persons, 10 cars, 286.2ms
Speed: 4.0ms preprocess, 286.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 246/650

0: 384x640 14 persons, 10 cars, 1 backpack, 330.2ms
Speed: 4.0ms preprocess, 330.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 247/650

0: 384x640 13 persons, 10 cars, 348.4ms
Speed: 5.0ms preprocess, 348.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 248/650

0: 384x640 13 persons, 10 cars, 1 backpack, 383.1ms
Speed: 4.7ms preprocess, 383.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 249/650

0: 384x640 13 persons, 10 cars, 1 backpack, 413.8ms
Speed: 5.0ms preprocess, 413.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 250/650

0: 384x640 13 persons, 10 cars, 2 backpacks, 358.6ms
Speed: 5.1ms preprocess, 358.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 251/650

0: 384x640 11 persons, 10 cars, 1 backpack, 351.9ms
Speed: 4.0ms preprocess, 351.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 252/650

0: 384x640 11 persons, 10 cars, 1 truck, 1 backpack, 500.4ms
Speed: 5.0ms preprocess, 500.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 253/650

0: 384x640 11 persons, 9 cars, 1 truck, 359.7ms
Speed: 5.0ms preprocess, 359.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 254/650

0: 384x640 11 persons, 9 cars, 1 truck, 331.8ms
Speed: 6.0ms preprocess, 331.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 255/650

0: 384x640 8 persons, 9 cars, 1 truck, 346.2ms
Speed: 5.0ms preprocess, 346.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 256/650

0: 384x640 8 persons, 9 cars, 1 truck, 353.5ms
Speed: 4.0ms preprocess, 353.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 257/650

0: 384x640 9 persons, 9 cars, 1 truck, 353.7ms
Speed: 5.0ms preprocess, 353.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 258/650

0: 384x640 8 persons, 10 cars, 1 truck, 370.8ms
Speed: 4.0ms preprocess, 370.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 259/650

0: 384x640 8 persons, 10 cars, 1 truck, 347.2ms
Speed: 7.0ms preprocess, 347.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 260/650

0: 384x640 9 persons, 9 cars, 352.5ms
Speed: 5.2ms preprocess, 352.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 261/650

0: 384x640 10 persons, 11 cars, 359.1ms
Speed: 4.0ms preprocess, 359.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 262/650

0: 384x640 10 persons, 10 cars, 349.8ms
Speed: 4.0ms preprocess, 349.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 263/650

0: 384x640 9 persons, 9 cars, 343.4ms
Speed: 6.0ms preprocess, 343.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 264/650

0: 384x640 8 persons, 9 cars, 346.2ms
Speed: 6.0ms preprocess, 346.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 265/650

0: 384x640 9 persons, 9 cars, 497.6ms
Speed: 5.0ms preprocess, 497.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 266/650

0: 384x640 9 persons, 9 cars, 355.4ms
Speed: 5.0ms preprocess, 355.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 267/650

0: 384x640 8 persons, 9 cars, 349.5ms
Speed: 5.0ms preprocess, 349.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 268/650

0: 384x640 9 persons, 8 cars, 326.2ms
Speed: 6.0ms preprocess, 326.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 269/650

0: 384x640 10 persons, 10 cars, 366.8ms
Speed: 5.0ms preprocess, 366.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 270/650

0: 384x640 10 persons, 8 cars, 357.0ms
Speed: 5.0ms preprocess, 357.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 271/650

0: 384x640 8 persons, 8 cars, 354.9ms
Speed: 5.0ms preprocess, 354.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 272/650

0: 384x640 8 persons, 8 cars, 358.1ms
Speed: 4.5ms preprocess, 358.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 273/650

0: 384x640 10 persons, 8 cars, 350.1ms
Speed: 5.0ms preprocess, 350.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 274/650

0: 384x640 12 persons, 8 cars, 393.3ms
Speed: 4.5ms preprocess, 393.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 275/650

0: 384x640 12 persons, 8 cars, 410.8ms
Speed: 5.0ms preprocess, 410.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 276/650

0: 384x640 11 persons, 7 cars, 1 backpack, 382.1ms
Speed: 6.0ms preprocess, 382.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 277/650

0: 384x640 11 persons, 7 cars, 1 backpack, 550.8ms
Speed: 6.5ms preprocess, 550.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 278/650

0: 384x640 11 persons, 6 cars, 1 backpack, 357.7ms
Speed: 6.0ms preprocess, 357.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 279/650

0: 384x640 11 persons, 6 cars, 1 backpack, 342.6ms
Speed: 5.0ms preprocess, 342.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 280/650

0: 384x640 10 persons, 7 cars, 1 backpack, 356.5ms
Speed: 5.0ms preprocess, 356.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 281/650

0: 384x640 10 persons, 9 cars, 1 backpack, 343.0ms
Speed: 5.0ms preprocess, 343.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 282/650

0: 384x640 10 persons, 9 cars, 1 backpack, 344.8ms
Speed: 5.0ms preprocess, 344.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 283/650

0: 384x640 10 persons, 9 cars, 353.2ms
Speed: 5.0ms preprocess, 353.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 284/650

0: 384x640 10 persons, 8 cars, 323.6ms
Speed: 6.0ms preprocess, 323.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 285/650

0: 384x640 10 persons, 7 cars, 339.4ms
Speed: 5.0ms preprocess, 339.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 286/650

0: 384x640 10 persons, 6 cars, 347.6ms
Speed: 6.0ms preprocess, 347.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 287/650

0: 384x640 10 persons, 7 cars, 351.5ms
Speed: 4.0ms preprocess, 351.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 288/650

0: 384x640 10 persons, 7 cars, 1 backpack, 375.9ms
Speed: 5.0ms preprocess, 375.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 289/650

0: 384x640 10 persons, 6 cars, 372.5ms
Speed: 12.6ms preprocess, 372.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 290/650

0: 384x640 10 persons, 8 cars, 334.8ms
Speed: 5.0ms preprocess, 334.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 291/650

0: 384x640 11 persons, 8 cars, 1 motorcycle, 1 backpack, 336.4ms
Speed: 4.0ms preprocess, 336.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 292/650

0: 384x640 11 persons, 8 cars, 1 motorcycle, 1 backpack, 327.6ms
Speed: 4.0ms preprocess, 327.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 293/650

0: 384x640 11 persons, 7 cars, 1 backpack, 321.6ms
Speed: 5.0ms preprocess, 321.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 294/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 1 backpack, 329.7ms
Speed: 5.0ms preprocess, 329.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 295/650

0: 384x640 11 persons, 6 cars, 1 motorcycle, 1 backpack, 307.3ms
Speed: 4.0ms preprocess, 307.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 296/650

0: 384x640 11 persons, 9 cars, 1 backpack, 331.3ms
Speed: 4.0ms preprocess, 331.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 297/650

0: 384x640 11 persons, 9 cars, 1 motorcycle, 331.7ms
Speed: 5.0ms preprocess, 331.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 298/650

0: 384x640 11 persons, 9 cars, 1 backpack, 306.5ms
Speed: 4.0ms preprocess, 306.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 299/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 backpack, 339.6ms
Speed: 5.0ms preprocess, 339.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 300/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 1 backpack, 324.8ms
Speed: 4.5ms preprocess, 324.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 301/650

0: 384x640 11 persons, 8 cars, 1 backpack, 478.0ms
Speed: 5.0ms preprocess, 478.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 302/650

0: 384x640 11 persons, 8 cars, 1 backpack, 362.8ms
Speed: 5.0ms preprocess, 362.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 303/650

0: 384x640 12 persons, 8 cars, 2 backpacks, 366.1ms
Speed: 4.0ms preprocess, 366.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 304/650

0: 384x640 12 persons, 7 cars, 2 backpacks, 340.1ms
Speed: 5.0ms preprocess, 340.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 305/650

0: 384x640 10 persons, 7 cars, 2 backpacks, 329.7ms
Speed: 4.0ms preprocess, 329.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 306/650

0: 384x640 10 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 322.1ms
Speed: 4.0ms preprocess, 322.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 307/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 1 backpack, 336.2ms
Speed: 5.0ms preprocess, 336.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 308/650

0: 384x640 9 persons, 7 cars, 1 truck, 1 backpack, 327.7ms
Speed: 4.0ms preprocess, 327.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 309/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 1 backpack, 327.4ms
Speed: 4.0ms preprocess, 327.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 310/650

0: 384x640 12 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 316.3ms
Speed: 4.0ms preprocess, 316.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 311/650

0: 384x640 10 persons, 6 cars, 1 truck, 332.9ms
Speed: 4.5ms preprocess, 332.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 312/650

0: 384x640 10 persons, 6 cars, 1 truck, 1 backpack, 485.7ms
Speed: 5.0ms preprocess, 485.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 313/650

0: 384x640 10 persons, 7 cars, 1 truck, 331.1ms
Speed: 5.0ms preprocess, 331.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 314/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 1 backpack, 320.9ms
Speed: 4.0ms preprocess, 320.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 315/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 316.6ms
Speed: 5.0ms preprocess, 316.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 316/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 327.1ms
Speed: 5.0ms preprocess, 327.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 317/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 337.0ms
Speed: 5.0ms preprocess, 337.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 318/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 329.2ms
Speed: 4.0ms preprocess, 329.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 319/650

0: 384x640 11 persons, 8 cars, 1 truck, 339.0ms
Speed: 5.0ms preprocess, 339.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 320/650

0: 384x640 11 persons, 8 cars, 1 motorcycle, 1 truck, 340.6ms
Speed: 4.0ms preprocess, 340.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 321/650

0: 384x640 11 persons, 9 cars, 1 motorcycle, 1 truck, 337.0ms
Speed: 3.5ms preprocess, 337.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 322/650

0: 384x640 11 persons, 9 cars, 1 truck, 512.7ms
Speed: 4.0ms preprocess, 512.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 323/650

0: 384x640 11 persons, 8 cars, 1 motorcycle, 1 truck, 319.0ms
Speed: 5.0ms preprocess, 319.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 324/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 316.0ms
Speed: 4.0ms preprocess, 316.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 325/650

0: 384x640 11 persons, 8 cars, 1 motorcycle, 1 truck, 305.4ms
Speed: 4.0ms preprocess, 305.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 326/650

0: 384x640 9 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 326.1ms
Speed: 4.0ms preprocess, 326.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 327/650

0: 384x640 10 persons, 10 cars, 1 motorcycle, 1 truck, 1 backpack, 318.8ms
Speed: 5.0ms preprocess, 318.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 328/650

0: 384x640 9 persons, 10 cars, 1 motorcycle, 1 backpack, 334.6ms
Speed: 4.0ms preprocess, 334.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 329/650

0: 384x640 10 persons, 9 cars, 1 motorcycle, 1 truck, 333.6ms
Speed: 5.0ms preprocess, 333.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 330/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 322.6ms
Speed: 5.0ms preprocess, 322.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 331/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 345.2ms
Speed: 4.0ms preprocess, 345.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 332/650

0: 384x640 11 persons, 6 cars, 1 motorcycle, 591.4ms
Speed: 4.0ms preprocess, 591.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 333/650

0: 384x640 11 persons, 6 cars, 1 motorcycle, 341.9ms
Speed: 5.0ms preprocess, 341.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 334/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 344.6ms
Speed: 4.0ms preprocess, 344.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 335/650

0: 384x640 10 persons, 8 cars, 1 motorcycle, 353.5ms
Speed: 4.0ms preprocess, 353.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 336/650

0: 384x640 10 persons, 10 cars, 1 motorcycle, 335.5ms
Speed: 4.0ms preprocess, 335.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 337/650

0: 384x640 12 persons, 11 cars, 1 motorcycle, 330.0ms
Speed: 4.0ms preprocess, 330.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 338/650

0: 384x640 11 persons, 10 cars, 317.0ms
Speed: 4.0ms preprocess, 317.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 339/650

0: 384x640 10 persons, 9 cars, 1 motorcycle, 334.1ms
Speed: 4.0ms preprocess, 334.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 340/650

0: 384x640 10 persons, 11 cars, 1 motorcycle, 337.0ms
Speed: 4.0ms preprocess, 337.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 341/650

0: 384x640 10 persons, 10 cars, 1 motorcycle, 341.7ms
Speed: 5.0ms preprocess, 341.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 342/650

0: 384x640 10 persons, 10 cars, 2 motorcycles, 485.4ms
Speed: 4.5ms preprocess, 485.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 343/650

0: 384x640 10 persons, 9 cars, 2 motorcycles, 339.2ms
Speed: 6.0ms preprocess, 339.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 344/650

0: 384x640 7 persons, 8 cars, 2 motorcycles, 307.8ms
Speed: 5.0ms preprocess, 307.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 345/650

0: 384x640 8 persons, 7 cars, 1 motorcycle, 334.0ms
Speed: 5.0ms preprocess, 334.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 346/650

0: 384x640 9 persons, 6 cars, 1 motorcycle, 321.1ms
Speed: 5.0ms preprocess, 321.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 347/650

0: 384x640 9 persons, 6 cars, 1 motorcycle, 325.1ms
Speed: 4.9ms preprocess, 325.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 348/650

0: 384x640 9 persons, 6 cars, 1 motorcycle, 315.6ms
Speed: 4.0ms preprocess, 315.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 349/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 328.7ms
Speed: 5.0ms preprocess, 328.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 350/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 331.7ms
Speed: 5.0ms preprocess, 331.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 351/650

0: 384x640 9 persons, 7 cars, 1 motorcycle, 356.1ms
Speed: 4.5ms preprocess, 356.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 352/650

0: 384x640 9 persons, 6 cars, 1 motorcycle, 484.8ms
Speed: 4.0ms preprocess, 484.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 353/650

0: 384x640 8 persons, 10 cars, 1 motorcycle, 315.7ms
Speed: 5.5ms preprocess, 315.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 354/650

0: 384x640 8 persons, 6 cars, 1 motorcycle, 307.9ms
Speed: 5.1ms preprocess, 307.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 355/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 333.4ms
Speed: 4.0ms preprocess, 333.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 356/650

0: 384x640 10 persons, 8 cars, 2 motorcycles, 333.8ms
Speed: 5.0ms preprocess, 333.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 357/650

0: 384x640 10 persons, 6 cars, 2 motorcycles, 328.7ms
Speed: 5.0ms preprocess, 328.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 358/650

0: 384x640 10 persons, 8 cars, 1 motorcycle, 330.3ms
Speed: 4.0ms preprocess, 330.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 359/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 337.0ms
Speed: 5.0ms preprocess, 337.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 360/650

0: 384x640 10 persons, 5 cars, 1 motorcycle, 355.8ms
Speed: 4.0ms preprocess, 355.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 361/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 550.6ms
Speed: 5.0ms preprocess, 550.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 362/650

0: 384x640 10 persons, 6 cars, 1 motorcycle, 409.2ms
Speed: 6.2ms preprocess, 409.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 363/650

0: 384x640 10 persons, 5 cars, 1 motorcycle, 359.7ms
Speed: 4.1ms preprocess, 359.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 364/650

0: 384x640 9 persons, 5 cars, 1 motorcycle, 354.8ms
Speed: 4.0ms preprocess, 354.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 365/650

0: 384x640 10 persons, 5 cars, 329.6ms
Speed: 5.0ms preprocess, 329.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 366/650

0: 384x640 9 persons, 5 cars, 1 motorcycle, 376.6ms
Speed: 5.0ms preprocess, 376.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 367/650

0: 384x640 8 persons, 7 cars, 1 motorcycle, 344.7ms
Speed: 4.2ms preprocess, 344.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 368/650

0: 384x640 9 persons, 7 cars, 1 motorcycle, 355.5ms
Speed: 4.0ms preprocess, 355.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 369/650

0: 384x640 9 persons, 5 cars, 1 motorcycle, 331.2ms
Speed: 4.0ms preprocess, 331.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 370/650

0: 384x640 8 persons, 5 cars, 1 motorcycle, 504.7ms
Speed: 6.0ms preprocess, 504.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 371/650

0: 384x640 10 persons, 5 cars, 382.2ms
Speed: 4.4ms preprocess, 382.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 372/650

0: 384x640 10 persons, 5 cars, 367.5ms
Speed: 5.0ms preprocess, 367.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 373/650

0: 384x640 8 persons, 5 cars, 364.8ms
Speed: 5.0ms preprocess, 364.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 374/650

0: 384x640 11 persons, 6 cars, 371.3ms
Speed: 5.0ms preprocess, 371.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 375/650

0: 384x640 11 persons, 7 cars, 396.0ms
Speed: 5.0ms preprocess, 396.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 376/650

0: 384x640 10 persons, 7 cars, 357.2ms
Speed: 5.0ms preprocess, 357.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 377/650

0: 384x640 10 persons, 6 cars, 372.0ms
Speed: 5.0ms preprocess, 372.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 378/650

0: 384x640 10 persons, 5 cars, 543.4ms
Speed: 5.0ms preprocess, 543.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 379/650

0: 384x640 9 persons, 5 cars, 367.0ms
Speed: 5.0ms preprocess, 367.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 380/650

0: 384x640 14 persons, 8 cars, 369.3ms
Speed: 4.0ms preprocess, 369.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 381/650

0: 384x640 11 persons, 8 cars, 380.1ms
Speed: 5.0ms preprocess, 380.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 382/650

0: 384x640 11 persons, 7 cars, 374.1ms
Speed: 5.0ms preprocess, 374.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 383/650

0: 384x640 12 persons, 8 cars, 369.9ms
Speed: 5.0ms preprocess, 369.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 384/650

0: 384x640 12 persons, 7 cars, 339.4ms
Speed: 5.0ms preprocess, 339.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 385/650

0: 384x640 12 persons, 6 cars, 366.9ms
Speed: 4.0ms preprocess, 366.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 386/650

0: 384x640 11 persons, 7 cars, 1 backpack, 365.8ms
Speed: 5.0ms preprocess, 365.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 387/650

0: 384x640 12 persons, 8 cars, 573.1ms
Speed: 5.0ms preprocess, 573.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 388/650

0: 384x640 13 persons, 9 cars, 467.4ms
Speed: 7.9ms preprocess, 467.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 389/650

0: 384x640 12 persons, 7 cars, 432.6ms
Speed: 5.0ms preprocess, 432.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 390/650

0: 384x640 12 persons, 6 cars, 1 backpack, 324.6ms
Speed: 6.0ms preprocess, 324.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 391/650

0: 384x640 12 persons, 8 cars, 1 backpack, 362.7ms
Speed: 5.0ms preprocess, 362.7ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 392/650

0: 384x640 12 persons, 7 cars, 1 backpack, 398.8ms
Speed: 4.0ms preprocess, 398.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 393/650

0: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 367.8ms
Speed: 4.0ms preprocess, 367.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 394/650

0: 384x640 12 persons, 6 cars, 1 truck, 1 backpack, 336.2ms
Speed: 5.2ms preprocess, 336.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 395/650

0: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 335.1ms
Speed: 5.0ms preprocess, 335.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 396/650

0: 384x640 13 persons, 7 cars, 1 truck, 1 backpack, 582.1ms
Speed: 5.0ms preprocess, 582.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 397/650

0: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 328.9ms
Speed: 6.0ms preprocess, 328.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 398/650

0: 384x640 12 persons, 6 cars, 1 truck, 1 backpack, 328.0ms
Speed: 5.0ms preprocess, 328.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 399/650

0: 384x640 11 persons, 8 cars, 1 truck, 1 backpack, 341.1ms
Speed: 5.0ms preprocess, 341.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 400/650

0: 384x640 11 persons, 8 cars, 1 truck, 1 backpack, 330.5ms
Speed: 4.0ms preprocess, 330.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 401/650

0: 384x640 9 persons, 7 cars, 1 truck, 1 backpack, 329.7ms
Speed: 4.0ms preprocess, 329.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 402/650

0: 384x640 11 persons, 6 cars, 1 truck, 1 backpack, 357.5ms
Speed: 4.0ms preprocess, 357.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 403/650

0: 384x640 10 persons, 6 cars, 1 truck, 2 backpacks, 342.4ms
Speed: 4.0ms preprocess, 342.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 404/650

0: 384x640 10 persons, 5 cars, 1 truck, 1 backpack, 332.0ms
Speed: 4.0ms preprocess, 332.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 405/650

0: 384x640 11 persons, 6 cars, 1 truck, 1 backpack, 476.4ms
Speed: 5.0ms preprocess, 476.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 406/650

0: 384x640 12 persons, 6 cars, 1 truck, 1 backpack, 308.7ms
Speed: 5.0ms preprocess, 308.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 407/650

0: 384x640 11 persons, 5 cars, 1 truck, 1 backpack, 336.7ms
Speed: 5.0ms preprocess, 336.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 408/650

0: 384x640 11 persons, 5 cars, 1 truck, 2 backpacks, 335.0ms
Speed: 5.0ms preprocess, 335.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 409/650

0: 384x640 11 persons, 7 cars, 1 truck, 1 backpack, 320.7ms
Speed: 4.0ms preprocess, 320.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 410/650

0: 384x640 11 persons, 6 cars, 1 truck, 1 backpack, 342.5ms
Speed: 4.0ms preprocess, 342.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 411/650

0: 384x640 10 persons, 6 cars, 1 truck, 334.1ms
Speed: 5.0ms preprocess, 334.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 412/650

0: 384x640 10 persons, 5 cars, 1 truck, 373.9ms
Speed: 5.0ms preprocess, 373.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 413/650

0: 384x640 10 persons, 5 cars, 1 truck, 367.6ms
Speed: 5.0ms preprocess, 367.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 414/650

0: 384x640 10 persons, 5 cars, 1 truck, 482.2ms
Speed: 4.0ms preprocess, 482.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 415/650

0: 384x640 12 persons, 4 cars, 1 truck, 1 backpack, 515.5ms
Speed: 5.0ms preprocess, 515.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 416/650

0: 384x640 12 persons, 4 cars, 1 truck, 2 backpacks, 425.4ms
Speed: 5.0ms preprocess, 425.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 417/650

0: 384x640 13 persons, 5 cars, 1 truck, 2 backpacks, 328.7ms
Speed: 4.5ms preprocess, 328.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 418/650

0: 384x640 12 persons, 6 cars, 1 truck, 1 backpack, 359.4ms
Speed: 4.0ms preprocess, 359.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 419/650

0: 384x640 11 persons, 6 cars, 1 truck, 1 backpack, 329.0ms
Speed: 4.0ms preprocess, 329.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 420/650

0: 384x640 12 persons, 6 cars, 1 truck, 1 backpack, 337.9ms
Speed: 4.0ms preprocess, 337.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 421/650

0: 384x640 11 persons, 6 cars, 1 truck, 1 backpack, 335.5ms
Speed: 5.0ms preprocess, 335.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 422/650

0: 384x640 11 persons, 6 cars, 1 truck, 2 backpacks, 360.5ms
Speed: 4.0ms preprocess, 360.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 423/650

0: 384x640 10 persons, 5 cars, 1 truck, 482.0ms
Speed: 4.0ms preprocess, 482.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 424/650

0: 384x640 8 persons, 5 cars, 1 truck, 2 backpacks, 323.9ms
Speed: 5.0ms preprocess, 323.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 425/650

0: 384x640 8 persons, 5 cars, 1 motorcycle, 1 truck, 2 backpacks, 344.4ms
Speed: 5.0ms preprocess, 344.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 426/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 2 backpacks, 337.2ms
Speed: 4.0ms preprocess, 337.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 427/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 backpack, 311.9ms
Speed: 4.9ms preprocess, 311.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 428/650

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 backpack, 356.0ms
Speed: 4.0ms preprocess, 356.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 429/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 backpack, 376.3ms
Speed: 4.0ms preprocess, 376.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 430/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 386.0ms
Speed: 5.0ms preprocess, 386.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 431/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 516.3ms
Speed: 4.0ms preprocess, 516.3ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 432/650

0: 384x640 11 persons, 7 cars, 1 motorcycle, 387.3ms
Speed: 6.0ms preprocess, 387.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 433/650

0: 384x640 12 persons, 7 cars, 1 motorcycle, 366.7ms
Speed: 5.0ms preprocess, 366.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 434/650

0: 384x640 12 persons, 7 cars, 1 motorcycle, 348.0ms
Speed: 4.0ms preprocess, 348.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 435/650

0: 384x640 14 persons, 7 cars, 1 motorcycle, 374.7ms
Speed: 4.0ms preprocess, 374.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 436/650

0: 384x640 12 persons, 7 cars, 367.8ms
Speed: 5.9ms preprocess, 367.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 437/650

0: 384x640 12 persons, 7 cars, 1 motorcycle, 381.0ms
Speed: 5.0ms preprocess, 381.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 438/650

0: 384x640 12 persons, 7 cars, 323.8ms
Speed: 4.0ms preprocess, 323.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 439/650

0: 384x640 11 persons, 6 cars, 1 motorcycle, 551.8ms
Speed: 5.0ms preprocess, 551.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 440/650

0: 384x640 11 persons, 6 cars, 1 motorcycle, 373.8ms
Speed: 5.0ms preprocess, 373.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 441/650

0: 384x640 14 persons, 6 cars, 1 motorcycle, 362.4ms
Speed: 5.0ms preprocess, 362.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 442/650

0: 384x640 12 persons, 7 cars, 1 motorcycle, 437.2ms
Speed: 6.0ms preprocess, 437.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 443/650

0: 384x640 13 persons, 6 cars, 1 motorcycle, 412.1ms
Speed: 6.0ms preprocess, 412.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 444/650

0: 384x640 11 persons, 6 cars, 340.2ms
Speed: 5.0ms preprocess, 340.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 445/650

0: 384x640 11 persons, 6 cars, 375.9ms
Speed: 4.0ms preprocess, 375.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 446/650

0: 384x640 11 persons, 5 cars, 1 backpack, 388.2ms
Speed: 5.0ms preprocess, 388.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 447/650

0: 384x640 10 persons, 6 cars, 1 backpack, 509.8ms
Speed: 5.0ms preprocess, 509.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 448/650

0: 384x640 11 persons, 6 cars, 1 backpack, 423.1ms
Speed: 5.2ms preprocess, 423.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 449/650

0: 384x640 10 persons, 7 cars, 1 backpack, 371.2ms
Speed: 5.0ms preprocess, 371.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 450/650

0: 384x640 11 persons, 7 cars, 1 backpack, 385.1ms
Speed: 4.0ms preprocess, 385.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 451/650

0: 384x640 10 persons, 5 cars, 1 backpack, 388.3ms
Speed: 5.0ms preprocess, 388.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 452/650

0: 384x640 10 persons, 5 cars, 1 backpack, 384.5ms
Speed: 4.0ms preprocess, 384.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 453/650

0: 384x640 10 persons, 7 cars, 1 backpack, 382.0ms
Speed: 5.5ms preprocess, 382.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 454/650

0: 384x640 9 persons, 7 cars, 1 backpack, 344.3ms
Speed: 5.0ms preprocess, 344.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 455/650

0: 384x640 10 persons, 7 cars, 1 backpack, 443.9ms
Speed: 5.0ms preprocess, 443.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 456/650

0: 384x640 10 persons, 7 cars, 1 backpack, 415.1ms
Speed: 7.0ms preprocess, 415.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 457/650

0: 384x640 10 persons, 7 cars, 1 backpack, 381.7ms
Speed: 5.1ms preprocess, 381.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 458/650

0: 384x640 10 persons, 7 cars, 1 backpack, 375.2ms
Speed: 5.2ms preprocess, 375.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 459/650

0: 384x640 11 persons, 5 cars, 1 backpack, 384.0ms
Speed: 4.6ms preprocess, 384.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 460/650

0: 384x640 11 persons, 7 cars, 1 backpack, 384.3ms
Speed: 5.3ms preprocess, 384.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 461/650

0: 384x640 12 persons, 7 cars, 1 backpack, 369.2ms
Speed: 5.0ms preprocess, 369.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 462/650

0: 384x640 11 persons, 7 cars, 1 backpack, 1 handbag, 552.4ms
Speed: 5.0ms preprocess, 552.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 463/650

0: 384x640 11 persons, 7 cars, 1 backpack, 338.7ms
Speed: 6.0ms preprocess, 338.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 464/650

0: 384x640 11 persons, 7 cars, 1 backpack, 306.9ms
Speed: 4.0ms preprocess, 306.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 465/650

0: 384x640 11 persons, 6 cars, 1 backpack, 348.6ms
Speed: 4.0ms preprocess, 348.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 466/650

0: 384x640 11 persons, 6 cars, 1 backpack, 313.3ms
Speed: 4.0ms preprocess, 313.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 467/650

0: 384x640 11 persons, 6 cars, 1 backpack, 330.5ms
Speed: 5.0ms preprocess, 330.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 468/650

0: 384x640 10 persons, 6 cars, 1 backpack, 375.4ms
Speed: 5.0ms preprocess, 375.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 469/650

0: 384x640 10 persons, 7 cars, 1 backpack, 372.7ms
Speed: 4.0ms preprocess, 372.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 470/650

0: 384x640 10 persons, 7 cars, 1 backpack, 570.3ms
Speed: 4.0ms preprocess, 570.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 471/650

0: 384x640 11 persons, 8 cars, 1 backpack, 380.4ms
Speed: 5.0ms preprocess, 380.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 472/650

0: 384x640 11 persons, 8 cars, 1 backpack, 355.7ms
Speed: 5.0ms preprocess, 355.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 473/650

0: 384x640 11 persons, 7 cars, 1 backpack, 340.2ms
Speed: 4.0ms preprocess, 340.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 474/650

0: 384x640 11 persons, 6 cars, 1 backpack, 348.1ms
Speed: 4.0ms preprocess, 348.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 475/650

0: 384x640 11 persons, 8 cars, 1 backpack, 363.8ms
Speed: 5.0ms preprocess, 363.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 476/650

0: 384x640 11 persons, 7 cars, 1 backpack, 307.8ms
Speed: 4.0ms preprocess, 307.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 477/650

0: 384x640 11 persons, 7 cars, 1 backpack, 449.3ms
Speed: 4.0ms preprocess, 449.3ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 478/650

0: 384x640 10 persons, 6 cars, 1 backpack, 429.1ms
Speed: 5.0ms preprocess, 429.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 479/650

0: 384x640 11 persons, 6 cars, 1 backpack, 322.1ms
Speed: 5.0ms preprocess, 322.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 480/650

0: 384x640 10 persons, 6 cars, 1 backpack, 364.6ms
Speed: 5.0ms preprocess, 364.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 481/650

0: 384x640 11 persons, 5 cars, 339.3ms
Speed: 5.5ms preprocess, 339.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 482/650

0: 384x640 10 persons, 6 cars, 1 backpack, 362.9ms
Speed: 4.0ms preprocess, 362.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 483/650

0: 384x640 10 persons, 5 cars, 1 backpack, 390.1ms
Speed: 4.1ms preprocess, 390.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 484/650

0: 384x640 10 persons, 5 cars, 1 backpack, 374.5ms
Speed: 4.0ms preprocess, 374.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 485/650

0: 384x640 10 persons, 5 cars, 1 backpack, 513.9ms
Speed: 5.1ms preprocess, 513.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 486/650

0: 384x640 11 persons, 6 cars, 1 backpack, 369.1ms
Speed: 4.0ms preprocess, 369.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 487/650

0: 384x640 11 persons, 5 cars, 1 backpack, 375.7ms
Speed: 4.0ms preprocess, 375.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 488/650

0: 384x640 10 persons, 5 cars, 1 backpack, 357.6ms
Speed: 5.0ms preprocess, 357.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 489/650

0: 384x640 9 persons, 7 cars, 363.3ms
Speed: 4.0ms preprocess, 363.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 490/650

0: 384x640 9 persons, 6 cars, 362.7ms
Speed: 6.0ms preprocess, 362.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 491/650

0: 384x640 9 persons, 6 cars, 365.2ms
Speed: 4.0ms preprocess, 365.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 492/650

0: 384x640 9 persons, 6 cars, 341.2ms
Speed: 4.0ms preprocess, 341.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 493/650

0: 384x640 9 persons, 6 cars, 536.8ms
Speed: 6.0ms preprocess, 536.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 494/650

0: 384x640 8 persons, 6 cars, 348.6ms
Speed: 5.0ms preprocess, 348.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 495/650

0: 384x640 8 persons, 6 cars, 408.0ms
Speed: 6.3ms preprocess, 408.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 496/650

0: 384x640 9 persons, 6 cars, 456.7ms
Speed: 6.0ms preprocess, 456.7ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 497/650

0: 384x640 10 persons, 6 cars, 381.7ms
Speed: 5.0ms preprocess, 381.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 498/650

0: 384x640 11 persons, 6 cars, 360.3ms
Speed: 6.0ms preprocess, 360.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 499/650

0: 384x640 11 persons, 6 cars, 346.3ms
Speed: 5.0ms preprocess, 346.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 500/650

0: 384x640 12 persons, 6 cars, 567.6ms
Speed: 5.0ms preprocess, 567.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 501/650

0: 384x640 12 persons, 6 cars, 376.4ms
Speed: 5.0ms preprocess, 376.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 502/650

0: 384x640 12 persons, 6 cars, 370.1ms
Speed: 4.0ms preprocess, 370.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 503/650

0: 384x640 12 persons, 6 cars, 376.0ms
Speed: 5.0ms preprocess, 376.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 504/650

0: 384x640 11 persons, 6 cars, 385.5ms
Speed: 4.0ms preprocess, 385.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 505/650

0: 384x640 10 persons, 5 cars, 369.2ms
Speed: 5.0ms preprocess, 369.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 506/650

0: 384x640 10 persons, 6 cars, 378.3ms
Speed: 5.0ms preprocess, 378.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 507/650

0: 384x640 9 persons, 6 cars, 476.4ms
Speed: 5.0ms preprocess, 476.4ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 508/650

0: 384x640 9 persons, 5 cars, 417.5ms
Speed: 4.0ms preprocess, 417.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 509/650

0: 384x640 10 persons, 6 cars, 370.6ms
Speed: 5.0ms preprocess, 370.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 510/650

0: 384x640 10 persons, 6 cars, 384.8ms
Speed: 4.0ms preprocess, 384.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 511/650

0: 384x640 11 persons, 6 cars, 365.8ms
Speed: 4.0ms preprocess, 365.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 512/650

0: 384x640 10 persons, 7 cars, 371.1ms
Speed: 5.0ms preprocess, 371.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 513/650

0: 384x640 9 persons, 5 cars, 494.1ms
Speed: 3.6ms preprocess, 494.1ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 514/650

0: 384x640 9 persons, 5 cars, 413.2ms
Speed: 5.0ms preprocess, 413.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 515/650

0: 384x640 11 persons, 5 cars, 383.9ms
Speed: 5.0ms preprocess, 383.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 516/650

0: 384x640 10 persons, 6 cars, 376.2ms
Speed: 5.0ms preprocess, 376.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 517/650

0: 384x640 10 persons, 6 cars, 365.8ms
Speed: 5.0ms preprocess, 365.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 518/650

0: 384x640 9 persons, 6 cars, 381.4ms
Speed: 4.0ms preprocess, 381.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 519/650

0: 384x640 10 persons, 5 cars, 367.5ms
Speed: 4.0ms preprocess, 367.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 520/650

0: 384x640 9 persons, 6 cars, 608.0ms
Speed: 6.0ms preprocess, 608.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 521/650

0: 384x640 9 persons, 7 cars, 421.6ms
Speed: 6.0ms preprocess, 421.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 522/650

0: 384x640 9 persons, 8 cars, 445.4ms
Speed: 5.0ms preprocess, 445.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 523/650

0: 384x640 10 persons, 6 cars, 464.3ms
Speed: 5.0ms preprocess, 464.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 524/650

0: 384x640 10 persons, 6 cars, 459.8ms
Speed: 6.0ms preprocess, 459.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 525/650

0: 384x640 8 persons, 6 cars, 479.0ms
Speed: 6.0ms preprocess, 479.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 526/650

0: 384x640 9 persons, 7 cars, 468.6ms
Speed: 6.1ms preprocess, 468.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 527/650

0: 384x640 10 persons, 6 cars, 589.5ms
Speed: 5.5ms preprocess, 589.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 528/650

0: 384x640 10 persons, 6 cars, 455.0ms
Speed: 10.8ms preprocess, 455.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 529/650

0: 384x640 9 persons, 6 cars, 411.0ms
Speed: 6.0ms preprocess, 411.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 530/650

0: 384x640 8 persons, 6 cars, 420.0ms
Speed: 6.5ms preprocess, 420.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 531/650

0: 384x640 12 persons, 7 cars, 417.1ms
Speed: 6.0ms preprocess, 417.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 532/650

0: 384x640 11 persons, 7 cars, 1 handbag, 428.1ms
Speed: 5.3ms preprocess, 428.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 533/650

0: 384x640 12 persons, 7 cars, 1 handbag, 638.7ms
Speed: 5.0ms preprocess, 638.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 534/650

0: 384x640 11 persons, 7 cars, 1 handbag, 444.1ms
Speed: 5.0ms preprocess, 444.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 535/650

0: 384x640 12 persons, 7 cars, 1 handbag, 420.5ms
Speed: 6.0ms preprocess, 420.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 536/650

0: 384x640 10 persons, 7 cars, 1 handbag, 440.2ms
Speed: 5.6ms preprocess, 440.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 537/650

0: 384x640 10 persons, 6 cars, 410.9ms
Speed: 6.0ms preprocess, 410.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 538/650

0: 384x640 10 persons, 6 cars, 371.3ms
Speed: 4.0ms preprocess, 371.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 539/650

0: 384x640 10 persons, 7 cars, 365.2ms
Speed: 5.0ms preprocess, 365.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 540/650

0: 384x640 11 persons, 7 cars, 463.7ms
Speed: 5.0ms preprocess, 463.7ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 541/650

0: 384x640 12 persons, 6 cars, 464.2ms
Speed: 6.0ms preprocess, 464.2ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 542/650

0: 384x640 12 persons, 5 cars, 393.5ms
Speed: 5.0ms preprocess, 393.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 543/650

0: 384x640 12 persons, 7 cars, 436.5ms
Speed: 6.0ms preprocess, 436.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 544/650

0: 384x640 11 persons, 7 cars, 480.8ms
Speed: 5.0ms preprocess, 480.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 545/650

0: 384x640 11 persons, 8 cars, 479.6ms
Speed: 6.0ms preprocess, 479.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 546/650

0: 384x640 10 persons, 7 cars, 440.8ms
Speed: 5.0ms preprocess, 440.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 547/650

0: 384x640 11 persons, 7 cars, 597.1ms
Speed: 5.0ms preprocess, 597.1ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 548/650

0: 384x640 11 persons, 6 cars, 423.8ms
Speed: 5.0ms preprocess, 423.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 549/650

0: 384x640 12 persons, 6 cars, 405.7ms
Speed: 5.0ms preprocess, 405.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 550/650

0: 384x640 11 persons, 5 cars, 396.6ms
Speed: 6.0ms preprocess, 396.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 551/650

0: 384x640 12 persons, 5 cars, 350.5ms
Speed: 5.0ms preprocess, 350.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 552/650

0: 384x640 12 persons, 5 cars, 362.0ms
Speed: 4.0ms preprocess, 362.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 553/650

0: 384x640 12 persons, 5 cars, 403.0ms
Speed: 5.5ms preprocess, 403.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 554/650

0: 384x640 13 persons, 5 cars, 595.2ms
Speed: 5.0ms preprocess, 595.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 555/650

0: 384x640 14 persons, 5 cars, 379.2ms
Speed: 4.0ms preprocess, 379.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 556/650

0: 384x640 12 persons, 5 cars, 386.5ms
Speed: 5.0ms preprocess, 386.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 557/650

0: 384x640 12 persons, 5 cars, 363.3ms
Speed: 5.0ms preprocess, 363.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 558/650

0: 384x640 12 persons, 5 cars, 396.9ms
Speed: 5.0ms preprocess, 396.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 559/650

0: 384x640 12 persons, 5 cars, 370.4ms
Speed: 4.3ms preprocess, 370.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 560/650

0: 384x640 11 persons, 5 cars, 398.1ms
Speed: 6.0ms preprocess, 398.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 561/650

0: 384x640 10 persons, 5 cars, 589.6ms
Speed: 6.0ms preprocess, 589.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 562/650

0: 384x640 10 persons, 5 cars, 380.7ms
Speed: 4.5ms preprocess, 380.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 563/650

0: 384x640 10 persons, 5 cars, 350.4ms
Speed: 5.0ms preprocess, 350.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 564/650

0: 384x640 10 persons, 5 cars, 367.5ms
Speed: 5.0ms preprocess, 367.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 565/650

0: 384x640 10 persons, 5 cars, 345.7ms
Speed: 7.0ms preprocess, 345.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 566/650

0: 384x640 10 persons, 5 cars, 356.8ms
Speed: 4.0ms preprocess, 356.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 567/650

0: 384x640 10 persons, 6 cars, 369.4ms
Speed: 6.0ms preprocess, 369.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 568/650

0: 384x640 10 persons, 6 cars, 441.0ms
Speed: 5.0ms preprocess, 441.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 569/650

0: 384x640 10 persons, 5 cars, 437.8ms
Speed: 6.0ms preprocess, 437.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 570/650

0: 384x640 10 persons, 5 cars, 392.3ms
Speed: 5.0ms preprocess, 392.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 571/650

0: 384x640 11 persons, 4 cars, 348.7ms
Speed: 5.6ms preprocess, 348.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 572/650

0: 384x640 13 persons, 5 cars, 375.0ms
Speed: 5.0ms preprocess, 375.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 573/650

0: 384x640 13 persons, 5 cars, 362.4ms
Speed: 5.0ms preprocess, 362.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 574/650

0: 384x640 10 persons, 5 cars, 537.9ms
Speed: 4.0ms preprocess, 537.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 575/650

0: 384x640 11 persons, 6 cars, 371.9ms
Speed: 4.0ms preprocess, 371.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 576/650

0: 384x640 14 persons, 6 cars, 337.3ms
Speed: 5.0ms preprocess, 337.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 577/650

0: 384x640 13 persons, 6 cars, 338.4ms
Speed: 7.0ms preprocess, 338.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 578/650

0: 384x640 12 persons, 6 cars, 351.6ms
Speed: 4.0ms preprocess, 351.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 579/650

0: 384x640 12 persons, 5 cars, 338.4ms
Speed: 5.0ms preprocess, 338.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 580/650

0: 384x640 12 persons, 5 cars, 320.0ms
Speed: 5.0ms preprocess, 320.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 581/650

0: 384x640 11 persons, 5 cars, 356.8ms
Speed: 5.0ms preprocess, 356.8ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 582/650

0: 384x640 12 persons, 5 cars, 437.1ms
Speed: 11.0ms preprocess, 437.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 583/650

0: 384x640 12 persons, 5 cars, 326.3ms
Speed: 5.0ms preprocess, 326.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 584/650

0: 384x640 11 persons, 5 cars, 351.0ms
Speed: 5.0ms preprocess, 351.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 585/650

0: 384x640 12 persons, 5 cars, 366.6ms
Speed: 5.0ms preprocess, 366.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 586/650

0: 384x640 11 persons, 7 cars, 313.4ms
Speed: 5.0ms preprocess, 313.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 587/650

0: 384x640 12 persons, 7 cars, 486.5ms
Speed: 5.0ms preprocess, 486.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 588/650

0: 384x640 12 persons, 7 cars, 413.8ms
Speed: 3.9ms preprocess, 413.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 589/650

0: 384x640 11 persons, 7 cars, 387.3ms
Speed: 4.0ms preprocess, 387.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 590/650

0: 384x640 11 persons, 7 cars, 385.2ms
Speed: 6.0ms preprocess, 385.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 591/650

0: 384x640 12 persons, 6 cars, 356.2ms
Speed: 4.9ms preprocess, 356.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 592/650

0: 384x640 13 persons, 6 cars, 362.4ms
Speed: 4.0ms preprocess, 362.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 593/650

0: 384x640 13 persons, 6 cars, 377.8ms
Speed: 4.0ms preprocess, 377.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 594/650

0: 384x640 13 persons, 7 cars, 506.6ms
Speed: 10.0ms preprocess, 506.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 595/650

0: 384x640 12 persons, 7 cars, 383.2ms
Speed: 5.0ms preprocess, 383.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 596/650

0: 384x640 11 persons, 8 cars, 423.8ms
Speed: 5.0ms preprocess, 423.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 597/650

0: 384x640 11 persons, 8 cars, 379.0ms
Speed: 4.0ms preprocess, 379.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 598/650

0: 384x640 11 persons, 8 cars, 354.2ms
Speed: 4.0ms preprocess, 354.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 599/650

0: 384x640 12 persons, 7 cars, 503.0ms
Speed: 5.0ms preprocess, 503.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 600/650

0: 384x640 12 persons, 7 cars, 417.6ms
Speed: 5.0ms preprocess, 417.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 601/650

0: 384x640 12 persons, 6 cars, 361.1ms
Speed: 5.0ms preprocess, 361.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 602/650

0: 384x640 11 persons, 4 cars, 361.6ms
Speed: 4.0ms preprocess, 361.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 603/650

0: 384x640 11 persons, 5 cars, 358.5ms
Speed: 5.0ms preprocess, 358.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 604/650

0: 384x640 10 persons, 5 cars, 364.9ms
Speed: 5.0ms preprocess, 364.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 605/650

0: 384x640 11 persons, 4 cars, 485.7ms
Speed: 4.0ms preprocess, 485.7ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 606/650

0: 384x640 10 persons, 3 cars, 368.9ms
Speed: 5.0ms preprocess, 368.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 607/650

0: 384x640 10 persons, 3 cars, 344.7ms
Speed: 4.0ms preprocess, 344.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 608/650

0: 384x640 11 persons, 3 cars, 372.5ms
Speed: 4.0ms preprocess, 372.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 609/650

0: 384x640 11 persons, 3 cars, 370.3ms
Speed: 5.0ms preprocess, 370.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 610/650

0: 384x640 12 persons, 3 cars, 415.9ms
Speed: 5.0ms preprocess, 415.9ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 611/650

0: 384x640 13 persons, 4 cars, 420.1ms
Speed: 13.0ms preprocess, 420.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 612/650

0: 384x640 12 persons, 4 cars, 364.1ms
Speed: 5.0ms preprocess, 364.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 613/650

0: 384x640 12 persons, 5 cars, 358.9ms
Speed: 4.0ms preprocess, 358.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 614/650

0: 384x640 12 persons, 4 cars, 364.9ms
Speed: 5.0ms preprocess, 364.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 615/650

0: 384x640 13 persons, 5 cars, 387.8ms
Speed: 4.0ms preprocess, 387.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 616/650

0: 384x640 13 persons, 6 cars, 462.8ms
Speed: 9.0ms preprocess, 462.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 617/650

0: 384x640 13 persons, 5 cars, 402.0ms
Speed: 4.0ms preprocess, 402.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 618/650

0: 384x640 12 persons, 7 cars, 339.9ms
Speed: 5.1ms preprocess, 339.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 619/650

0: 384x640 12 persons, 7 cars, 359.7ms
Speed: 4.5ms preprocess, 359.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 620/650

0: 384x640 11 persons, 7 cars, 386.1ms
Speed: 5.0ms preprocess, 386.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 621/650

0: 384x640 11 persons, 5 cars, 337.7ms
Speed: 3.4ms preprocess, 337.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 622/650

0: 384x640 12 persons, 7 cars, 442.6ms
Speed: 6.0ms preprocess, 442.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 623/650

0: 384x640 12 persons, 8 cars, 678.8ms
Speed: 5.0ms preprocess, 678.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 624/650

0: 384x640 14 persons, 8 cars, 379.2ms
Speed: 5.0ms preprocess, 379.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 625/650

0: 384x640 13 persons, 9 cars, 366.2ms
Speed: 5.0ms preprocess, 366.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 626/650

0: 384x640 12 persons, 8 cars, 1 handbag, 401.3ms
Speed: 6.0ms preprocess, 401.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 627/650

0: 384x640 12 persons, 8 cars, 1 handbag, 365.4ms
Speed: 4.0ms preprocess, 365.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 628/650

0: 384x640 12 persons, 8 cars, 389.6ms
Speed: 4.0ms preprocess, 389.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 629/650

0: 384x640 12 persons, 9 cars, 364.4ms
Speed: 6.0ms preprocess, 364.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 630/650

0: 384x640 12 persons, 9 cars, 461.6ms
Speed: 4.0ms preprocess, 461.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 631/650

0: 384x640 13 persons, 7 cars, 400.9ms
Speed: 4.0ms preprocess, 400.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 632/650

0: 384x640 14 persons, 9 cars, 357.8ms
Speed: 5.0ms preprocess, 357.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 633/650

0: 384x640 13 persons, 8 cars, 368.7ms
Speed: 5.0ms preprocess, 368.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 634/650

0: 384x640 13 persons, 7 cars, 340.8ms
Speed: 5.3ms preprocess, 340.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 635/650

0: 384x640 13 persons, 7 cars, 344.0ms
Speed: 5.0ms preprocess, 344.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 636/650

0: 384x640 13 persons, 7 cars, 1 handbag, 373.0ms
Speed: 5.0ms preprocess, 373.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 637/650

0: 384x640 13 persons, 7 cars, 1 handbag, 504.9ms
Speed: 11.0ms preprocess, 504.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 638/650

0: 384x640 13 persons, 6 cars, 1 handbag, 395.2ms
Speed: 5.0ms preprocess, 395.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 639/650

0: 384x640 12 persons, 7 cars, 388.1ms
Speed: 4.0ms preprocess, 388.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 640/650

0: 384x640 12 persons, 6 cars, 439.4ms
Speed: 5.0ms preprocess, 439.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 641/650

0: 384x640 12 persons, 6 cars, 1 handbag, 450.6ms
Speed: 6.0ms preprocess, 450.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 642/650

0: 384x640 11 persons, 7 cars, 1 handbag, 354.3ms
Speed: 5.0ms preprocess, 354.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 643/650

0: 384x640 12 persons, 6 cars, 1 handbag, 532.1ms
Speed: 5.0ms preprocess, 532.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 644/650

0: 384x640 11 persons, 6 cars, 1 handbag, 385.8ms
Speed: 6.0ms preprocess, 385.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 645/650

0: 384x640 12 persons, 6 cars, 1 handbag, 390.3ms
Speed: 6.0ms preprocess, 390.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 646/650

0: 384x640 13 persons, 7 cars, 1 handbag, 367.9ms
Speed: 5.0ms preprocess, 367.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 647/650

0: 384x640 12 persons, 7 cars, 1 handbag, 373.0ms
Speed: 5.0ms preprocess, 373.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 648/650

0: 384x640 12 persons, 6 cars, 1 handbag, 365.3ms
Speed: 4.0ms preprocess, 365.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 649/650

0: 384x640 12 persons, 5 cars, 1 handbag, 354.5ms
Speed: 5.0ms preprocess, 354.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 650/650
Object detection completed. Annotated video saved at C:\Users\krna5\Downloads\detected_video.mp4
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ff14e5fa-5cdd-4997-acfb-207fe802d33f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Results Analysis and Explanation: </span>
<span class="n">The</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">process</span> <span class="n">was</span> <span class="n">successful</span><span class="p">,</span> <span class="k">as</span> <span class="n">demonstrated</span> <span class="n">by</span> <span class="n">the</span> <span class="n">detection</span> <span class="n">results</span> <span class="n">on</span> <span class="nb">all</span> <span class="mi">650</span> 
<span class="n">frames</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">video</span><span class="o">.</span>

<span class="n">Detection</span> <span class="n">Results</span><span class="p">:</span>
<span class="n">Detected</span> <span class="n">Objects</span><span class="p">:</span>

<span class="n">Detected</span> <span class="n">objects</span> <span class="n">include</span> <span class="n">persons</span><span class="p">,</span> <span class="n">cars</span><span class="p">,</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">handbag</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">frames</span><span class="o">.</span>
<span class="n">The</span> <span class="n">counts</span> <span class="n">of</span> <span class="n">detected</span> <span class="n">objects</span> <span class="n">per</span> <span class="n">frame</span> <span class="n">vary</span><span class="p">,</span> <span class="n">indicating</span> <span class="n">the</span> <span class="n">dynamic</span> <span class="n">nature</span> <span class="n">of</span> <span class="n">the</span> <span class="n">video</span> <span class="n">scenes</span><span class="o">.</span>
<span class="c1"># Performance Metrics:</span>

<span class="c1"># Inference Time:</span>
<span class="n">The</span> <span class="n">average</span> <span class="n">inference</span> <span class="n">time</span> <span class="n">per</span> <span class="n">frame</span> <span class="ow">is</span> <span class="n">around</span> <span class="mi">365</span><span class="err"></span><span class="mi">375</span> <span class="n">milliseconds</span><span class="o">.</span>
<span class="n">Preprocessing</span><span class="p">:</span> <span class="o">~</span><span class="mi">4</span><span class="o">-</span><span class="mi">5</span> <span class="n">milliseconds</span><span class="o">.</span>
<span class="n">Inference</span><span class="p">:</span> <span class="o">~</span><span class="mi">365</span> <span class="n">milliseconds</span><span class="o">.</span>
<span class="n">Post</span><span class="o">-</span><span class="n">processing</span><span class="p">:</span> <span class="o">~</span><span class="mi">3</span> <span class="n">milliseconds</span><span class="o">.</span>
<span class="c1"># Output File:</span>

<span class="n">The</span> <span class="n">annotated</span> <span class="n">video</span> <span class="p">(</span><span class="n">detected_video</span><span class="o">.</span><span class="n">mp4</span><span class="p">)</span> <span class="n">has</span> <span class="n">been</span> <span class="n">saved</span> <span class="n">successfully</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">specified</span> <span class="n">path</span><span class="o">.</span>

<span class="c1"># Insights:</span>

<span class="n">Consistent</span> <span class="n">Detection</span><span class="p">:</span>

<span class="n">The</span> <span class="n">YOLOv5</span> <span class="n">model</span> <span class="n">performed</span> <span class="n">well</span><span class="p">,</span> <span class="n">consistently</span> <span class="n">identifying</span> <span class="n">key</span> <span class="n">objects</span> <span class="n">across</span> <span class="n">frames</span><span class="o">.</span>
<span class="n">The</span> <span class="n">high</span> <span class="nb">object</span> <span class="n">count</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="mi">12</span><span class="err"></span><span class="mi">13</span> <span class="n">persons</span> <span class="ow">in</span> <span class="n">many</span> <span class="n">frames</span><span class="p">)</span> <span class="n">suggests</span> <span class="n">that</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">effective</span> <span class="ow">in</span>
<span class="n">recognizing</span> <span class="n">people</span> <span class="ow">in</span> <span class="n">various</span> <span class="n">positions</span> <span class="ow">and</span> <span class="n">occlusions</span><span class="o">.</span>

<span class="c1"># Dynamic Object Changes:</span>

<span class="n">Variations</span> <span class="ow">in</span> <span class="n">detected</span> <span class="n">objects</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">fewer</span> <span class="n">cars</span> <span class="ow">or</span> <span class="n">persons</span> <span class="ow">in</span> <span class="n">some</span> <span class="n">frames</span><span class="p">)</span> <span class="n">indicate</span> <span class="n">that</span> <span class="n">the</span> <span class="n">model</span> 
<span class="n">adapts</span> <span class="n">well</span> <span class="n">to</span> <span class="n">changes</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">video</span><span class="err"></span><span class="n">s</span> <span class="n">scene</span> <span class="ow">and</span> <span class="n">content</span><span class="o">.</span>

<span class="c1"># Model Speed:</span>

<span class="n">The</span> <span class="n">inference</span> <span class="n">speed</span> <span class="p">(</span><span class="o">~</span><span class="mi">375</span><span class="n">ms</span> <span class="n">per</span> <span class="n">frame</span><span class="p">)</span> <span class="ow">is</span> <span class="n">reasonable</span> <span class="k">for</span> <span class="n">non</span><span class="o">-</span><span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">batch</span> <span class="n">processing</span> <span class="n">but</span> <span class="n">may</span> <span class="n">require</span> 
<span class="n">optimization</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="o">.</span>

<span class="c1"># Suggestions for Improvement:</span>

<span class="c1"># 1.Evaluate Detection Accuracy:</span>
<span class="n">Visualize</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="ow">and</span> <span class="n">check</span> <span class="k">for</span><span class="p">:</span>
<span class="kc">False</span> <span class="n">Positives</span><span class="p">:</span> <span class="n">Objects</span> <span class="n">detected</span> <span class="n">incorrectly</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">a</span> <span class="n">shadow</span> <span class="n">identified</span> <span class="k">as</span> <span class="n">a</span> <span class="n">car</span><span class="p">)</span><span class="o">.</span>
<span class="kc">False</span> <span class="n">Negatives</span><span class="p">:</span> <span class="n">Missing</span> <span class="n">detections</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">a</span> <span class="n">person</span> <span class="ow">not</span> <span class="n">identified</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">crowded</span> <span class="n">scene</span><span class="p">)</span><span class="o">.</span>
                                     
<span class="c1"># 2. Model Optimization:</span>
<span class="n">If</span> <span class="n">inference</span> <span class="n">time</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">bottleneck</span><span class="p">:</span>
<span class="n">Use</span> <span class="n">YOLOv5n</span> <span class="p">(</span><span class="n">nano</span> <span class="n">version</span><span class="p">)</span> <span class="ow">or</span> <span class="n">YOLOv8n</span> <span class="k">for</span> <span class="n">faster</span> <span class="n">processing</span> <span class="k">with</span> <span class="n">a</span> <span class="n">slight</span> <span class="n">trade</span><span class="o">-</span><span class="n">off</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="o">.</span>
<span class="n">Convert</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">ONNX</span> <span class="ow">or</span> <span class="n">TensorRT</span> <span class="k">for</span> <span class="n">deployment</span> <span class="n">on</span> <span class="n">GPUs</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">performance</span><span class="o">.</span>
                                                       
<span class="c1"># 3. Fine-tune the Model (Optional):</span>
<span class="n">Fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="k">with</span> <span class="n">a</span> <span class="n">custom</span> <span class="n">dataset</span> <span class="k">if</span><span class="p">:</span>
<span class="n">You</span> <span class="n">have</span> <span class="n">domain</span><span class="o">-</span><span class="n">specific</span> <span class="n">objects</span> <span class="n">that</span> <span class="n">the</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">model</span> <span class="n">struggles</span> <span class="n">to</span> <span class="n">detect</span> 
<span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">unusual</span> <span class="ow">or</span> <span class="n">less</span> <span class="n">common</span> <span class="n">objects</span><span class="p">)</span><span class="o">.</span>
<span class="n">The</span> <span class="n">current</span> <span class="n">detections</span> <span class="n">do</span> <span class="ow">not</span> <span class="n">meet</span> <span class="n">accuracy</span> <span class="n">requirements</span> <span class="k">for</span> <span class="n">your</span> <span class="n">application</span><span class="o">.</span>
                                                       
<span class="c1"># 4. Evaluate Using Metrics:</span>
<span class="n">Calculate</span> <span class="n">metrics</span> <span class="n">such</span> <span class="k">as</span> <span class="n">mAP</span> <span class="p">(</span><span class="n">Mean</span> <span class="n">Average</span> <span class="n">Precision</span><span class="p">)</span> <span class="ow">and</span> <span class="n">IoU</span> <span class="p">(</span><span class="n">Intersection</span> <span class="n">over</span> <span class="n">Union</span><span class="p">)</span>
<span class="n">to</span> <span class="n">objectively</span> <span class="n">evaluate</span> <span class="n">detection</span> <span class="n">accuracy</span> <span class="ow">and</span> <span class="n">compare</span> <span class="n">against</span> <span class="n">benchmarks</span><span class="o">.</span>
                               
<span class="c1"># 5. Advanced Techniques:</span>
<span class="n">Incorporate</span> <span class="n">tracking</span> <span class="n">algorithms</span> <span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="n">SORT</span><span class="p">,</span> <span class="n">DeepSORT</span><span class="p">)</span> <span class="n">to</span> <span class="n">track</span> <span class="n">objects</span> <span class="n">across</span> <span class="n">frames</span> <span class="k">for</span>
<span class="n">better</span> <span class="n">scene</span> <span class="n">understanding</span> <span class="ow">and</span> <span class="n">insights</span><span class="o">.</span>
<span class="n">Post</span><span class="o">-</span><span class="n">process</span> <span class="n">results</span> <span class="n">to</span> <span class="n">derive</span> <span class="n">statistics</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">average</span> <span class="n">number</span> <span class="n">of</span> <span class="n">persons</span> <span class="n">per</span> <span class="n">frame</span><span class="p">,</span> <span class="n">most</span> <span class="n">common</span> <span class="nb">object</span> <span class="n">classes</span><span class="p">)</span><span class="o">.</span>
                               
<span class="c1"># 6. Contextual Analysis:</span>
<span class="n">Use</span> <span class="n">domain</span> <span class="n">knowledge</span> <span class="n">to</span> <span class="n">interpret</span> <span class="n">detections</span><span class="p">:</span>
<span class="n">For</span> <span class="n">instance</span><span class="p">,</span> <span class="k">if</span> <span class="n">this</span> <span class="n">project</span> <span class="n">involves</span> <span class="n">safety</span> <span class="ow">in</span> <span class="n">crowded</span> <span class="n">areas</span><span class="p">,</span> <span class="n">analyze</span> <span class="n">the</span> <span class="n">density</span> <span class="n">of</span> <span class="n">persons</span> <span class="n">per</span> <span class="n">frame</span><span class="o">.</span>
<span class="n">For</span> <span class="n">traffic</span> <span class="n">analysis</span><span class="p">,</span> <span class="n">measure</span> <span class="n">car</span> <span class="n">counts</span> <span class="n">across</span> <span class="n">time</span> <span class="n">to</span> <span class="n">identify</span> <span class="n">bottlenecks</span> <span class="ow">or</span> <span class="n">traffic</span> <span class="n">flow</span> <span class="n">patterns</span><span class="o">.</span>
                               
<span class="c1"># Next Steps</span>
<span class="c1"># Review the Annotated Video:</span>

<span class="n">Confirm</span> <span class="n">the</span> <span class="n">quality</span> <span class="n">of</span> <span class="n">detections</span> <span class="ow">and</span> <span class="n">ensure</span> <span class="n">they</span> <span class="n">align</span> <span class="k">with</span> <span class="n">your</span> <span class="n">project</span> <span class="n">goals</span><span class="o">.</span>
<span class="c1"># Apply Metrics for Validation:</span>

<span class="n">Use</span> <span class="n">ground</span><span class="o">-</span><span class="n">truth</span> <span class="n">annotations</span> <span class="p">(</span><span class="k">if</span> <span class="n">available</span><span class="p">)</span> <span class="n">to</span> <span class="n">validate</span> <span class="n">the</span> <span class="n">detection</span> <span class="n">results</span> <span class="n">quantitatively</span><span class="o">.</span>
                               
<span class="c1"># Enhance Deployment Readiness:</span>

<span class="n">Prepare</span> <span class="n">the</span> <span class="n">model</span> <span class="k">for</span> <span class="n">deployment</span> <span class="n">by</span> <span class="n">optimizing</span> <span class="n">performance</span> <span class="ow">and</span> <span class="n">ensuring</span> <span class="n">compatibility</span> <span class="k">with</span>
<span class="n">hardware</span> <span class="ow">and</span> <span class="n">software</span> <span class="n">environments</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=b26adb73-2b55-4dfe-ac93-c4a67a589fc0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Evaluating the detection accuracy by visualizing the annotated video:</span>
<span class="n">Steps</span> <span class="n">to</span> <span class="n">Check</span> <span class="kc">False</span> <span class="n">Positives</span> <span class="ow">and</span> <span class="kc">False</span> <span class="n">Negatives</span>
<span class="mf">1.</span><span class="n">Load</span> <span class="n">the</span> <span class="n">Annotated</span> <span class="n">Video</span>
<span class="n">loading</span> <span class="ow">and</span> <span class="n">play</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="p">(</span><span class="n">detected_video</span><span class="o">.</span><span class="n">mp4</span><span class="p">)</span> <span class="n">using</span> <span class="n">Python</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">media</span> <span class="n">player</span><span class="o">.</span>
<span class="n">How</span> <span class="n">to</span> <span class="n">load</span> <span class="n">it</span> <span class="n">programmatically</span><span class="p">:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ed7b0e9d-2375-4fb2-b51c-852987001f23">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Path to the annotated video</span>
<span class="n">annotated_video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\detected_video.mp4"</span>

<span class="c1"># Open the video</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">annotated_video_path</span><span class="p">)</span>

<span class="k">while</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="k">break</span>
    
    <span class="c1"># Display each frame</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s2">"Annotated Video"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>

    <span class="c1"># Press 'q' to exit</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">'q'</span><span class="p">):</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=56b9d9b0-be20-4d30-90fa-eff3776a01b0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="c1"># Paths</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\30952-383991415_small.mp4"</span>  <span class="c1"># Your actual video path</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\annotated_video.mp4"</span>  <span class="c1"># Output file path</span>

<span class="c1"># Load YOLO model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">'yolov8n.pt'</span><span class="p">)</span>  <span class="c1"># Use a small YOLO model for speed</span>

<span class="c1"># Open video</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">imageio</span><span class="o">.</span><span class="n">get_reader</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>
<span class="n">fps</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">get_meta_data</span><span class="p">()[</span><span class="s1">'fps'</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">imageio</span><span class="o">.</span><span class="n">get_writer</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="n">fps</span><span class="p">)</span>

<span class="c1"># Process each frame</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">frame</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reader</span><span class="p">):</span>
    <span class="c1"># Convert frame to RGB (YOLO expects RGB)</span>
    <span class="n">rgb_frame</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>

    <span class="c1"># Run YOLO on the frame</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rgb_frame</span><span class="p">)</span>

    <span class="c1"># Annotate frame</span>
    <span class="n">annotated_frame</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

    <span class="c1"># Write annotated frame to output video</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">append_data</span><span class="p">(</span><span class="n">annotated_frame</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Processed frame </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Annotated video saved at: </span><span class="si">{</span><span class="n">output_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
0: 384x640 11 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 371.7ms
Speed: 18.4ms preprocess, 371.7ms inference, 21.0ms postprocess per image at shape (1, 3, 384, 640)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (960, 540) to (960, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Processed frame 1

0: 384x640 10 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 233.3ms
Speed: 8.0ms preprocess, 233.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 2

0: 384x640 12 persons, 2 cars, 1 truck, 1 backpack, 1 bottle, 229.7ms
Speed: 5.0ms preprocess, 229.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 3

0: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 223.3ms
Speed: 6.0ms preprocess, 223.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 4

0: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 206.6ms
Speed: 8.0ms preprocess, 206.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 5

0: 384x640 13 persons, 3 cars, 1 truck, 1 bottle, 203.1ms
Speed: 6.3ms preprocess, 203.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 6

0: 384x640 13 persons, 3 cars, 1 truck, 2 bottles, 196.8ms
Speed: 8.0ms preprocess, 196.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 7

0: 384x640 12 persons, 3 cars, 1 truck, 2 bottles, 201.7ms
Speed: 6.6ms preprocess, 201.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 8

0: 384x640 11 persons, 4 cars, 1 truck, 190.5ms
Speed: 11.6ms preprocess, 190.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 9

0: 384x640 10 persons, 3 cars, 1 truck, 1 bottle, 202.5ms
Speed: 9.0ms preprocess, 202.5ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 10

0: 384x640 9 persons, 3 cars, 1 truck, 295.6ms
Speed: 25.9ms preprocess, 295.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 11

0: 384x640 11 persons, 4 cars, 1 truck, 2 bottles, 280.6ms
Speed: 6.3ms preprocess, 280.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 12

0: 384x640 9 persons, 3 cars, 1 truck, 2 bottles, 222.7ms
Speed: 7.4ms preprocess, 222.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 13

0: 384x640 10 persons, 5 cars, 1 truck, 3 bottles, 218.1ms
Speed: 5.0ms preprocess, 218.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 14

0: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 3 bottles, 242.9ms
Speed: 11.0ms preprocess, 242.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 15

0: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 1 bottle, 234.5ms
Speed: 8.0ms preprocess, 234.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 16

0: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 223.7ms
Speed: 6.0ms preprocess, 223.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 17

0: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 231.3ms
Speed: 5.0ms preprocess, 231.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 18

0: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 236.7ms
Speed: 7.1ms preprocess, 236.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 19

0: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 223.2ms
Speed: 6.0ms preprocess, 223.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 20

0: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 217.2ms
Speed: 5.0ms preprocess, 217.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 21

0: 384x640 10 persons, 4 cars, 1 bus, 255.3ms
Speed: 6.0ms preprocess, 255.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 22

0: 384x640 10 persons, 4 cars, 1 bus, 256.2ms
Speed: 6.0ms preprocess, 256.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 23

0: 384x640 10 persons, 5 cars, 1 bus, 243.6ms
Speed: 6.0ms preprocess, 243.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 24

0: 384x640 10 persons, 5 cars, 1 bus, 203.2ms
Speed: 6.0ms preprocess, 203.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 25

0: 384x640 10 persons, 5 cars, 1 bus, 227.1ms
Speed: 5.0ms preprocess, 227.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 26

0: 384x640 11 persons, 5 cars, 1 bus, 453.0ms
Speed: 7.0ms preprocess, 453.0ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 27

0: 384x640 9 persons, 5 cars, 1 bus, 235.5ms
Speed: 7.0ms preprocess, 235.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 28

0: 384x640 10 persons, 5 cars, 1 bus, 202.9ms
Speed: 5.0ms preprocess, 202.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 29

0: 384x640 11 persons, 6 cars, 1 bus, 213.4ms
Speed: 6.0ms preprocess, 213.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 30

0: 384x640 10 persons, 5 cars, 1 bus, 192.5ms
Speed: 7.0ms preprocess, 192.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 31

0: 384x640 10 persons, 7 cars, 1 bus, 207.4ms
Speed: 6.0ms preprocess, 207.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 32

0: 384x640 10 persons, 6 cars, 1 bus, 223.8ms
Speed: 6.0ms preprocess, 223.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 33

0: 384x640 9 persons, 5 cars, 1 bus, 236.0ms
Speed: 7.1ms preprocess, 236.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 34

0: 384x640 9 persons, 5 cars, 1 bus, 190.2ms
Speed: 7.0ms preprocess, 190.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 35

0: 384x640 11 persons, 5 cars, 1 bus, 225.0ms
Speed: 6.9ms preprocess, 225.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 36

0: 384x640 10 persons, 5 cars, 1 bus, 1 truck, 235.7ms
Speed: 6.0ms preprocess, 235.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 37

0: 384x640 12 persons, 5 cars, 1 bus, 242.8ms
Speed: 6.0ms preprocess, 242.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 38

0: 384x640 12 persons, 5 cars, 1 bus, 257.2ms
Speed: 8.0ms preprocess, 257.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 39

0: 384x640 12 persons, 5 cars, 1 bus, 217.2ms
Speed: 6.0ms preprocess, 217.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 40

0: 384x640 12 persons, 5 cars, 1 bus, 244.8ms
Speed: 7.0ms preprocess, 244.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 41

0: 384x640 12 persons, 5 cars, 1 bus, 223.3ms
Speed: 5.0ms preprocess, 223.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 42

0: 384x640 11 persons, 4 cars, 1 bus, 223.6ms
Speed: 7.0ms preprocess, 223.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 43

0: 384x640 12 persons, 4 cars, 1 bus, 1 truck, 233.9ms
Speed: 6.0ms preprocess, 233.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 44

0: 384x640 12 persons, 5 cars, 1 bus, 213.1ms
Speed: 6.0ms preprocess, 213.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 45

0: 384x640 10 persons, 4 cars, 1 bus, 233.7ms
Speed: 5.0ms preprocess, 233.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 46

0: 384x640 10 persons, 2 cars, 1 bus, 224.2ms
Speed: 6.0ms preprocess, 224.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 47

0: 384x640 11 persons, 2 cars, 1 bus, 235.2ms
Speed: 6.0ms preprocess, 235.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 48

0: 384x640 11 persons, 2 cars, 1 bus, 227.0ms
Speed: 5.0ms preprocess, 227.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 49

0: 384x640 11 persons, 1 car, 1 bus, 1 truck, 251.8ms
Speed: 9.0ms preprocess, 251.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 50

0: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 223.1ms
Speed: 5.0ms preprocess, 223.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 51

0: 384x640 11 persons, 2 cars, 1 bus, 198.7ms
Speed: 5.0ms preprocess, 198.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 52

0: 384x640 12 persons, 2 cars, 1 bus, 204.8ms
Speed: 5.0ms preprocess, 204.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 53

0: 384x640 11 persons, 2 cars, 1 bus, 231.1ms
Speed: 5.0ms preprocess, 231.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 54

0: 384x640 12 persons, 2 cars, 1 bus, 218.9ms
Speed: 6.0ms preprocess, 218.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 55

0: 384x640 12 persons, 2 cars, 1 bus, 231.8ms
Speed: 6.0ms preprocess, 231.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 56

0: 384x640 13 persons, 2 cars, 1 bus, 228.1ms
Speed: 7.0ms preprocess, 228.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 57

0: 384x640 14 persons, 1 car, 1 bus, 210.0ms
Speed: 7.0ms preprocess, 210.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 58

0: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 212.1ms
Speed: 6.0ms preprocess, 212.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 59

0: 384x640 13 persons, 2 cars, 1 bus, 229.1ms
Speed: 6.0ms preprocess, 229.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 60

0: 384x640 12 persons, 1 car, 1 bus, 252.0ms
Speed: 6.0ms preprocess, 252.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 61

0: 384x640 11 persons, 1 car, 1 bus, 1 truck, 259.0ms
Speed: 7.6ms preprocess, 259.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 62

0: 384x640 11 persons, 1 car, 1 truck, 230.5ms
Speed: 6.6ms preprocess, 230.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 63

0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 243.2ms
Speed: 5.0ms preprocess, 243.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 64

0: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 212.8ms
Speed: 11.0ms preprocess, 212.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 65

0: 384x640 11 persons, 1 car, 1 bus, 1 truck, 377.7ms
Speed: 7.0ms preprocess, 377.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 66

0: 384x640 12 persons, 2 cars, 1 bus, 218.6ms
Speed: 7.9ms preprocess, 218.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 67

0: 384x640 11 persons, 2 cars, 1 bus, 231.3ms
Speed: 7.0ms preprocess, 231.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 68

0: 384x640 12 persons, 2 cars, 1 bus, 237.7ms
Speed: 6.0ms preprocess, 237.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 69

0: 384x640 13 persons, 2 cars, 1 bus, 195.3ms
Speed: 6.0ms preprocess, 195.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 70

0: 384x640 13 persons, 2 cars, 1 bus, 190.3ms
Speed: 6.0ms preprocess, 190.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 71

0: 384x640 9 persons, 1 car, 1 bus, 229.0ms
Speed: 6.0ms preprocess, 229.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 72

0: 384x640 10 persons, 1 car, 1 bus, 233.2ms
Speed: 7.0ms preprocess, 233.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 73

0: 384x640 11 persons, 1 bus, 236.9ms
Speed: 7.4ms preprocess, 236.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 74

0: 384x640 11 persons, 1 bus, 221.2ms
Speed: 6.0ms preprocess, 221.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 75

0: 384x640 10 persons, 1 bus, 232.6ms
Speed: 7.0ms preprocess, 232.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 76

0: 384x640 10 persons, 1 bus, 218.7ms
Speed: 5.0ms preprocess, 218.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 77

0: 384x640 10 persons, 1 bus, 1 truck, 232.9ms
Speed: 5.9ms preprocess, 232.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 78

0: 384x640 10 persons, 1 bus, 236.8ms
Speed: 6.0ms preprocess, 236.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 79

0: 384x640 10 persons, 1 car, 1 bus, 282.5ms
Speed: 6.0ms preprocess, 282.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 80

0: 384x640 10 persons, 1 car, 1 bus, 1 truck, 236.0ms
Speed: 7.0ms preprocess, 236.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 81

0: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 244.9ms
Speed: 6.0ms preprocess, 244.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 82

0: 384x640 11 persons, 3 cars, 1 bus, 1 truck, 239.1ms
Speed: 5.0ms preprocess, 239.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 83

0: 384x640 9 persons, 2 cars, 1 truck, 1 handbag, 201.8ms
Speed: 6.0ms preprocess, 201.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 84

0: 384x640 11 persons, 1 car, 1 bus, 1 truck, 196.3ms
Speed: 5.6ms preprocess, 196.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 85

0: 384x640 10 persons, 1 car, 1 truck, 195.2ms
Speed: 5.0ms preprocess, 195.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 86

0: 384x640 8 persons, 2 cars, 1 bus, 208.9ms
Speed: 7.0ms preprocess, 208.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 87

0: 384x640 10 persons, 3 cars, 1 bus, 209.1ms
Speed: 6.2ms preprocess, 209.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 88

0: 384x640 9 persons, 3 cars, 1 bus, 199.6ms
Speed: 5.0ms preprocess, 199.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 89

0: 384x640 10 persons, 4 cars, 1 bus, 193.4ms
Speed: 8.0ms preprocess, 193.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 90

0: 384x640 10 persons, 3 cars, 1 bus, 203.9ms
Speed: 6.1ms preprocess, 203.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 91

0: 384x640 10 persons, 5 cars, 1 bus, 198.3ms
Speed: 6.0ms preprocess, 198.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 92

0: 384x640 10 persons, 6 cars, 1 bus, 202.3ms
Speed: 5.0ms preprocess, 202.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 93

0: 384x640 10 persons, 6 cars, 1 bus, 206.1ms
Speed: 6.0ms preprocess, 206.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 94

0: 384x640 11 persons, 6 cars, 1 bus, 1 handbag, 323.9ms
Speed: 4.0ms preprocess, 323.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 95

0: 384x640 10 persons, 7 cars, 1 bus, 209.1ms
Speed: 6.1ms preprocess, 209.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 96

0: 384x640 14 persons, 4 cars, 1 bus, 202.2ms
Speed: 5.5ms preprocess, 202.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 97

0: 384x640 13 persons, 4 cars, 1 bus, 197.6ms
Speed: 5.0ms preprocess, 197.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 98

0: 384x640 13 persons, 4 cars, 1 bus, 198.3ms
Speed: 5.0ms preprocess, 198.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 99

0: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 205.9ms
Speed: 6.0ms preprocess, 205.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 100

0: 384x640 13 persons, 4 cars, 1 bus, 208.9ms
Speed: 4.0ms preprocess, 208.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 101

0: 384x640 10 persons, 4 cars, 1 bus, 1 handbag, 198.6ms
Speed: 5.0ms preprocess, 198.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 102

0: 384x640 11 persons, 4 cars, 1 bus, 201.6ms
Speed: 5.0ms preprocess, 201.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 103

0: 384x640 11 persons, 4 cars, 1 bus, 216.3ms
Speed: 5.0ms preprocess, 216.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 104

0: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 225.5ms
Speed: 5.0ms preprocess, 225.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 105

0: 384x640 11 persons, 4 cars, 1 bus, 2 handbags, 223.8ms
Speed: 6.6ms preprocess, 223.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 106

0: 384x640 12 persons, 4 cars, 1 bus, 222.0ms
Speed: 5.0ms preprocess, 222.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 107

0: 384x640 12 persons, 5 cars, 1 bus, 237.8ms
Speed: 9.5ms preprocess, 237.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 108

0: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 197.9ms
Speed: 5.0ms preprocess, 197.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 109

0: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 202.3ms
Speed: 6.0ms preprocess, 202.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 110

0: 384x640 10 persons, 5 cars, 1 bus, 201.7ms
Speed: 5.0ms preprocess, 201.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 111

0: 384x640 11 persons, 5 cars, 1 bus, 232.2ms
Speed: 5.0ms preprocess, 232.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 112

0: 384x640 10 persons, 5 cars, 1 bus, 230.6ms
Speed: 6.0ms preprocess, 230.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 113

0: 384x640 8 persons, 4 cars, 1 bus, 236.6ms
Speed: 7.0ms preprocess, 236.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 114

0: 384x640 8 persons, 4 cars, 1 bus, 194.5ms
Speed: 5.0ms preprocess, 194.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 115

0: 384x640 8 persons, 4 cars, 1 bus, 208.2ms
Speed: 5.5ms preprocess, 208.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 116

0: 384x640 10 persons, 4 cars, 1 bus, 209.4ms
Speed: 5.0ms preprocess, 209.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 117

0: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 345.5ms
Speed: 5.0ms preprocess, 345.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 118

0: 384x640 10 persons, 4 cars, 1 bus, 186.0ms
Speed: 5.0ms preprocess, 186.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 119

0: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 209.7ms
Speed: 5.0ms preprocess, 209.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 120

0: 384x640 12 persons, 4 cars, 1 bus, 1 backpack, 206.0ms
Speed: 5.0ms preprocess, 206.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 121

0: 384x640 13 persons, 3 cars, 1 bus, 1 backpack, 234.5ms
Speed: 8.0ms preprocess, 234.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 122

0: 384x640 11 persons, 3 cars, 1 bus, 1 backpack, 239.8ms
Speed: 5.0ms preprocess, 239.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 123

0: 384x640 11 persons, 5 cars, 2 motorcycles, 1 bus, 1 backpack, 254.1ms
Speed: 6.0ms preprocess, 254.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 124

0: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 210.3ms
Speed: 6.5ms preprocess, 210.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 125

0: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 201.6ms
Speed: 5.0ms preprocess, 201.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 126

0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 200.5ms
Speed: 4.0ms preprocess, 200.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 127

0: 384x640 12 persons, 5 cars, 1 bus, 1 backpack, 213.4ms
Speed: 6.0ms preprocess, 213.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 128

0: 384x640 10 persons, 5 cars, 1 bus, 1 backpack, 230.5ms
Speed: 5.0ms preprocess, 230.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 129

0: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 209.2ms
Speed: 6.0ms preprocess, 209.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 130

0: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 208.1ms
Speed: 5.0ms preprocess, 208.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 131

0: 384x640 9 persons, 4 cars, 2 motorcycles, 1 bus, 1 backpack, 214.3ms
Speed: 5.0ms preprocess, 214.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 132

0: 384x640 9 persons, 4 cars, 1 bus, 1 backpack, 236.2ms
Speed: 6.0ms preprocess, 236.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 133

0: 384x640 9 persons, 4 cars, 1 bus, 239.1ms
Speed: 6.0ms preprocess, 239.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 134

0: 384x640 9 persons, 3 cars, 1 bus, 1 backpack, 226.8ms
Speed: 5.0ms preprocess, 226.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 135

0: 384x640 10 persons, 3 cars, 1 bus, 1 backpack, 404.5ms
Speed: 6.0ms preprocess, 404.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 136

0: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 1 backpack, 252.7ms
Speed: 5.3ms preprocess, 252.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 137

0: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 205.6ms
Speed: 5.0ms preprocess, 205.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 138

0: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 211.4ms
Speed: 4.0ms preprocess, 211.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 139

0: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 244.7ms
Speed: 6.0ms preprocess, 244.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 140

0: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 236.6ms
Speed: 5.0ms preprocess, 236.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 141

0: 384x640 11 persons, 6 cars, 1 bus, 1 truck, 227.3ms
Speed: 5.0ms preprocess, 227.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 142

0: 384x640 11 persons, 4 cars, 1 bus, 2 trucks, 205.3ms
Speed: 5.0ms preprocess, 205.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 143

0: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 224.2ms
Speed: 5.0ms preprocess, 224.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 144

0: 384x640 13 persons, 4 cars, 1 bus, 2 trucks, 231.7ms
Speed: 4.0ms preprocess, 231.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 145

0: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 1 backpack, 229.9ms
Speed: 5.0ms preprocess, 229.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 146

0: 384x640 12 persons, 5 cars, 1 bus, 1 truck, 231.8ms
Speed: 5.0ms preprocess, 231.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 147

0: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 246.8ms
Speed: 7.0ms preprocess, 246.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 148

0: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 232.4ms
Speed: 6.6ms preprocess, 232.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 149

0: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 1 backpack, 233.1ms
Speed: 7.0ms preprocess, 233.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 150

0: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 1 backpack, 231.8ms
Speed: 6.0ms preprocess, 231.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 151

0: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 254.0ms
Speed: 5.6ms preprocess, 254.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 152

0: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 387.6ms
Speed: 5.0ms preprocess, 387.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 153

0: 384x640 11 persons, 7 cars, 1 bus, 226.4ms
Speed: 5.0ms preprocess, 226.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 154

0: 384x640 11 persons, 7 cars, 1 bus, 226.8ms
Speed: 5.0ms preprocess, 226.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 155

0: 384x640 11 persons, 6 cars, 1 bus, 233.7ms
Speed: 6.0ms preprocess, 233.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 156

0: 384x640 11 persons, 6 cars, 1 bus, 220.8ms
Speed: 5.5ms preprocess, 220.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 157

0: 384x640 10 persons, 6 cars, 1 bus, 1 backpack, 218.1ms
Speed: 4.0ms preprocess, 218.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 158

0: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 230.1ms
Speed: 4.5ms preprocess, 230.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 159

0: 384x640 12 persons, 7 cars, 1 bus, 223.7ms
Speed: 6.0ms preprocess, 223.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 160

0: 384x640 11 persons, 7 cars, 1 bus, 213.9ms
Speed: 4.9ms preprocess, 213.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 161

0: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 223.5ms
Speed: 5.0ms preprocess, 223.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 162

0: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 255.0ms
Speed: 5.0ms preprocess, 255.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 163

0: 384x640 11 persons, 9 cars, 1 motorcycle, 1 bus, 269.3ms
Speed: 5.0ms preprocess, 269.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 164

0: 384x640 11 persons, 8 cars, 1 bus, 281.3ms
Speed: 6.0ms preprocess, 281.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 165

0: 384x640 10 persons, 8 cars, 1 bus, 1 truck, 238.8ms
Speed: 5.0ms preprocess, 238.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 166

0: 384x640 10 persons, 9 cars, 1 bus, 1 truck, 218.6ms
Speed: 5.0ms preprocess, 218.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 167

0: 384x640 9 persons, 9 cars, 1 bus, 1 truck, 429.2ms
Speed: 11.3ms preprocess, 429.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 168

0: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 229.7ms
Speed: 7.0ms preprocess, 229.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 169

0: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 229.6ms
Speed: 4.0ms preprocess, 229.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 170

0: 384x640 12 persons, 8 cars, 1 motorcycle, 1 bus, 216.8ms
Speed: 5.0ms preprocess, 216.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 171

0: 384x640 12 persons, 7 cars, 1 bus, 1 truck, 204.3ms
Speed: 4.0ms preprocess, 204.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 172

0: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 221.4ms
Speed: 4.0ms preprocess, 221.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 173

0: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 253.3ms
Speed: 5.0ms preprocess, 253.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 174

0: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 238.1ms
Speed: 6.0ms preprocess, 238.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 175

0: 384x640 14 persons, 5 cars, 244.0ms
Speed: 5.0ms preprocess, 244.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 176

0: 384x640 12 persons, 5 cars, 1 truck, 221.9ms
Speed: 5.0ms preprocess, 221.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 177

0: 384x640 12 persons, 5 cars, 206.7ms
Speed: 5.0ms preprocess, 206.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 178

0: 384x640 13 persons, 6 cars, 1 truck, 226.0ms
Speed: 5.0ms preprocess, 226.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 179

0: 384x640 13 persons, 7 cars, 240.9ms
Speed: 6.0ms preprocess, 240.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 180

0: 384x640 13 persons, 8 cars, 245.4ms
Speed: 6.0ms preprocess, 245.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 181

0: 384x640 10 persons, 7 cars, 411.1ms
Speed: 7.0ms preprocess, 411.1ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 182

0: 384x640 10 persons, 7 cars, 264.3ms
Speed: 7.0ms preprocess, 264.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 183

0: 384x640 12 persons, 7 cars, 357.7ms
Speed: 8.0ms preprocess, 357.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 184

0: 384x640 13 persons, 7 cars, 335.0ms
Speed: 10.0ms preprocess, 335.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 185

0: 384x640 11 persons, 8 cars, 1 motorcycle, 325.2ms
Speed: 6.0ms preprocess, 325.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 186

0: 384x640 11 persons, 8 cars, 1 truck, 307.5ms
Speed: 9.0ms preprocess, 307.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 187

0: 384x640 11 persons, 7 cars, 1 truck, 306.4ms
Speed: 14.4ms preprocess, 306.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 188

0: 384x640 11 persons, 7 cars, 1 truck, 290.7ms
Speed: 6.2ms preprocess, 290.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 189

0: 384x640 10 persons, 7 cars, 1 truck, 278.8ms
Speed: 6.0ms preprocess, 278.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 190

0: 384x640 12 persons, 7 cars, 1 truck, 287.2ms
Speed: 7.0ms preprocess, 287.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 191

0: 384x640 12 persons, 8 cars, 1 truck, 237.9ms
Speed: 6.5ms preprocess, 237.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 192

0: 384x640 13 persons, 8 cars, 1 truck, 224.1ms
Speed: 6.0ms preprocess, 224.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 193

0: 384x640 12 persons, 7 cars, 1 truck, 211.0ms
Speed: 6.0ms preprocess, 211.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 194

0: 384x640 13 persons, 8 cars, 1 truck, 346.4ms
Speed: 5.0ms preprocess, 346.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 195

0: 384x640 13 persons, 9 cars, 1 truck, 202.3ms
Speed: 6.0ms preprocess, 202.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 196

0: 384x640 14 persons, 8 cars, 1 truck, 211.3ms
Speed: 5.0ms preprocess, 211.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 197

0: 384x640 14 persons, 7 cars, 1 truck, 201.7ms
Speed: 6.0ms preprocess, 201.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 198

0: 384x640 12 persons, 7 cars, 1 truck, 201.2ms
Speed: 5.0ms preprocess, 201.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 199

0: 384x640 13 persons, 7 cars, 1 truck, 233.4ms
Speed: 5.0ms preprocess, 233.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 200

0: 384x640 13 persons, 8 cars, 1 truck, 239.5ms
Speed: 4.0ms preprocess, 239.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 201

0: 384x640 12 persons, 9 cars, 238.6ms
Speed: 5.0ms preprocess, 238.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 202

0: 384x640 13 persons, 8 cars, 235.4ms
Speed: 6.2ms preprocess, 235.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 203

0: 384x640 13 persons, 8 cars, 232.5ms
Speed: 5.0ms preprocess, 232.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 204

0: 384x640 14 persons, 11 cars, 1 motorcycle, 201.3ms
Speed: 6.0ms preprocess, 201.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 205

0: 384x640 15 persons, 9 cars, 211.3ms
Speed: 5.0ms preprocess, 211.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 206

0: 384x640 12 persons, 9 cars, 222.0ms
Speed: 5.0ms preprocess, 222.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 207

0: 384x640 14 persons, 9 cars, 1 suitcase, 424.4ms
Speed: 7.0ms preprocess, 424.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 208

0: 384x640 13 persons, 8 cars, 1 suitcase, 248.5ms
Speed: 6.0ms preprocess, 248.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 209

0: 384x640 13 persons, 8 cars, 192.1ms
Speed: 5.0ms preprocess, 192.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 210

0: 384x640 13 persons, 10 cars, 193.6ms
Speed: 4.4ms preprocess, 193.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 211

0: 384x640 12 persons, 10 cars, 209.6ms
Speed: 5.0ms preprocess, 209.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 212

0: 384x640 13 persons, 9 cars, 199.6ms
Speed: 5.0ms preprocess, 199.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 213

0: 384x640 12 persons, 9 cars, 209.2ms
Speed: 4.9ms preprocess, 209.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 214

0: 384x640 13 persons, 9 cars, 192.3ms
Speed: 5.0ms preprocess, 192.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 215

0: 384x640 13 persons, 9 cars, 203.1ms
Speed: 5.6ms preprocess, 203.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 216

0: 384x640 13 persons, 9 cars, 195.5ms
Speed: 5.0ms preprocess, 195.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 217

0: 384x640 12 persons, 9 cars, 199.3ms
Speed: 5.2ms preprocess, 199.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 218

0: 384x640 13 persons, 8 cars, 185.3ms
Speed: 6.0ms preprocess, 185.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 219

0: 384x640 12 persons, 9 cars, 205.8ms
Speed: 4.5ms preprocess, 205.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 220

0: 384x640 13 persons, 9 cars, 349.4ms
Speed: 6.0ms preprocess, 349.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 221

0: 384x640 13 persons, 9 cars, 203.9ms
Speed: 6.0ms preprocess, 203.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 222

0: 384x640 12 persons, 9 cars, 198.2ms
Speed: 4.0ms preprocess, 198.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 223

0: 384x640 13 persons, 9 cars, 198.3ms
Speed: 5.0ms preprocess, 198.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 224

0: 384x640 12 persons, 9 cars, 210.6ms
Speed: 6.2ms preprocess, 210.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 225

0: 384x640 12 persons, 9 cars, 200.9ms
Speed: 5.0ms preprocess, 200.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 226

0: 384x640 11 persons, 10 cars, 247.5ms
Speed: 3.5ms preprocess, 247.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 227

0: 384x640 12 persons, 12 cars, 218.9ms
Speed: 5.0ms preprocess, 218.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 228

0: 384x640 12 persons, 11 cars, 206.3ms
Speed: 5.0ms preprocess, 206.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 229

0: 384x640 12 persons, 9 cars, 193.0ms
Speed: 5.3ms preprocess, 193.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 230

0: 384x640 12 persons, 11 cars, 220.8ms
Speed: 4.1ms preprocess, 220.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 231

0: 384x640 13 persons, 9 cars, 239.3ms
Speed: 5.0ms preprocess, 239.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 232

0: 384x640 13 persons, 10 cars, 379.7ms
Speed: 6.0ms preprocess, 379.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 233

0: 384x640 12 persons, 9 cars, 255.8ms
Speed: 8.0ms preprocess, 255.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 234

0: 384x640 12 persons, 10 cars, 238.1ms
Speed: 5.0ms preprocess, 238.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 235

0: 384x640 12 persons, 10 cars, 237.6ms
Speed: 10.6ms preprocess, 237.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 236

0: 384x640 13 persons, 9 cars, 208.3ms
Speed: 5.0ms preprocess, 208.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 237

0: 384x640 13 persons, 9 cars, 203.9ms
Speed: 5.0ms preprocess, 203.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 238

0: 384x640 14 persons, 10 cars, 197.9ms
Speed: 5.0ms preprocess, 197.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 239

0: 384x640 14 persons, 10 cars, 1 truck, 226.8ms
Speed: 4.0ms preprocess, 226.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 240

0: 384x640 13 persons, 10 cars, 193.5ms
Speed: 5.0ms preprocess, 193.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 241

0: 384x640 14 persons, 10 cars, 1 truck, 202.3ms
Speed: 4.9ms preprocess, 202.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 242

0: 384x640 14 persons, 9 cars, 1 truck, 197.1ms
Speed: 5.0ms preprocess, 197.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 243

0: 384x640 14 persons, 9 cars, 1 truck, 343.3ms
Speed: 5.7ms preprocess, 343.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 244

0: 384x640 14 persons, 9 cars, 1 truck, 193.5ms
Speed: 4.5ms preprocess, 193.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 245

0: 384x640 14 persons, 9 cars, 1 truck, 202.9ms
Speed: 5.0ms preprocess, 202.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 246

0: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 1 handbag, 203.7ms
Speed: 5.0ms preprocess, 203.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 247

0: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 1 handbag, 211.5ms
Speed: 6.6ms preprocess, 211.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 248

0: 384x640 15 persons, 9 cars, 1 truck, 1 handbag, 226.9ms
Speed: 5.0ms preprocess, 226.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 249

0: 384x640 13 persons, 8 cars, 1 truck, 199.4ms
Speed: 5.0ms preprocess, 199.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 250

0: 384x640 12 persons, 8 cars, 204.2ms
Speed: 5.0ms preprocess, 204.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 251

0: 384x640 13 persons, 8 cars, 1 backpack, 199.9ms
Speed: 6.0ms preprocess, 199.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 252

0: 384x640 15 persons, 9 cars, 1 backpack, 203.1ms
Speed: 4.5ms preprocess, 203.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 253

0: 384x640 14 persons, 9 cars, 320.6ms
Speed: 5.0ms preprocess, 320.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 254

0: 384x640 12 persons, 7 cars, 209.3ms
Speed: 5.0ms preprocess, 209.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 255

0: 384x640 12 persons, 8 cars, 210.6ms
Speed: 4.0ms preprocess, 210.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 256

0: 384x640 12 persons, 9 cars, 197.3ms
Speed: 6.0ms preprocess, 197.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 257

0: 384x640 11 persons, 8 cars, 1 truck, 193.7ms
Speed: 6.4ms preprocess, 193.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 258

0: 384x640 13 persons, 8 cars, 213.9ms
Speed: 4.0ms preprocess, 213.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 259

0: 384x640 12 persons, 8 cars, 1 handbag, 207.8ms
Speed: 5.1ms preprocess, 207.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 260

0: 384x640 13 persons, 8 cars, 1 backpack, 1 handbag, 201.6ms
Speed: 6.0ms preprocess, 201.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 261

0: 384x640 12 persons, 9 cars, 206.3ms
Speed: 7.0ms preprocess, 206.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 262

0: 384x640 12 persons, 8 cars, 1 backpack, 193.5ms
Speed: 5.0ms preprocess, 193.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 263

0: 384x640 13 persons, 8 cars, 1 backpack, 333.1ms
Speed: 5.0ms preprocess, 333.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 264

0: 384x640 12 persons, 8 cars, 196.1ms
Speed: 5.0ms preprocess, 196.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 265

0: 384x640 12 persons, 8 cars, 192.6ms
Speed: 5.0ms preprocess, 192.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 266

0: 384x640 11 persons, 9 cars, 195.9ms
Speed: 5.0ms preprocess, 195.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 267

0: 384x640 12 persons, 9 cars, 206.8ms
Speed: 6.0ms preprocess, 206.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 268

0: 384x640 11 persons, 8 cars, 208.7ms
Speed: 4.0ms preprocess, 208.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 269

0: 384x640 13 persons, 7 cars, 206.8ms
Speed: 4.0ms preprocess, 206.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 270

0: 384x640 10 persons, 10 cars, 197.7ms
Speed: 6.0ms preprocess, 197.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 271

0: 384x640 11 persons, 10 cars, 205.2ms
Speed: 5.5ms preprocess, 205.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 272

0: 384x640 14 persons, 7 cars, 207.8ms
Speed: 5.0ms preprocess, 207.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 273

0: 384x640 15 persons, 6 cars, 198.8ms
Speed: 5.0ms preprocess, 198.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 274

0: 384x640 12 persons, 5 cars, 1 backpack, 352.6ms
Speed: 5.0ms preprocess, 352.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 275

0: 384x640 16 persons, 6 cars, 1 backpack, 286.5ms
Speed: 7.0ms preprocess, 286.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 276

0: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 253.5ms
Speed: 6.0ms preprocess, 253.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 277

0: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 240.8ms
Speed: 5.0ms preprocess, 240.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 278

0: 384x640 13 persons, 7 cars, 1 truck, 1 backpack, 209.1ms
Speed: 5.5ms preprocess, 209.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 279

0: 384x640 12 persons, 9 cars, 1 truck, 1 backpack, 223.9ms
Speed: 6.0ms preprocess, 223.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 280

0: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 214.4ms
Speed: 4.0ms preprocess, 214.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 281

0: 384x640 12 persons, 10 cars, 1 backpack, 1 handbag, 196.1ms
Speed: 5.0ms preprocess, 196.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 282

0: 384x640 11 persons, 9 cars, 1 backpack, 1 handbag, 203.2ms
Speed: 5.0ms preprocess, 203.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 283

0: 384x640 13 persons, 10 cars, 1 backpack, 1 handbag, 330.3ms
Speed: 5.0ms preprocess, 330.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 284

0: 384x640 12 persons, 10 cars, 1 backpack, 213.7ms
Speed: 10.0ms preprocess, 213.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 285

0: 384x640 12 persons, 13 cars, 1 handbag, 220.2ms
Speed: 4.0ms preprocess, 220.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 286

0: 384x640 12 persons, 9 cars, 2 motorcycles, 1 handbag, 219.9ms
Speed: 5.0ms preprocess, 219.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 287

0: 384x640 12 persons, 8 cars, 2 backpacks, 1 handbag, 230.1ms
Speed: 5.0ms preprocess, 230.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 288

0: 384x640 12 persons, 8 cars, 1 motorcycle, 2 backpacks, 203.9ms
Speed: 5.6ms preprocess, 203.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 289

0: 384x640 13 persons, 8 cars, 2 backpacks, 203.0ms
Speed: 5.0ms preprocess, 203.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 290

0: 384x640 14 persons, 7 cars, 1 motorcycle, 2 backpacks, 210.9ms
Speed: 5.0ms preprocess, 210.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 291

0: 384x640 14 persons, 7 cars, 1 backpack, 222.3ms
Speed: 5.0ms preprocess, 222.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 292

0: 384x640 15 persons, 7 cars, 1 backpack, 313.1ms
Speed: 5.0ms preprocess, 313.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 293

0: 384x640 14 persons, 6 cars, 1 backpack, 213.7ms
Speed: 7.0ms preprocess, 213.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 294

0: 384x640 14 persons, 6 cars, 2 motorcycles, 2 backpacks, 201.5ms
Speed: 5.0ms preprocess, 201.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 295

0: 384x640 13 persons, 6 cars, 1 motorcycle, 2 backpacks, 213.6ms
Speed: 5.2ms preprocess, 213.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 296

0: 384x640 14 persons, 7 cars, 2 motorcycles, 2 backpacks, 205.6ms
Speed: 4.0ms preprocess, 205.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 297

0: 384x640 15 persons, 7 cars, 1 motorcycle, 2 backpacks, 208.0ms
Speed: 5.0ms preprocess, 208.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 298

0: 384x640 15 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 207.6ms
Speed: 6.6ms preprocess, 207.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 299

0: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 219.3ms
Speed: 4.0ms preprocess, 219.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 300

0: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 202.0ms
Speed: 5.0ms preprocess, 202.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 301

0: 384x640 15 persons, 7 cars, 1 truck, 1 backpack, 315.7ms
Speed: 5.0ms preprocess, 315.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 302

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 206.7ms
Speed: 5.0ms preprocess, 206.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 303

0: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 212.3ms
Speed: 4.0ms preprocess, 212.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 304

0: 384x640 13 persons, 10 cars, 1 truck, 1 backpack, 193.3ms
Speed: 5.0ms preprocess, 193.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 305

0: 384x640 12 persons, 10 cars, 1 truck, 1 backpack, 209.3ms
Speed: 4.6ms preprocess, 209.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 306

0: 384x640 12 persons, 10 cars, 1 motorcycle, 1 truck, 1 backpack, 208.0ms
Speed: 4.0ms preprocess, 208.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 307

0: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 206.9ms
Speed: 5.0ms preprocess, 206.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 308

0: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 206.6ms
Speed: 5.0ms preprocess, 206.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 309

0: 384x640 13 persons, 8 cars, 1 truck, 1 backpack, 289.3ms
Speed: 5.0ms preprocess, 289.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 310

0: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 211.4ms
Speed: 7.0ms preprocess, 211.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 311

0: 384x640 13 persons, 7 cars, 1 truck, 1 boat, 2 backpacks, 209.8ms
Speed: 6.0ms preprocess, 209.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 312

0: 384x640 14 persons, 7 cars, 1 motorcycle, 1 truck, 193.1ms
Speed: 4.0ms preprocess, 193.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 313

0: 384x640 13 persons, 8 cars, 1 truck, 196.8ms
Speed: 5.0ms preprocess, 196.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 314

0: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 195.8ms
Speed: 5.0ms preprocess, 195.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 315

0: 384x640 14 persons, 9 cars, 1 truck, 1 umbrella, 213.3ms
Speed: 4.9ms preprocess, 213.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 316

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 208.9ms
Speed: 5.0ms preprocess, 208.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 317

0: 384x640 13 persons, 8 cars, 1 truck, 205.2ms
Speed: 5.0ms preprocess, 205.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 318

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 230.0ms
Speed: 7.0ms preprocess, 230.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 319

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 251.5ms
Speed: 7.0ms preprocess, 251.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 320

0: 384x640 15 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 255.6ms
Speed: 5.0ms preprocess, 255.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 321

0: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 232.0ms
Speed: 5.0ms preprocess, 232.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 322

0: 384x640 17 persons, 7 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 209.2ms
Speed: 5.0ms preprocess, 209.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 323

0: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 224.8ms
Speed: 4.0ms preprocess, 224.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 324

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 208.1ms
Speed: 6.0ms preprocess, 208.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 325

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 212.8ms
Speed: 6.0ms preprocess, 212.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 326

0: 384x640 14 persons, 11 cars, 1 motorcycle, 1 truck, 1 backpack, 214.1ms
Speed: 6.0ms preprocess, 214.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 327

0: 384x640 12 persons, 9 cars, 1 motorcycle, 1 truck, 1 backpack, 214.2ms
Speed: 5.0ms preprocess, 214.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 328

0: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 197.2ms
Speed: 5.0ms preprocess, 197.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 329

0: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 210.7ms
Speed: 5.0ms preprocess, 210.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 330

0: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 199.0ms
Speed: 5.0ms preprocess, 199.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 331

0: 384x640 13 persons, 1 bicycle, 11 cars, 1 motorcycle, 2 trucks, 2 backpacks, 203.2ms
Speed: 5.1ms preprocess, 203.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 332

0: 384x640 13 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 203.9ms
Speed: 5.0ms preprocess, 203.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 333

0: 384x640 14 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 210.2ms
Speed: 5.9ms preprocess, 210.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 334

0: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 199.6ms
Speed: 5.6ms preprocess, 199.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 335

0: 384x640 13 persons, 5 cars, 1 motorcycle, 2 trucks, 1 horse, 189.4ms
Speed: 4.0ms preprocess, 189.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 336

0: 384x640 14 persons, 10 cars, 1 motorcycle, 2 trucks, 216.5ms
Speed: 6.0ms preprocess, 216.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 337

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 214.2ms
Speed: 5.5ms preprocess, 214.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 338

0: 384x640 14 persons, 9 cars, 1 motorcycle, 1 truck, 187.3ms
Speed: 5.0ms preprocess, 187.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 339

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 193.1ms
Speed: 5.0ms preprocess, 193.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 340

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 222.3ms
Speed: 6.0ms preprocess, 222.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 341

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 198.2ms
Speed: 5.9ms preprocess, 198.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 342

0: 384x640 13 persons, 8 cars, 1 motorcycle, 2 trucks, 209.1ms
Speed: 5.0ms preprocess, 209.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 343

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 221.1ms
Speed: 6.0ms preprocess, 221.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 344

0: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 211.9ms
Speed: 4.0ms preprocess, 211.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 345

0: 384x640 13 persons, 6 cars, 1 motorcycle, 1 truck, 224.5ms
Speed: 4.0ms preprocess, 224.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 346

0: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 206.4ms
Speed: 5.0ms preprocess, 206.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 347

0: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 206.3ms
Speed: 5.0ms preprocess, 206.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 348

0: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 200.4ms
Speed: 5.0ms preprocess, 200.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 349

0: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 210.6ms
Speed: 5.3ms preprocess, 210.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 350

0: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 206.9ms
Speed: 6.0ms preprocess, 206.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 351

0: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 195.6ms
Speed: 5.0ms preprocess, 195.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 352

0: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 208.0ms
Speed: 5.0ms preprocess, 208.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 353

0: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 219.4ms
Speed: 4.0ms preprocess, 219.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 354

0: 384x640 8 persons, 10 cars, 1 motorcycle, 1 truck, 293.2ms
Speed: 6.0ms preprocess, 293.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 355

0: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 217.5ms
Speed: 6.0ms preprocess, 217.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 356

0: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 201.4ms
Speed: 5.0ms preprocess, 201.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 357

0: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 232.0ms
Speed: 7.0ms preprocess, 232.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 358

0: 384x640 9 persons, 8 cars, 1 motorcycle, 1 truck, 199.6ms
Speed: 5.0ms preprocess, 199.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 359

0: 384x640 9 persons, 10 cars, 1 motorcycle, 1 truck, 211.3ms
Speed: 6.0ms preprocess, 211.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 360

0: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 200.5ms
Speed: 5.0ms preprocess, 200.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 361

0: 384x640 9 persons, 10 cars, 1 truck, 214.8ms
Speed: 6.0ms preprocess, 214.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 362

0: 384x640 10 persons, 7 cars, 1 truck, 204.1ms
Speed: 5.0ms preprocess, 204.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 363

0: 384x640 11 persons, 10 cars, 1 truck, 238.7ms
Speed: 5.0ms preprocess, 238.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 364

0: 384x640 13 persons, 11 cars, 1 truck, 243.0ms
Speed: 5.0ms preprocess, 243.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 365

0: 384x640 11 persons, 10 cars, 1 truck, 260.3ms
Speed: 5.0ms preprocess, 260.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 366

0: 384x640 12 persons, 9 cars, 1 truck, 233.9ms
Speed: 6.0ms preprocess, 233.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 367

0: 384x640 12 persons, 8 cars, 1 truck, 209.6ms
Speed: 4.0ms preprocess, 209.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 368

0: 384x640 14 persons, 8 cars, 1 truck, 219.8ms
Speed: 6.0ms preprocess, 219.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 369

0: 384x640 11 persons, 7 cars, 1 truck, 213.4ms
Speed: 5.0ms preprocess, 213.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 370

0: 384x640 12 persons, 6 cars, 1 truck, 209.3ms
Speed: 6.0ms preprocess, 209.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 371

0: 384x640 15 persons, 8 cars, 1 truck, 194.0ms
Speed: 5.0ms preprocess, 194.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 372

0: 384x640 11 persons, 6 cars, 1 truck, 204.3ms
Speed: 4.5ms preprocess, 204.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 373

0: 384x640 12 persons, 8 cars, 1 truck, 215.6ms
Speed: 5.0ms preprocess, 215.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 374

0: 384x640 13 persons, 9 cars, 1 truck, 221.4ms
Speed: 5.0ms preprocess, 221.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 375

0: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 211.3ms
Speed: 5.1ms preprocess, 211.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 376

0: 384x640 12 persons, 5 cars, 2 trucks, 1 backpack, 199.3ms
Speed: 5.0ms preprocess, 199.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 377

0: 384x640 12 persons, 6 cars, 2 trucks, 1 backpack, 1 handbag, 217.8ms
Speed: 5.0ms preprocess, 217.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 378

0: 384x640 12 persons, 7 cars, 2 trucks, 1 backpack, 222.9ms
Speed: 6.0ms preprocess, 222.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 379

0: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 1 handbag, 194.5ms
Speed: 5.0ms preprocess, 194.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 380

0: 384x640 14 persons, 7 cars, 2 trucks, 203.6ms
Speed: 5.0ms preprocess, 203.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 381

0: 384x640 14 persons, 6 cars, 2 trucks, 1 backpack, 219.4ms
Speed: 6.0ms preprocess, 219.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 382

0: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 195.6ms
Speed: 6.0ms preprocess, 195.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 383

0: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 219.1ms
Speed: 5.5ms preprocess, 219.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 384

0: 384x640 14 persons, 3 cars, 2 trucks, 1 backpack, 202.5ms
Speed: 4.5ms preprocess, 202.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 385

0: 384x640 15 persons, 4 cars, 1 truck, 1 backpack, 217.3ms
Speed: 5.0ms preprocess, 217.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 386

0: 384x640 15 persons, 5 cars, 1 truck, 1 backpack, 208.1ms
Speed: 4.0ms preprocess, 208.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 387

0: 384x640 14 persons, 5 cars, 1 truck, 1 backpack, 212.8ms
Speed: 6.6ms preprocess, 212.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 388

0: 384x640 14 persons, 5 cars, 2 trucks, 1 backpack, 212.8ms
Speed: 5.0ms preprocess, 212.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 389

0: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 214.1ms
Speed: 5.9ms preprocess, 214.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 390

0: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 202.1ms
Speed: 5.0ms preprocess, 202.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 391

0: 384x640 12 persons, 5 cars, 1 truck, 2 backpacks, 214.2ms
Speed: 5.3ms preprocess, 214.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 392

0: 384x640 13 persons, 5 cars, 1 truck, 2 backpacks, 204.1ms
Speed: 3.9ms preprocess, 204.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 393

0: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 357.1ms
Speed: 4.9ms preprocess, 357.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 394

0: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 205.2ms
Speed: 7.0ms preprocess, 205.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 395

0: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 218.1ms
Speed: 4.0ms preprocess, 218.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 396

0: 384x640 14 persons, 8 cars, 2 trucks, 3 backpacks, 200.8ms
Speed: 4.0ms preprocess, 200.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 397

0: 384x640 14 persons, 6 cars, 2 trucks, 2 backpacks, 220.9ms
Speed: 6.0ms preprocess, 220.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 398

0: 384x640 17 persons, 7 cars, 1 truck, 2 backpacks, 221.2ms
Speed: 7.0ms preprocess, 221.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 399

0: 384x640 14 persons, 6 cars, 2 backpacks, 203.6ms
Speed: 6.0ms preprocess, 203.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 400

0: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 209.1ms
Speed: 4.0ms preprocess, 209.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 401

0: 384x640 11 persons, 7 cars, 1 truck, 1 backpack, 216.5ms
Speed: 5.9ms preprocess, 216.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 402

0: 384x640 10 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 215.0ms
Speed: 5.9ms preprocess, 215.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 403

0: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 211.7ms
Speed: 5.0ms preprocess, 211.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 404

0: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 213.0ms
Speed: 5.0ms preprocess, 213.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 405

0: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 227.0ms
Speed: 4.0ms preprocess, 227.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 406

0: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 226.3ms
Speed: 6.0ms preprocess, 226.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 407

0: 384x640 16 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 283.0ms
Speed: 6.0ms preprocess, 283.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 408

0: 384x640 17 persons, 5 cars, 1 truck, 1 backpack, 1 handbag, 239.5ms
Speed: 6.0ms preprocess, 239.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 409

0: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 279.0ms
Speed: 6.0ms preprocess, 279.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 410

0: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 242.9ms
Speed: 6.0ms preprocess, 242.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 411

0: 384x640 15 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 218.1ms
Speed: 6.0ms preprocess, 218.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 412

0: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 234.8ms
Speed: 4.0ms preprocess, 234.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 413

0: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 226.6ms
Speed: 6.0ms preprocess, 226.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 414

0: 384x640 16 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 225.9ms
Speed: 6.0ms preprocess, 225.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 415

0: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 195.2ms
Speed: 5.0ms preprocess, 195.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 416

0: 384x640 14 persons, 7 cars, 1 truck, 2 backpacks, 203.3ms
Speed: 5.3ms preprocess, 203.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 417

0: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 1 suitcase, 221.4ms
Speed: 6.0ms preprocess, 221.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 418

0: 384x640 16 persons, 7 cars, 1 truck, 3 backpacks, 214.0ms
Speed: 5.0ms preprocess, 214.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 419

0: 384x640 15 persons, 8 cars, 1 truck, 3 backpacks, 202.3ms
Speed: 5.0ms preprocess, 202.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 420

0: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 209.6ms
Speed: 5.0ms preprocess, 209.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 421

0: 384x640 13 persons, 7 cars, 1 truck, 2 backpacks, 213.4ms
Speed: 5.0ms preprocess, 213.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 422

0: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 211.2ms
Speed: 5.0ms preprocess, 211.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 423

0: 384x640 14 persons, 8 cars, 2 trucks, 1 backpack, 205.0ms
Speed: 4.0ms preprocess, 205.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 424

0: 384x640 13 persons, 8 cars, 2 trucks, 1 backpack, 213.9ms
Speed: 6.0ms preprocess, 213.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 425

0: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 221.8ms
Speed: 4.5ms preprocess, 221.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 426

0: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 208.9ms
Speed: 5.0ms preprocess, 208.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 427

0: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 196.2ms
Speed: 5.5ms preprocess, 196.2ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 428

0: 384x640 11 persons, 8 cars, 2 trucks, 2 backpacks, 289.1ms
Speed: 14.0ms preprocess, 289.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 429

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 2 backpacks, 217.6ms
Speed: 5.5ms preprocess, 217.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 430

0: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 3 backpacks, 219.4ms
Speed: 4.0ms preprocess, 219.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 431

0: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 205.4ms
Speed: 4.2ms preprocess, 205.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 432

0: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 212.3ms
Speed: 4.0ms preprocess, 212.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 433

0: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 216.3ms
Speed: 6.0ms preprocess, 216.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 434

0: 384x640 12 persons, 7 cars, 1 motorcycle, 215.6ms
Speed: 5.0ms preprocess, 215.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 435

0: 384x640 13 persons, 7 cars, 1 motorcycle, 193.6ms
Speed: 5.0ms preprocess, 193.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 436

0: 384x640 13 persons, 7 cars, 1 motorcycle, 1 backpack, 213.9ms
Speed: 4.5ms preprocess, 213.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 437

0: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 217.0ms
Speed: 5.0ms preprocess, 217.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 438

0: 384x640 15 persons, 6 cars, 1 motorcycle, 1 backpack, 216.1ms
Speed: 6.0ms preprocess, 216.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 439

0: 384x640 14 persons, 6 cars, 1 motorcycle, 1 backpack, 208.0ms
Speed: 5.0ms preprocess, 208.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 440

0: 384x640 13 persons, 7 cars, 2 backpacks, 212.5ms
Speed: 5.0ms preprocess, 212.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 441

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 217.0ms
Speed: 5.0ms preprocess, 217.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 442

0: 384x640 14 persons, 8 cars, 1 motorcycle, 1 backpack, 1 suitcase, 206.9ms
Speed: 6.0ms preprocess, 206.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 443

0: 384x640 14 persons, 8 cars, 1 backpack, 1 suitcase, 194.8ms
Speed: 4.0ms preprocess, 194.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 444

0: 384x640 14 persons, 7 cars, 1 backpack, 1 suitcase, 212.0ms
Speed: 5.0ms preprocess, 212.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 445

0: 384x640 14 persons, 9 cars, 206.8ms
Speed: 6.0ms preprocess, 206.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 446

0: 384x640 14 persons, 6 cars, 216.4ms
Speed: 5.8ms preprocess, 216.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 447

0: 384x640 14 persons, 7 cars, 189.8ms
Speed: 5.0ms preprocess, 189.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 448

0: 384x640 15 persons, 7 cars, 210.0ms
Speed: 7.0ms preprocess, 210.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 449

0: 384x640 15 persons, 7 cars, 1 backpack, 210.0ms
Speed: 5.0ms preprocess, 210.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 450

0: 384x640 14 persons, 7 cars, 226.6ms
Speed: 4.0ms preprocess, 226.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 451

0: 384x640 15 persons, 7 cars, 1 truck, 203.5ms
Speed: 4.0ms preprocess, 203.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 452

0: 384x640 15 persons, 8 cars, 1 truck, 227.9ms
Speed: 6.0ms preprocess, 227.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 453

0: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 239.3ms
Speed: 5.0ms preprocess, 239.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 454

0: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 262.1ms
Speed: 6.6ms preprocess, 262.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 455

0: 384x640 12 persons, 8 cars, 1 backpack, 240.4ms
Speed: 4.5ms preprocess, 240.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 456

0: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 239.0ms
Speed: 6.0ms preprocess, 239.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 457

0: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 226.4ms
Speed: 7.0ms preprocess, 226.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 458

0: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 356.7ms
Speed: 6.0ms preprocess, 356.7ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 459

0: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 226.5ms
Speed: 8.0ms preprocess, 226.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 460

0: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 221.2ms
Speed: 4.0ms preprocess, 221.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 461

0: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 218.6ms
Speed: 5.0ms preprocess, 218.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 462

0: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 216.5ms
Speed: 5.0ms preprocess, 216.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 463

0: 384x640 15 persons, 9 cars, 1 truck, 1 backpack, 197.6ms
Speed: 4.0ms preprocess, 197.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 464

0: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 204.0ms
Speed: 5.0ms preprocess, 204.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 465

0: 384x640 14 persons, 9 cars, 2 backpacks, 221.7ms
Speed: 5.0ms preprocess, 221.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 466

0: 384x640 16 persons, 9 cars, 1 truck, 2 backpacks, 209.0ms
Speed: 5.0ms preprocess, 209.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 467

0: 384x640 17 persons, 9 cars, 1 truck, 2 backpacks, 211.9ms
Speed: 4.9ms preprocess, 211.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 468

0: 384x640 16 persons, 11 cars, 1 truck, 1 backpack, 207.3ms
Speed: 5.0ms preprocess, 207.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 469

0: 384x640 16 persons, 9 cars, 1 truck, 1 backpack, 220.1ms
Speed: 5.0ms preprocess, 220.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 470

0: 384x640 15 persons, 11 cars, 1 backpack, 213.4ms
Speed: 5.0ms preprocess, 213.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 471

0: 384x640 15 persons, 12 cars, 1 backpack, 202.9ms
Speed: 5.0ms preprocess, 202.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 472

0: 384x640 15 persons, 9 cars, 1 backpack, 215.8ms
Speed: 4.0ms preprocess, 215.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 473

0: 384x640 14 persons, 8 cars, 1 backpack, 217.4ms
Speed: 5.0ms preprocess, 217.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 474

0: 384x640 14 persons, 7 cars, 1 backpack, 208.5ms
Speed: 5.0ms preprocess, 208.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 475

0: 384x640 14 persons, 6 cars, 197.4ms
Speed: 5.0ms preprocess, 197.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 476

0: 384x640 15 persons, 6 cars, 1 backpack, 213.8ms
Speed: 5.0ms preprocess, 213.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 477

0: 384x640 14 persons, 7 cars, 208.3ms
Speed: 5.0ms preprocess, 208.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 478

0: 384x640 14 persons, 8 cars, 213.7ms
Speed: 5.0ms preprocess, 213.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 479

0: 384x640 14 persons, 5 cars, 233.9ms
Speed: 5.0ms preprocess, 233.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 480

0: 384x640 15 persons, 5 cars, 230.3ms
Speed: 5.5ms preprocess, 230.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 481

0: 384x640 14 persons, 5 cars, 1 backpack, 239.9ms
Speed: 5.0ms preprocess, 239.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 482

0: 384x640 14 persons, 6 cars, 239.5ms
Speed: 7.0ms preprocess, 239.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 483

0: 384x640 15 persons, 5 cars, 225.9ms
Speed: 5.0ms preprocess, 225.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 484

0: 384x640 14 persons, 5 cars, 238.0ms
Speed: 5.0ms preprocess, 238.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 485

0: 384x640 15 persons, 5 cars, 258.9ms
Speed: 6.0ms preprocess, 258.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 486

0: 384x640 14 persons, 6 cars, 349.3ms
Speed: 5.0ms preprocess, 349.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 487

0: 384x640 14 persons, 6 cars, 1 backpack, 205.5ms
Speed: 7.0ms preprocess, 205.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 488

0: 384x640 15 persons, 6 cars, 1 backpack, 221.0ms
Speed: 4.0ms preprocess, 221.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 489

0: 384x640 15 persons, 7 cars, 218.4ms
Speed: 5.0ms preprocess, 218.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 490

0: 384x640 14 persons, 7 cars, 201.3ms
Speed: 5.0ms preprocess, 201.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 491

0: 384x640 14 persons, 6 cars, 1 backpack, 187.4ms
Speed: 5.0ms preprocess, 187.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 492

0: 384x640 14 persons, 6 cars, 198.6ms
Speed: 6.0ms preprocess, 198.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 493

0: 384x640 15 persons, 6 cars, 224.7ms
Speed: 5.0ms preprocess, 224.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 494

0: 384x640 13 persons, 6 cars, 2 backpacks, 200.2ms
Speed: 5.0ms preprocess, 200.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 495

0: 384x640 15 persons, 6 cars, 1 backpack, 207.6ms
Speed: 5.0ms preprocess, 207.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 496

0: 384x640 16 persons, 7 cars, 227.3ms
Speed: 5.0ms preprocess, 227.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 497

0: 384x640 16 persons, 7 cars, 269.0ms
Speed: 5.0ms preprocess, 269.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 498

0: 384x640 14 persons, 7 cars, 253.0ms
Speed: 6.0ms preprocess, 253.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 499

0: 384x640 15 persons, 7 cars, 239.4ms
Speed: 6.0ms preprocess, 239.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 500

0: 384x640 14 persons, 7 cars, 217.6ms
Speed: 6.0ms preprocess, 217.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 501

0: 384x640 15 persons, 9 cars, 1 backpack, 223.9ms
Speed: 5.0ms preprocess, 223.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 502

0: 384x640 16 persons, 10 cars, 1 backpack, 223.3ms
Speed: 5.0ms preprocess, 223.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 503

0: 384x640 14 persons, 8 cars, 1 backpack, 197.0ms
Speed: 5.0ms preprocess, 197.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 504

0: 384x640 15 persons, 10 cars, 1 backpack, 198.9ms
Speed: 5.0ms preprocess, 198.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 505

0: 384x640 15 persons, 8 cars, 1 backpack, 211.6ms
Speed: 6.0ms preprocess, 211.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 506

0: 384x640 17 persons, 8 cars, 1 backpack, 199.7ms
Speed: 5.0ms preprocess, 199.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 507

0: 384x640 16 persons, 8 cars, 1 backpack, 194.9ms
Speed: 4.0ms preprocess, 194.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 508

0: 384x640 18 persons, 7 cars, 1 backpack, 214.2ms
Speed: 5.0ms preprocess, 214.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 509

0: 384x640 19 persons, 7 cars, 1 backpack, 210.2ms
Speed: 5.0ms preprocess, 210.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 510

0: 384x640 19 persons, 8 cars, 214.8ms
Speed: 6.0ms preprocess, 214.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 511

0: 384x640 19 persons, 7 cars, 196.8ms
Speed: 5.9ms preprocess, 196.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 512

0: 384x640 19 persons, 7 cars, 350.4ms
Speed: 13.0ms preprocess, 350.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 513

0: 384x640 16 persons, 7 cars, 234.3ms
Speed: 7.0ms preprocess, 234.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 514

0: 384x640 16 persons, 8 cars, 1 backpack, 202.4ms
Speed: 5.0ms preprocess, 202.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 515

0: 384x640 17 persons, 8 cars, 1 backpack, 252.7ms
Speed: 9.0ms preprocess, 252.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 516

0: 384x640 16 persons, 7 cars, 1 backpack, 202.6ms
Speed: 4.0ms preprocess, 202.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 517

0: 384x640 15 persons, 8 cars, 1 backpack, 212.6ms
Speed: 5.0ms preprocess, 212.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 518

0: 384x640 18 persons, 8 cars, 208.2ms
Speed: 5.2ms preprocess, 208.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 519

0: 384x640 17 persons, 6 cars, 204.8ms
Speed: 4.1ms preprocess, 204.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 520

0: 384x640 13 persons, 8 cars, 206.6ms
Speed: 6.0ms preprocess, 206.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 521

0: 384x640 14 persons, 9 cars, 1 backpack, 212.1ms
Speed: 6.0ms preprocess, 212.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 522

0: 384x640 14 persons, 8 cars, 1 backpack, 213.7ms
Speed: 5.0ms preprocess, 213.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 523

0: 384x640 15 persons, 8 cars, 2 backpacks, 222.2ms
Speed: 5.9ms preprocess, 222.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 524

0: 384x640 15 persons, 8 cars, 1 backpack, 210.7ms
Speed: 6.1ms preprocess, 210.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 525

0: 384x640 16 persons, 7 cars, 233.1ms
Speed: 7.0ms preprocess, 233.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 526

0: 384x640 16 persons, 7 cars, 202.4ms
Speed: 5.0ms preprocess, 202.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 527

0: 384x640 18 persons, 7 cars, 198.0ms
Speed: 5.0ms preprocess, 198.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 528

0: 384x640 18 persons, 7 cars, 227.4ms
Speed: 5.0ms preprocess, 227.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 529

0: 384x640 19 persons, 7 cars, 224.8ms
Speed: 4.0ms preprocess, 224.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 530

0: 384x640 17 persons, 7 cars, 196.3ms
Speed: 5.2ms preprocess, 196.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 531

0: 384x640 19 persons, 7 cars, 199.4ms
Speed: 5.0ms preprocess, 199.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 532

0: 384x640 21 persons, 8 cars, 1 truck, 1 backpack, 203.0ms
Speed: 5.0ms preprocess, 203.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 533

0: 384x640 20 persons, 9 cars, 1 backpack, 209.2ms
Speed: 6.6ms preprocess, 209.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 534

0: 384x640 19 persons, 9 cars, 1 truck, 1 backpack, 217.3ms
Speed: 5.0ms preprocess, 217.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 535

0: 384x640 18 persons, 7 cars, 1 truck, 203.0ms
Speed: 6.0ms preprocess, 203.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 536

0: 384x640 19 persons, 10 cars, 199.8ms
Speed: 4.0ms preprocess, 199.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 537

0: 384x640 20 persons, 9 cars, 308.0ms
Speed: 13.0ms preprocess, 308.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 538

0: 384x640 20 persons, 8 cars, 214.1ms
Speed: 4.0ms preprocess, 214.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 539

0: 384x640 19 persons, 8 cars, 203.4ms
Speed: 5.0ms preprocess, 203.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 540

0: 384x640 16 persons, 7 cars, 207.9ms
Speed: 5.0ms preprocess, 207.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 541

0: 384x640 17 persons, 7 cars, 207.2ms
Speed: 5.0ms preprocess, 207.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 542

0: 384x640 17 persons, 6 cars, 203.8ms
Speed: 4.0ms preprocess, 203.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 543

0: 384x640 17 persons, 7 cars, 223.7ms
Speed: 5.0ms preprocess, 223.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 544

0: 384x640 17 persons, 7 cars, 231.6ms
Speed: 4.0ms preprocess, 231.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 545

0: 384x640 17 persons, 6 cars, 249.3ms
Speed: 6.0ms preprocess, 249.3ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 546

0: 384x640 19 persons, 7 cars, 271.0ms
Speed: 5.0ms preprocess, 271.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 547

0: 384x640 21 persons, 6 cars, 216.4ms
Speed: 6.0ms preprocess, 216.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 548

0: 384x640 18 persons, 6 cars, 202.6ms
Speed: 6.0ms preprocess, 202.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 549

0: 384x640 18 persons, 7 cars, 220.6ms
Speed: 5.0ms preprocess, 220.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 550

0: 384x640 18 persons, 7 cars, 219.1ms
Speed: 5.0ms preprocess, 219.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 551

0: 384x640 17 persons, 7 cars, 203.0ms
Speed: 5.0ms preprocess, 203.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 552

0: 384x640 16 persons, 7 cars, 210.2ms
Speed: 6.6ms preprocess, 210.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 553

0: 384x640 16 persons, 6 cars, 213.7ms
Speed: 5.0ms preprocess, 213.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 554

0: 384x640 19 persons, 5 cars, 202.8ms
Speed: 5.0ms preprocess, 202.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 555

0: 384x640 18 persons, 5 cars, 217.0ms
Speed: 5.0ms preprocess, 217.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 556

0: 384x640 18 persons, 7 cars, 205.4ms
Speed: 5.0ms preprocess, 205.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 557

0: 384x640 18 persons, 6 cars, 205.0ms
Speed: 5.0ms preprocess, 205.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 558

0: 384x640 18 persons, 6 cars, 191.5ms
Speed: 5.0ms preprocess, 191.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 559

0: 384x640 18 persons, 6 cars, 213.6ms
Speed: 6.0ms preprocess, 213.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 560

0: 384x640 16 persons, 6 cars, 334.2ms
Speed: 5.0ms preprocess, 334.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 561

0: 384x640 18 persons, 6 cars, 201.0ms
Speed: 5.0ms preprocess, 201.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 562

0: 384x640 18 persons, 6 cars, 212.5ms
Speed: 4.0ms preprocess, 212.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 563

0: 384x640 17 persons, 5 cars, 201.5ms
Speed: 6.0ms preprocess, 201.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 564

0: 384x640 15 persons, 5 cars, 186.0ms
Speed: 5.0ms preprocess, 186.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 565

0: 384x640 16 persons, 5 cars, 208.5ms
Speed: 5.0ms preprocess, 208.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 566

0: 384x640 15 persons, 5 cars, 199.9ms
Speed: 4.0ms preprocess, 199.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 567

0: 384x640 14 persons, 5 cars, 194.8ms
Speed: 5.0ms preprocess, 194.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 568

0: 384x640 17 persons, 5 cars, 188.5ms
Speed: 8.0ms preprocess, 188.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 569

0: 384x640 15 persons, 5 cars, 204.4ms
Speed: 5.0ms preprocess, 204.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 570

0: 384x640 17 persons, 5 cars, 208.2ms
Speed: 6.0ms preprocess, 208.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 571

0: 384x640 17 persons, 5 cars, 205.5ms
Speed: 4.0ms preprocess, 205.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 572

0: 384x640 17 persons, 6 cars, 196.4ms
Speed: 5.0ms preprocess, 196.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 573

0: 384x640 16 persons, 6 cars, 193.7ms
Speed: 4.0ms preprocess, 193.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 574

0: 384x640 15 persons, 6 cars, 221.4ms
Speed: 5.0ms preprocess, 221.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 575

0: 384x640 16 persons, 8 cars, 216.0ms
Speed: 5.3ms preprocess, 216.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 576

0: 384x640 17 persons, 7 cars, 192.1ms
Speed: 6.0ms preprocess, 192.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 577

0: 384x640 16 persons, 7 cars, 209.1ms
Speed: 5.0ms preprocess, 209.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 578

0: 384x640 17 persons, 8 cars, 223.8ms
Speed: 6.0ms preprocess, 223.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 579

0: 384x640 16 persons, 8 cars, 192.6ms
Speed: 6.0ms preprocess, 192.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 580

0: 384x640 16 persons, 7 cars, 191.3ms
Speed: 6.0ms preprocess, 191.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 581

0: 384x640 17 persons, 8 cars, 1 handbag, 204.4ms
Speed: 5.0ms preprocess, 204.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 582

0: 384x640 15 persons, 8 cars, 286.9ms
Speed: 5.0ms preprocess, 286.9ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 583

0: 384x640 16 persons, 8 cars, 241.6ms
Speed: 8.0ms preprocess, 241.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 584

0: 384x640 15 persons, 7 cars, 218.7ms
Speed: 5.0ms preprocess, 218.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 585

0: 384x640 16 persons, 9 cars, 204.2ms
Speed: 4.0ms preprocess, 204.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 586

0: 384x640 17 persons, 9 cars, 1 backpack, 231.1ms
Speed: 5.0ms preprocess, 231.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 587

0: 384x640 17 persons, 9 cars, 203.7ms
Speed: 5.0ms preprocess, 203.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 588

0: 384x640 17 persons, 7 cars, 200.1ms
Speed: 6.0ms preprocess, 200.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 589

0: 384x640 16 persons, 7 cars, 207.2ms
Speed: 6.0ms preprocess, 207.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 590

0: 384x640 16 persons, 9 cars, 217.5ms
Speed: 4.0ms preprocess, 217.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 591

0: 384x640 17 persons, 7 cars, 1 handbag, 198.8ms
Speed: 6.0ms preprocess, 198.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 592

0: 384x640 17 persons, 7 cars, 1 handbag, 195.2ms
Speed: 4.5ms preprocess, 195.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 593

0: 384x640 17 persons, 6 cars, 1 handbag, 203.3ms
Speed: 5.0ms preprocess, 203.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 594

0: 384x640 16 persons, 7 cars, 1 handbag, 244.2ms
Speed: 5.0ms preprocess, 244.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 595

0: 384x640 16 persons, 7 cars, 258.1ms
Speed: 6.0ms preprocess, 258.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 596

0: 384x640 17 persons, 7 cars, 244.2ms
Speed: 5.9ms preprocess, 244.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 597

0: 384x640 18 persons, 8 cars, 205.1ms
Speed: 5.0ms preprocess, 205.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 598

0: 384x640 17 persons, 8 cars, 212.5ms
Speed: 6.0ms preprocess, 212.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 599

0: 384x640 17 persons, 9 cars, 1 handbag, 230.5ms
Speed: 6.0ms preprocess, 230.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 600

0: 384x640 17 persons, 6 cars, 202.8ms
Speed: 5.0ms preprocess, 202.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 601

0: 384x640 16 persons, 6 cars, 199.2ms
Speed: 5.0ms preprocess, 199.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 602

0: 384x640 16 persons, 6 cars, 203.8ms
Speed: 5.0ms preprocess, 203.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 603

0: 384x640 16 persons, 7 cars, 215.2ms
Speed: 5.0ms preprocess, 215.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 604

0: 384x640 15 persons, 6 cars, 201.9ms
Speed: 6.0ms preprocess, 201.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 605

0: 384x640 16 persons, 6 cars, 308.5ms
Speed: 5.0ms preprocess, 308.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 606

0: 384x640 16 persons, 5 cars, 218.2ms
Speed: 8.0ms preprocess, 218.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 607

0: 384x640 15 persons, 5 cars, 198.5ms
Speed: 5.0ms preprocess, 198.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 608

0: 384x640 17 persons, 7 cars, 195.7ms
Speed: 5.0ms preprocess, 195.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 609

0: 384x640 16 persons, 7 cars, 209.2ms
Speed: 5.0ms preprocess, 209.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 610

0: 384x640 16 persons, 10 cars, 198.7ms
Speed: 4.0ms preprocess, 198.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 611

0: 384x640 15 persons, 8 cars, 204.2ms
Speed: 5.0ms preprocess, 204.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 612

0: 384x640 15 persons, 8 cars, 190.5ms
Speed: 6.0ms preprocess, 190.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 613

0: 384x640 15 persons, 9 cars, 182.1ms
Speed: 4.0ms preprocess, 182.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 614

0: 384x640 15 persons, 7 cars, 216.6ms
Speed: 5.0ms preprocess, 216.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 615

0: 384x640 14 persons, 7 cars, 1 handbag, 237.0ms
Speed: 5.0ms preprocess, 237.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 616

0: 384x640 14 persons, 6 cars, 1 handbag, 187.8ms
Speed: 5.0ms preprocess, 187.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 617

0: 384x640 16 persons, 6 cars, 183.1ms
Speed: 5.0ms preprocess, 183.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 618

0: 384x640 15 persons, 9 cars, 188.7ms
Speed: 5.0ms preprocess, 188.7ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 619

0: 384x640 15 persons, 8 cars, 213.9ms
Speed: 5.0ms preprocess, 213.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 620

0: 384x640 16 persons, 9 cars, 210.0ms
Speed: 6.0ms preprocess, 210.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 621

0: 384x640 15 persons, 11 cars, 185.1ms
Speed: 4.0ms preprocess, 185.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 622

0: 384x640 16 persons, 10 cars, 1 truck, 191.3ms
Speed: 6.0ms preprocess, 191.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 623

0: 384x640 15 persons, 13 cars, 1 truck, 198.7ms
Speed: 4.0ms preprocess, 198.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 624

0: 384x640 16 persons, 8 cars, 1 truck, 203.3ms
Speed: 5.8ms preprocess, 203.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 625

0: 384x640 14 persons, 10 cars, 1 truck, 206.0ms
Speed: 5.1ms preprocess, 206.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 626

0: 384x640 16 persons, 10 cars, 1 truck, 210.5ms
Speed: 5.0ms preprocess, 210.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 627

0: 384x640 16 persons, 10 cars, 1 truck, 200.8ms
Speed: 6.0ms preprocess, 200.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 628

0: 384x640 16 persons, 11 cars, 1 truck, 342.3ms
Speed: 5.0ms preprocess, 342.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 629

0: 384x640 17 persons, 9 cars, 1 truck, 198.1ms
Speed: 6.0ms preprocess, 198.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 630

0: 384x640 15 persons, 7 cars, 1 truck, 190.0ms
Speed: 5.0ms preprocess, 190.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 631

0: 384x640 17 persons, 9 cars, 1 truck, 1 handbag, 202.1ms
Speed: 5.0ms preprocess, 202.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 632

0: 384x640 15 persons, 9 cars, 1 truck, 223.2ms
Speed: 5.0ms preprocess, 223.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 633

0: 384x640 17 persons, 9 cars, 1 truck, 191.3ms
Speed: 5.0ms preprocess, 191.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 634

0: 384x640 19 persons, 9 cars, 206.1ms
Speed: 5.0ms preprocess, 206.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 635

0: 384x640 15 persons, 9 cars, 203.4ms
Speed: 5.0ms preprocess, 203.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 636

0: 384x640 15 persons, 8 cars, 1 backpack, 230.8ms
Speed: 12.0ms preprocess, 230.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 637

0: 384x640 16 persons, 9 cars, 1 backpack, 198.1ms
Speed: 4.0ms preprocess, 198.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 638

0: 384x640 16 persons, 8 cars, 1 backpack, 216.8ms
Speed: 5.0ms preprocess, 216.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 639

0: 384x640 15 persons, 10 cars, 1 backpack, 225.6ms
Speed: 7.0ms preprocess, 225.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 640

0: 384x640 17 persons, 9 cars, 1 backpack, 253.1ms
Speed: 5.0ms preprocess, 253.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 641

0: 384x640 19 persons, 7 cars, 1 backpack, 264.3ms
Speed: 5.0ms preprocess, 264.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 642

0: 384x640 19 persons, 10 cars, 1 backpack, 1 handbag, 233.9ms
Speed: 7.0ms preprocess, 233.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 643

0: 384x640 19 persons, 10 cars, 1 backpack, 337.7ms
Speed: 7.0ms preprocess, 337.7ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 644

0: 384x640 15 persons, 9 cars, 1 backpack, 1 handbag, 321.2ms
Speed: 8.0ms preprocess, 321.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 645

0: 384x640 15 persons, 7 cars, 1 backpack, 273.6ms
Speed: 7.0ms preprocess, 273.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 646

0: 384x640 17 persons, 9 cars, 1 backpack, 1 handbag, 252.7ms
Speed: 6.0ms preprocess, 252.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 647

0: 384x640 16 persons, 7 cars, 1 handbag, 257.9ms
Speed: 6.0ms preprocess, 257.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 648

0: 384x640 15 persons, 6 cars, 278.9ms
Speed: 7.6ms preprocess, 278.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 649

0: 384x640 15 persons, 5 cars, 318.7ms
Speed: 8.0ms preprocess, 318.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)
Processed frame 650
Annotated video saved at: C:\Users\krna5\Downloads\annotated_video.mp4
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=7c300497-4e3c-4578-9055-f5a11327f515">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The YOLO model successfully processed all 650 frames of your video and annotated objects, </span>
<span class="n">such</span> <span class="k">as</span> <span class="n">persons</span> <span class="ow">and</span> <span class="n">cars</span><span class="p">,</span> <span class="k">with</span> <span class="n">their</span> <span class="n">respective</span> <span class="n">detections</span><span class="o">.</span>

<span class="mf">1.</span> <span class="n">Results</span> <span class="n">Analysis</span>
<span class="n">Detection</span> <span class="n">Accuracy</span>
<span class="n">Detected</span> <span class="n">Objects</span><span class="p">:</span> <span class="n">The</span> <span class="n">model</span> <span class="n">detected</span> <span class="mi">15</span> <span class="n">persons</span> <span class="ow">and</span> <span class="mi">5</span> <span class="n">cars</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">final</span> <span class="n">frame</span><span class="o">.</span>
<span class="n">Processing</span> <span class="n">Speed</span><span class="p">:</span>
<span class="n">Preprocess</span> <span class="n">Time</span><span class="p">:</span> <span class="mi">8</span><span class="n">ms</span>
<span class="n">Inference</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">318.7</span><span class="n">ms</span>
<span class="n">Postprocess</span> <span class="n">Time</span><span class="p">:</span> <span class="mi">4</span><span class="n">ms</span>
<span class="n">Total</span> <span class="n">time</span> <span class="n">per</span> <span class="n">frame</span><span class="p">:</span> <span class="o">~</span><span class="mf">330.7</span><span class="n">ms</span> <span class="p">(</span><span class="n">approx</span><span class="o">.</span> <span class="mi">3</span> <span class="n">FPS</span><span class="p">)</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">suitable</span> <span class="k">for</span> <span class="n">offline</span> <span class="n">processing</span> <span class="n">but</span> <span class="n">may</span>
<span class="n">need</span> <span class="n">optimization</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="o">.</span>
                 
<span class="n">Overall</span><span class="p">:</span> <span class="n">The</span> <span class="n">detection</span> <span class="ow">is</span> <span class="n">consistent</span><span class="p">,</span> <span class="k">with</span> <span class="n">annotations</span> <span class="n">saved</span> <span class="n">to</span> <span class="n">the</span> <span class="n">video</span><span class="o">.</span>
                 
<span class="c1"># Insights</span>
<span class="c1"># Strengths</span>
<span class="n">Detection</span> <span class="n">Performance</span><span class="p">:</span> <span class="n">The</span> <span class="n">model</span> <span class="n">detects</span> <span class="n">multiple</span> <span class="nb">object</span> <span class="n">categories</span> <span class="n">accurately</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">persons</span><span class="p">,</span> <span class="n">cars</span><span class="p">)</span><span class="o">.</span>
<span class="n">Robustness</span> <span class="ow">in</span> <span class="n">Complex</span> <span class="n">Scenes</span><span class="p">:</span> <span class="n">Despite</span> <span class="n">potential</span> <span class="n">challenges</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">lighting</span><span class="p">,</span> <span class="n">occlusion</span><span class="p">),</span> <span class="n">the</span> <span class="n">model</span> 
<span class="n">consistently</span> <span class="n">identifies</span> <span class="n">objects</span><span class="o">.</span>
                 
<span class="c1"># Challenges to Address</span>
<span class="n">Potential</span> <span class="kc">False</span> <span class="n">Positives</span><span class="o">/</span><span class="n">Negatives</span><span class="p">:</span> <span class="n">Verify</span> <span class="n">detection</span> <span class="n">quality</span> <span class="n">visually</span><span class="o">.</span>
<span class="kc">False</span> <span class="n">Positives</span><span class="p">:</span> <span class="n">Objects</span> <span class="n">incorrectly</span> <span class="n">labeled</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">shadows</span> <span class="n">identified</span> <span class="k">as</span> <span class="n">cars</span><span class="p">)</span><span class="o">.</span>
<span class="kc">False</span> <span class="n">Negatives</span><span class="p">:</span> <span class="n">Missed</span> <span class="n">detections</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">persons</span> <span class="ow">in</span> <span class="n">crowded</span> <span class="n">areas</span><span class="p">)</span><span class="o">.</span>
<span class="n">Class</span> <span class="n">Imbalance</span><span class="p">:</span> <span class="n">If</span> <span class="n">one</span> <span class="n">category</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="s2">"persons"</span><span class="p">)</span> <span class="n">dominates</span><span class="p">,</span> <span class="n">it</span> <span class="n">could</span> <span class="n">affect</span> <span class="n">model</span> 
<span class="n">robustness</span> <span class="k">for</span> <span class="n">other</span> <span class="n">categories</span><span class="o">.</span>
<span class="n">Processing</span> <span class="n">Speed</span><span class="p">:</span> <span class="n">While</span> <span class="n">acceptable</span> <span class="k">for</span> <span class="n">batch</span> <span class="n">processing</span><span class="p">,</span> <span class="n">speed</span> <span class="n">may</span> <span class="n">be</span> <span class="n">insufficient</span> 
<span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">scenarios</span><span class="o">.</span>
                                                                     
<span class="c1">#  Suggestions to Improve the Model</span>
<span class="n">Preprocessing</span> <span class="ow">and</span> <span class="n">Training</span> <span class="n">Enhancements</span>
    
<span class="c1"># Enhanced Data Augmentation: </span>
<span class="n">Improve</span> <span class="n">robustness</span> <span class="n">by</span> <span class="n">introducing</span> <span class="n">more</span> <span class="n">augmentations</span> <span class="ow">in</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span><span class="p">:</span>
<span class="n">Weather</span> <span class="n">Simulations</span><span class="p">:</span> <span class="n">Rain</span><span class="p">,</span> <span class="n">fog</span><span class="p">,</span> <span class="ow">or</span> <span class="n">low</span><span class="o">-</span><span class="n">light</span> <span class="n">conditions</span><span class="o">.</span>
    
<span class="c1"># Occlusion: Randomly hide parts of objects to mimic real-world scenarios.</span>
<span class="n">Custom</span> <span class="n">Dataset</span><span class="p">:</span> <span class="n">If</span> <span class="n">available</span><span class="p">,</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="k">with</span> <span class="n">a</span> <span class="n">dataset</span> <span class="n">specific</span> <span class="n">to</span> <span class="n">your</span> <span class="n">domain</span>
<span class="n">to</span> <span class="n">improve</span> <span class="n">precision</span><span class="o">.</span>
<span class="n">Model</span> <span class="n">Optimization</span>
    
<span class="c1"># Use a Larger Model:</span>
<span class="n">If</span> <span class="n">inference</span> <span class="n">speed</span> <span class="n">isn</span> <span class="ow">not</span> <span class="n">critical</span><span class="p">,</span> <span class="k">try</span> <span class="n">yolov8l</span> <span class="ow">or</span> <span class="n">yolov8x</span> <span class="k">for</span> <span class="n">better</span> <span class="n">detection</span> <span class="n">accuracy</span><span class="o">.</span>
<span class="n">Real</span><span class="o">-</span><span class="n">Time</span> <span class="n">Applications</span><span class="p">:</span> <span class="n">Optimize</span> <span class="k">for</span> <span class="n">speed</span> <span class="n">by</span> <span class="n">using</span> <span class="n">YOLO</span><span class="o">-</span><span class="n">Tiny</span> <span class="n">versions</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">yolov8n</span><span class="p">)</span> <span class="ow">or</span>
<span class="n">ONNX</span><span class="o">/</span><span class="n">TensorRT</span> <span class="n">optimizations</span><span class="o">.</span>
    
<span class="c1"># Evaluation and Post-Processing</span>
<span class="n">Quantitative</span> <span class="n">Evaluation</span><span class="p">:</span> <span class="n">Evaluate</span> <span class="n">the</span> <span class="n">model</span> <span class="n">using</span> <span class="n">metrics</span> <span class="n">like</span><span class="p">:</span>
<span class="n">mAP</span> <span class="p">(</span><span class="n">Mean</span> <span class="n">Average</span> <span class="n">Precision</span><span class="p">):</span> <span class="n">Measures</span> <span class="n">detection</span> <span class="n">accuracy</span> <span class="n">across</span> <span class="n">classes</span><span class="o">.</span>
<span class="n">Precision</span> <span class="ow">and</span> <span class="n">Recall</span><span class="p">:</span> <span class="n">Ensure</span> <span class="n">the</span> <span class="n">model</span> <span class="n">isn</span><span class="s1">'t overfitting or underfitting.</span>
<span class="n">Visual</span> <span class="n">Inspection</span><span class="p">:</span> <span class="n">Carefully</span> <span class="n">review</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="k">for</span> <span class="n">misdetections</span> <span class="ow">and</span> <span class="n">address</span> 
<span class="n">patterns</span> <span class="n">of</span> <span class="n">errors</span><span class="o">.</span>
<span class="n">Frame</span><span class="o">-</span><span class="n">by</span><span class="o">-</span><span class="n">Frame</span> <span class="n">Comparison</span><span class="p">:</span> <span class="n">Randomly</span> <span class="n">select</span> <span class="n">frames</span> <span class="ow">and</span> <span class="n">compare</span> <span class="n">detections</span> <span class="k">with</span> <span class="n">ground</span> <span class="n">truth</span> 
<span class="p">(</span><span class="n">manual</span> <span class="n">annotations</span><span class="p">,</span> <span class="k">if</span> <span class="n">available</span><span class="p">)</span><span class="o">.</span>
                                                 
<span class="c1"># Insights to Make the Project Outstanding</span>
                                                 
<span class="n">Explainable</span> <span class="n">AI</span><span class="p">:</span> <span class="n">Visualize</span> <span class="n">detection</span> <span class="n">confidence</span> <span class="k">for</span> <span class="n">each</span> <span class="n">annotation</span> <span class="n">to</span> <span class="n">provide</span> <span class="n">insights</span> <span class="n">into</span> 
<span class="n">model</span> <span class="n">reliability</span><span class="o">.</span>
<span class="c1"># Integration with Tracking: </span>
<span class="n">Combine</span> <span class="nb">object</span> <span class="n">detection</span> <span class="k">with</span> <span class="n">tracking</span> <span class="n">algorithms</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">DeepSORT</span><span class="p">)</span> <span class="n">to</span> <span class="n">handle</span> <span class="nb">object</span> <span class="n">re</span><span class="o">-</span><span class="n">identification</span>
<span class="n">across</span> <span class="n">frames</span><span class="o">.</span>
                                                 
<span class="n">Use</span> <span class="n">Case</span> <span class="n">Deployment</span><span class="p">:</span>
<span class="n">Real</span><span class="o">-</span><span class="n">time</span> <span class="n">surveillance</span><span class="o">.</span>
<span class="n">Autonomous</span> <span class="n">vehicle</span> <span class="n">monitoring</span><span class="o">.</span>
<span class="n">Crowd</span> <span class="n">management</span> <span class="n">analysis</span><span class="o">.</span>
                                                 
<span class="c1"># Comprehensive Documentation: Highlight your preprocessing pipeline, model choice, evaluation</span>
<span class="n">metrics</span><span class="p">,</span> <span class="ow">and</span> <span class="n">error</span> <span class="n">analysis</span> <span class="ow">in</span> <span class="n">your</span> <span class="n">report</span><span class="o">.</span>
                                                 
<span class="c1"># Next Steps</span>
<span class="n">Visualize</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="ow">and</span> <span class="n">assess</span> <span class="n">detection</span> <span class="n">quality</span><span class="o">.</span>
<span class="n">Implement</span> <span class="n">a</span> <span class="n">quantitative</span> <span class="n">evaluation</span> <span class="k">for</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="ow">and</span> <span class="n">mAP</span><span class="o">.</span>
           
<span class="n">Explore</span> <span class="n">fine</span><span class="o">-</span><span class="n">tuning</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">on</span> <span class="n">a</span> <span class="n">custom</span> <span class="n">dataset</span> <span class="n">to</span> <span class="n">improve</span> <span class="n">robustness</span><span class="o">.</span>
<span class="n">By</span> <span class="n">following</span> <span class="n">these</span> <span class="n">suggestions</span><span class="p">,</span> <span class="n">the</span> <span class="n">project</span> <span class="n">can</span> <span class="n">evolve</span> <span class="n">into</span> <span class="n">a</span> <span class="n">robust</span> <span class="ow">and</span> <span class="n">outstanding</span> <span class="nb">object</span> <span class="n">detection</span> 
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=8a4056d8-1504-4183-86e3-b85fbc2cc9e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">Checking</span>  <span class="n">Video</span> <span class="n">File</span> <span class="n">Path</span>
<span class="n">Ensure</span> <span class="n">the</span> <span class="n">video</span> <span class="n">path</span> <span class="p">(</span><span class="n">annotated_video_path</span><span class="p">)</span> <span class="ow">is</span> <span class="n">correct</span> <span class="ow">and</span> <span class="n">accessible</span><span class="o">.</span>

<span class="n">Verifying</span> <span class="k">if</span> <span class="n">the</span> <span class="n">file</span> <span class="n">exists</span> <span class="n">using</span> <span class="n">Python</span><span class="p">:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=e47daf16-d3df-4486-b29a-51ce2cc84787">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\detected_video.mp4"</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">video_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Video file exists!"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Video file not found. Please check the path."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Video file exists!
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=dc1ab301-e867-43c5-96fa-bdcc8ee01796">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>imageio<span class="o">[</span>ffmpeg<span class="o">]</span><span class="w"> </span>opencv-python-headless<span class="w"> </span>ultralytics
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Requirement already satisfied: opencv-python-headless in c:\users\krna5\anaconda3\lib\site-packages (4.10.0.84)
Requirement already satisfied: ultralytics in c:\users\krna5\anaconda3\lib\site-packages (8.3.33)
Requirement already satisfied: imageio[ffmpeg] in c:\users\krna5\anaconda3\lib\site-packages (2.33.1)
Requirement already satisfied: numpy in c:\users\krna5\anaconda3\lib\site-packages (from imageio[ffmpeg]) (1.26.4)
Requirement already satisfied: pillow&gt;=8.3.2 in c:\users\krna5\anaconda3\lib\site-packages (from imageio[ffmpeg]) (10.4.0)
Requirement already satisfied: imageio-ffmpeg in c:\users\krna5\anaconda3\lib\site-packages (from imageio[ffmpeg]) (0.5.1)
Requirement already satisfied: psutil in c:\users\krna5\anaconda3\lib\site-packages (from imageio[ffmpeg]) (6.1.0)
Requirement already satisfied: matplotlib&gt;=3.3.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (3.9.2)
Requirement already satisfied: opencv-python&gt;=4.6.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (4.10.0.84)
Requirement already satisfied: pyyaml&gt;=5.3.1 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (6.0.2)
Requirement already satisfied: requests&gt;=2.23.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (2.32.3)
Requirement already satisfied: scipy&gt;=1.4.1 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (1.14.1)
Requirement already satisfied: torch&gt;=1.8.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (2.5.1)
Requirement already satisfied: torchvision&gt;=0.9.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (0.20.1)
Requirement already satisfied: tqdm&gt;=4.64.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (4.67.0)
Requirement already satisfied: py-cpuinfo in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (9.0.0)
Requirement already satisfied: pandas&gt;=1.1.4 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (2.2.3)
Requirement already satisfied: seaborn&gt;=0.11.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (0.13.2)
Requirement already satisfied: ultralytics-thop&gt;=2.0.0 in c:\users\krna5\anaconda3\lib\site-packages (from ultralytics) (2.0.11)
Requirement already satisfied: contourpy&gt;=1.0.1 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (1.2.0)
Requirement already satisfied: cycler&gt;=0.10 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (4.51.0)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (1.4.4)
Requirement already satisfied: packaging&gt;=20.0 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (24.1)
Requirement already satisfied: pyparsing&gt;=2.3.1 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (3.1.2)
Requirement already satisfied: python-dateutil&gt;=2.7 in c:\users\krna5\anaconda3\lib\site-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in c:\users\krna5\anaconda3\lib\site-packages (from pandas&gt;=1.1.4-&gt;ultralytics) (2024.1)
Requirement already satisfied: tzdata&gt;=2022.7 in c:\users\krna5\anaconda3\lib\site-packages (from pandas&gt;=1.1.4-&gt;ultralytics) (2023.3)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\users\krna5\anaconda3\lib\site-packages (from requests&gt;=2.23.0-&gt;ultralytics) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in c:\users\krna5\anaconda3\lib\site-packages (from requests&gt;=2.23.0-&gt;ultralytics) (3.7)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\users\krna5\anaconda3\lib\site-packages (from requests&gt;=2.23.0-&gt;ultralytics) (2.2.3)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\krna5\anaconda3\lib\site-packages (from requests&gt;=2.23.0-&gt;ultralytics) (2024.8.30)
Requirement already satisfied: filelock in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (3.13.1)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (4.12.2)
Requirement already satisfied: networkx in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (3.3)
Requirement already satisfied: jinja2 in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (3.1.4)
Requirement already satisfied: fsspec in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (2024.6.1)
Requirement already satisfied: setuptools in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (75.6.0)
Requirement already satisfied: sympy==1.13.1 in c:\users\krna5\anaconda3\lib\site-packages (from torch&gt;=1.8.0-&gt;ultralytics) (1.13.1)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in c:\users\krna5\anaconda3\lib\site-packages (from sympy==1.13.1-&gt;torch&gt;=1.8.0-&gt;ultralytics) (1.3.0)
Requirement already satisfied: colorama in c:\users\krna5\anaconda3\lib\site-packages (from tqdm&gt;=4.64.0-&gt;ultralytics) (0.4.6)
Requirement already satisfied: six&gt;=1.5 in c:\users\krna5\anaconda3\lib\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.3.0-&gt;ultralytics) (1.16.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in c:\users\krna5\anaconda3\lib\site-packages (from jinja2-&gt;torch&gt;=1.8.0-&gt;ultralytics) (2.1.3)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=7bfe4ceb-1062-46ac-9bfa-2f570b4b3e5c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The Annotated Video</span>
<span class="n">To</span> <span class="n">visualize</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span><span class="p">,</span> <span class="n">using</span> <span class="n">OpenCV</span> <span class="n">to</span> <span class="n">read</span> <span class="ow">and</span> <span class="n">display</span> <span class="n">the</span> <span class="n">processed</span> <span class="n">video</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8cbee1c5-5e08-462b-b279-e3fc1e5327a8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Path to the annotated video</span>
<span class="n">annotated_video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\annotated_video.mp4"</span>

<span class="c1"># Open the video file</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">annotated_video_path</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Error: Cannot open the video file."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Video opened successfully!"</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"End of video or cannot read frame."</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Frame read successfully."</span><span class="p">)</span>
        
        <span class="c1"># Display the frame</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s2">"Annotated Video"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
        
        <span class="c1"># Press 'q' to exit visualization</span>
        <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">'q'</span><span class="p">):</span>
            <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Video opened successfully!
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
Frame read successfully.
End of video or cannot read frame.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=12b3e618-7030-4444-af96-ef2f6592f940">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The script successfully processed several frames from the video before reaching the end or</span>
<span class="n">encountering</span> <span class="n">an</span> <span class="n">unreadable</span> <span class="n">frame</span><span class="o">.</span> <span class="n">This</span> <span class="n">behavior</span> <span class="ow">is</span> <span class="n">expected</span> <span class="k">if</span> <span class="n">the</span> <span class="n">video</span> <span class="n">has</span> <span class="n">been</span> <span class="n">read</span> <span class="n">completely</span><span class="o">.</span>

<span class="c1"># Confirmation</span>
<span class="n">The</span> <span class="n">output</span> <span class="n">confirms</span> <span class="n">that</span><span class="p">:</span>
<span class="n">The</span> <span class="n">video</span> <span class="n">file</span> <span class="n">was</span> <span class="n">successfully</span> <span class="n">loaded</span><span class="o">.</span>
<span class="n">Multiple</span> <span class="n">frames</span> <span class="n">were</span> <span class="n">read</span> <span class="ow">and</span> <span class="n">processed</span><span class="o">.</span>
<span class="n">The</span> <span class="n">message</span> <span class="s2">"End of video or cannot read frame"</span> <span class="n">simply</span> <span class="n">signals</span> <span class="n">the</span> <span class="n">end</span> <span class="n">of</span> <span class="n">the</span> <span class="n">video</span><span class="o">.</span>
<span class="n">This</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">normal</span> <span class="ow">and</span> <span class="n">expected</span> <span class="n">result</span> <span class="n">when</span> <span class="n">the</span> <span class="n">video</span> <span class="n">has</span> <span class="n">been</span> <span class="n">fully</span> <span class="n">processed</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=15765ce2-0a86-4630-8cd1-b17e079a753b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Visualization: open and play the annotated video</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8d2036ce-669c-4c5a-b271-365ee633e2c6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Path to the annotated video</span>
<span class="n">annotated_video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\annotated_video.mp4"</span>

<span class="c1"># Open video</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">annotated_video_path</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Error: Cannot open video file."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Playing annotated video..."</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"End of video."</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="c1"># Display the frame</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s2">"Annotated Video"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
        
        <span class="c1"># Exit if 'q' is pressed</span>
        <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">'q'</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Video playback interrupted by user."</span><span class="p">)</span>
            <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Playing annotated video...
End of video.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=bff5f0c9-f09a-4a73-aae9-080f805049c8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">The</span> <span class="n">result</span> <span class="n">indicates</span> <span class="n">that</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="n">played</span> <span class="n">successfully</span> <span class="ow">and</span> <span class="n">reached</span> <span class="n">its</span> <span class="n">end</span><span class="o">.</span> <span class="n">This</span> 
<span class="n">confirms</span> <span class="n">that</span> <span class="n">the</span> <span class="n">video</span> <span class="ow">is</span> <span class="n">correctly</span> <span class="n">processed</span> <span class="ow">and</span> <span class="n">viewable</span><span class="o">.</span>

<span class="c1"># Interpretation :</span>
<span class="n">Video</span> <span class="n">Content</span> <span class="n">Review</span>
<span class="n">Detection</span> <span class="n">Performance</span><span class="p">:</span> <span class="n">During</span> <span class="n">playback</span><span class="p">,</span> <span class="n">visually</span> <span class="n">inspect</span> <span class="n">how</span> <span class="n">well</span> <span class="n">the</span> <span class="n">model</span> <span class="n">detected</span> <span class="n">objects</span> 

<span class="ow">in</span> <span class="n">the</span> <span class="n">video</span><span class="o">.</span> <span class="n">Look</span> <span class="k">for</span><span class="p">:</span>
    
<span class="c1"># Accuracy: Are all objects (e.g., persons, cars, etc.) detected correctly?</span>
    
<span class="n">Missed</span> <span class="n">Detections</span><span class="p">:</span> <span class="n">Were</span> <span class="n">there</span> <span class="nb">any</span> <span class="n">objects</span> <span class="n">that</span> <span class="n">the</span> <span class="n">model</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">detect</span> <span class="p">(</span><span class="n">false</span> <span class="n">negatives</span><span class="p">)</span><span class="o">?</span>
    
<span class="n">Incorrect</span> <span class="n">Detections</span><span class="p">:</span> <span class="n">Did</span> <span class="n">the</span> <span class="n">model</span> <span class="n">identify</span> <span class="n">non</span><span class="o">-</span><span class="n">existent</span> <span class="n">objects</span> <span class="p">(</span><span class="n">false</span> <span class="n">positives</span><span class="p">)</span><span class="o">?</span>
    
<span class="n">Frame</span><span class="o">-</span><span class="n">by</span><span class="o">-</span><span class="n">Frame</span> <span class="n">Verification</span>
    
<span class="n">Pause</span> <span class="ow">or</span> <span class="n">slow</span> <span class="n">down</span> <span class="n">the</span> <span class="n">video</span> <span class="n">to</span> <span class="n">inspect</span> <span class="n">detections</span> <span class="n">frame</span><span class="o">-</span><span class="n">by</span><span class="o">-</span><span class="n">frame</span> <span class="k">for</span> <span class="n">detailed</span> <span class="n">analysis</span><span class="o">.</span>
    
<span class="n">Note</span> <span class="k">if</span> <span class="nb">any</span> <span class="n">specific</span> <span class="nb">object</span> <span class="n">classes</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">handbags</span><span class="p">,</span> <span class="n">cars</span><span class="p">)</span> <span class="n">have</span> <span class="n">consistently</span> <span class="n">poor</span> <span class="n">detection</span> <span class="n">quality</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2cb4eb94-aa40-4e98-96bd-668c153e0acb">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=4d4a1b3f-765b-451a-a99c-d678da454a27">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Install dependencies</span>
<span class="c1"># pip install pycocotools</span>

<span class="c1"># Use a COCO evaluation script or integrate with YOLO's native mAP evaluation.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=3edb1445-caf1-4cae-9c6a-57f8dcd6c2c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span> <span class="n">Set</span> <span class="n">Up</span> <span class="k">for</span> <span class="n">Evaluation</span>
<span class="n">Prepare</span> <span class="n">Ground</span> <span class="n">Truth</span>
<span class="n">Manually</span> <span class="n">annotate</span> <span class="n">a</span> <span class="n">small</span> <span class="nb">set</span> <span class="n">of</span> <span class="n">video</span> <span class="n">frames</span> <span class="k">for</span> <span class="n">validation</span><span class="o">.</span> <span class="n">Tools</span> <span class="n">like</span> <span class="n">LabelImg</span> <span class="ow">or</span> <span class="n">CVAT</span> <span class="n">are</span> <span class="n">suitable</span> <span class="k">for</span> <span class="n">this</span> <span class="n">task</span><span class="o">.</span>
<span class="n">Save</span> <span class="n">the</span> <span class="n">annotations</span> <span class="ow">in</span> <span class="n">YOLO</span> <span class="nb">format</span><span class="p">,</span> <span class="n">which</span> <span class="n">consists</span> <span class="n">of</span><span class="p">:</span>
<span class="n">A</span> <span class="n">text</span> <span class="n">file</span> <span class="k">for</span> <span class="n">each</span> <span class="n">image</span><span class="o">.</span>
<span class="n">Each</span> <span class="n">line</span> <span class="n">contains</span><span class="p">:</span> <span class="n">class_id</span> <span class="n">x_center</span> <span class="n">y_center</span> <span class="n">width</span> <span class="n">height</span><span class="o">.</span>
<span class="n">Store</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">folder</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">ground_truth</span><span class="o">/</span><span class="p">)</span><span class="o">.</span>
<span class="mf">1.2</span> <span class="n">Save</span> <span class="n">YOLO</span> <span class="n">Predictions</span>
<span class="n">Modify</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">inference</span> <span class="n">script</span> <span class="n">to</span> <span class="n">save</span> <span class="n">predictions</span> <span class="k">for</span> <span class="n">each</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">text</span> <span class="n">file</span><span class="o">.</span>
<span class="n">Format</span> <span class="n">each</span> <span class="n">line</span> <span class="k">as</span> <span class="n">class_id</span> <span class="n">confidence</span> <span class="n">x_center</span> <span class="n">y_center</span> <span class="n">width</span> <span class="n">height</span><span class="o">.</span>
<span class="n">A</span> <span class="n">code</span> <span class="n">snippet</span> <span class="k">for</span> <span class="n">saving</span> <span class="n">predictions</span><span class="p">:</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0c6c2e95-a9a1-45b6-af9e-40a1b9c55d96">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Object Detection</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=56459f85-fcf5-4abb-addf-b75af411e5eb">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\30952-383991415_small.mp4"</span>

<span class="c1"># Load pre-trained YOLO model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolov8n.pt"</span><span class="p">)</span>  

<span class="c1"># Perform inference on the video</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">video_path</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Inference without saving automatically</span>

<span class="c1"># Initialize predictions list</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Process each frame in results</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="n">frame_detections</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">box</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">boxes</span><span class="p">:</span>  <span class="c1"># Each detection box</span>
        <span class="n">class_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">box</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>  <span class="c1"># Class ID</span>
        <span class="n">confidence</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">box</span><span class="o">.</span><span class="n">conf</span><span class="p">)</span>  <span class="c1"># Confidence score</span>
        <span class="n">x_center</span><span class="p">,</span> <span class="n">y_center</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">box</span><span class="o">.</span><span class="n">xywh</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Bounding box in [x_center, y_center, w, h] format</span>
        <span class="n">frame_detections</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">class_id</span><span class="p">,</span> <span class="n">confidence</span><span class="p">,</span> <span class="n">x_center</span><span class="p">,</span> <span class="n">y_center</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">])</span>
    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame_detections</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>

WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory
errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.

Example:
    results = model(source=..., stream=True)  # generator of Results objects
    for r in results:
        boxes = r.boxes  # Boxes object for bbox outputs
        masks = r.masks  # Masks object for segment masks outputs
        probs = r.probs  # Class probabilities for classification outputs

video 1/1 (frame 1/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 257.7ms
video 1/1 (frame 2/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 222.2ms
video 1/1 (frame 3/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 1 backpack, 1 bottle, 201.9ms
video 1/1 (frame 4/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 203.1ms
video 1/1 (frame 5/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 186.4ms
video 1/1 (frame 6/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 truck, 1 bottle, 192.2ms
video 1/1 (frame 7/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 truck, 2 bottles, 203.0ms
video 1/1 (frame 8/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 3 cars, 1 truck, 2 bottles, 202.9ms
video 1/1 (frame 9/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 truck, 211.5ms
video 1/1 (frame 10/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 truck, 1 bottle, 193.5ms
video 1/1 (frame 11/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 truck, 194.3ms
video 1/1 (frame 12/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 truck, 2 bottles, 194.9ms
video 1/1 (frame 13/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 truck, 2 bottles, 226.9ms
video 1/1 (frame 14/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 truck, 3 bottles, 232.4ms
video 1/1 (frame 15/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 3 bottles, 208.3ms
video 1/1 (frame 16/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 1 bottle, 369.0ms
video 1/1 (frame 17/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 207.0ms
video 1/1 (frame 18/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 209.2ms
video 1/1 (frame 19/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 210.9ms
video 1/1 (frame 20/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 218.4ms
video 1/1 (frame 21/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 217.2ms
video 1/1 (frame 22/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 217.9ms
video 1/1 (frame 23/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 218.1ms
video 1/1 (frame 24/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 250.5ms
video 1/1 (frame 25/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 260.7ms
video 1/1 (frame 26/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 222.0ms
video 1/1 (frame 27/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 208.9ms
video 1/1 (frame 28/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 212.7ms
video 1/1 (frame 29/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 215.3ms
video 1/1 (frame 30/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 219.9ms
video 1/1 (frame 31/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 264.3ms
video 1/1 (frame 32/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 bus, 260.4ms
video 1/1 (frame 33/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 271.7ms
video 1/1 (frame 34/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 320.3ms
video 1/1 (frame 35/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 316.4ms
video 1/1 (frame 36/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 314.0ms
video 1/1 (frame 37/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 1 truck, 297.3ms
video 1/1 (frame 38/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 313.4ms
video 1/1 (frame 39/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 322.5ms
video 1/1 (frame 40/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 342.5ms
video 1/1 (frame 41/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 353.6ms
video 1/1 (frame 42/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 295.5ms
video 1/1 (frame 43/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 356.8ms
video 1/1 (frame 44/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 1 truck, 281.0ms
video 1/1 (frame 45/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 210.9ms
video 1/1 (frame 46/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 207.4ms
video 1/1 (frame 47/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 2 cars, 1 bus, 196.1ms
video 1/1 (frame 48/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 187.4ms
video 1/1 (frame 49/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 193.8ms
video 1/1 (frame 50/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 199.6ms
video 1/1 (frame 51/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 212.4ms
video 1/1 (frame 52/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 182.9ms
video 1/1 (frame 53/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 195.5ms
video 1/1 (frame 54/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 193.5ms
video 1/1 (frame 55/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 212.6ms
video 1/1 (frame 56/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 197.5ms
video 1/1 (frame 57/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 184.6ms
video 1/1 (frame 58/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 1 car, 1 bus, 196.7ms
video 1/1 (frame 59/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 187.3ms
video 1/1 (frame 60/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 227.1ms
video 1/1 (frame 61/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 1 car, 1 bus, 198.1ms
video 1/1 (frame 62/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 186.5ms
video 1/1 (frame 63/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 truck, 198.7ms
video 1/1 (frame 64/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 190.8ms
video 1/1 (frame 65/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 216.4ms
video 1/1 (frame 66/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 202.2ms
video 1/1 (frame 67/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 194.1ms
video 1/1 (frame 68/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 194.4ms
video 1/1 (frame 69/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 185.0ms
video 1/1 (frame 70/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 198.1ms
video 1/1 (frame 71/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 205.4ms
video 1/1 (frame 72/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 1 car, 1 bus, 184.6ms
video 1/1 (frame 73/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 190.3ms
video 1/1 (frame 74/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 bus, 194.6ms
video 1/1 (frame 75/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 bus, 193.9ms
video 1/1 (frame 76/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 196.7ms
video 1/1 (frame 77/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 183.9ms
video 1/1 (frame 78/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 1 truck, 184.3ms
video 1/1 (frame 79/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 192.3ms
video 1/1 (frame 80/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 196.3ms
video 1/1 (frame 81/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 1 truck, 194.2ms
video 1/1 (frame 82/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 197.5ms
video 1/1 (frame 83/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 1 truck, 252.4ms
video 1/1 (frame 84/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 2 cars, 1 truck, 1 handbag, 274.1ms
video 1/1 (frame 85/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 249.5ms
video 1/1 (frame 86/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 truck, 240.6ms
video 1/1 (frame 87/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 2 cars, 1 bus, 203.2ms
video 1/1 (frame 88/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 208.7ms
video 1/1 (frame 89/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 195.4ms
video 1/1 (frame 90/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 206.7ms
video 1/1 (frame 91/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 218.4ms
video 1/1 (frame 92/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 195.5ms
video 1/1 (frame 93/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 203.6ms
video 1/1 (frame 94/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 183.6ms
video 1/1 (frame 95/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 1 handbag, 200.6ms
video 1/1 (frame 96/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 bus, 197.2ms
video 1/1 (frame 97/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 4 cars, 1 bus, 217.6ms
video 1/1 (frame 98/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 187.4ms
video 1/1 (frame 99/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 163.5ms
video 1/1 (frame 100/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 164.1ms
video 1/1 (frame 101/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 153.6ms
video 1/1 (frame 102/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 handbag, 158.1ms
video 1/1 (frame 103/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 174.9ms
video 1/1 (frame 104/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 153.3ms
video 1/1 (frame 105/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 157.0ms
video 1/1 (frame 106/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 2 handbags, 161.4ms
video 1/1 (frame 107/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 156.3ms
video 1/1 (frame 108/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 180.9ms
video 1/1 (frame 109/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 167.9ms
video 1/1 (frame 110/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 154.5ms
video 1/1 (frame 111/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 174.6ms
video 1/1 (frame 112/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 176.1ms
video 1/1 (frame 113/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 174.2ms
video 1/1 (frame 114/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 180.3ms
video 1/1 (frame 115/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 197.5ms
video 1/1 (frame 116/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 201.9ms
video 1/1 (frame 117/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 199.6ms
video 1/1 (frame 118/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 165.8ms
video 1/1 (frame 119/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 172.0ms
video 1/1 (frame 120/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 183.4ms
video 1/1 (frame 121/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 1 backpack, 156.3ms
video 1/1 (frame 122/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 bus, 1 backpack, 181.3ms
video 1/1 (frame 123/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 1 backpack, 171.6ms
video 1/1 (frame 124/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 2 motorcycles, 1 bus, 1 backpack, 176.4ms
video 1/1 (frame 125/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 174.5ms
video 1/1 (frame 126/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 166.4ms
video 1/1 (frame 127/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 160.7ms
video 1/1 (frame 128/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 1 backpack, 167.5ms
video 1/1 (frame 129/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 1 backpack, 160.5ms
video 1/1 (frame 130/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 171.4ms
video 1/1 (frame 131/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 156.2ms
video 1/1 (frame 132/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 2 motorcycles, 1 bus, 1 backpack, 155.7ms
video 1/1 (frame 133/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 bus, 1 backpack, 153.4ms
video 1/1 (frame 134/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 bus, 161.6ms
video 1/1 (frame 135/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 backpack, 158.8ms
video 1/1 (frame 136/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 backpack, 190.0ms
video 1/1 (frame 137/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 1 backpack, 167.8ms
video 1/1 (frame 138/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 163.7ms
video 1/1 (frame 139/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 194.9ms
video 1/1 (frame 140/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 210.8ms
video 1/1 (frame 141/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 221.8ms
video 1/1 (frame 142/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 1 truck, 208.9ms
video 1/1 (frame 143/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 2 trucks, 190.1ms
video 1/1 (frame 144/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 171.3ms
video 1/1 (frame 145/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 2 trucks, 177.4ms
video 1/1 (frame 146/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 1 backpack, 185.9ms
video 1/1 (frame 147/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 1 truck, 172.9ms
video 1/1 (frame 148/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 173.0ms
video 1/1 (frame 149/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 157.6ms
video 1/1 (frame 150/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 1 backpack, 165.2ms
video 1/1 (frame 151/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 1 backpack, 168.4ms
video 1/1 (frame 152/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 160.9ms
video 1/1 (frame 153/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 155.3ms
video 1/1 (frame 154/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 156.6ms
video 1/1 (frame 155/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 159.3ms
video 1/1 (frame 156/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 163.6ms
video 1/1 (frame 157/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 171.9ms
video 1/1 (frame 158/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 backpack, 159.8ms
video 1/1 (frame 159/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 159.6ms
video 1/1 (frame 160/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 bus, 168.3ms
video 1/1 (frame 161/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 187.6ms
video 1/1 (frame 162/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 180.1ms
video 1/1 (frame 163/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 170.8ms
video 1/1 (frame 164/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 1 motorcycle, 1 bus, 184.2ms
video 1/1 (frame 165/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 166.6ms
video 1/1 (frame 166/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 8 cars, 1 bus, 1 truck, 217.5ms
video 1/1 (frame 167/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 9 cars, 1 bus, 1 truck, 250.3ms
video 1/1 (frame 168/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 bus, 1 truck, 261.8ms
video 1/1 (frame 169/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 185.0ms
video 1/1 (frame 170/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 154.1ms
video 1/1 (frame 171/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 bus, 157.7ms
video 1/1 (frame 172/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 bus, 1 truck, 175.1ms
video 1/1 (frame 173/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 169.7ms
video 1/1 (frame 174/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 162.7ms
video 1/1 (frame 175/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 163.9ms
video 1/1 (frame 176/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 163.8ms
video 1/1 (frame 177/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 truck, 163.6ms
video 1/1 (frame 178/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 178.4ms
video 1/1 (frame 179/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 183.5ms
video 1/1 (frame 180/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 177.8ms
video 1/1 (frame 181/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 155.1ms
video 1/1 (frame 182/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 157.2ms
video 1/1 (frame 183/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 169.3ms
video 1/1 (frame 184/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 161.4ms
video 1/1 (frame 185/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 164.1ms
video 1/1 (frame 186/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 motorcycle, 157.1ms
video 1/1 (frame 187/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 truck, 157.7ms
video 1/1 (frame 188/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 155.4ms
video 1/1 (frame 189/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 175.5ms
video 1/1 (frame 190/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 truck, 170.5ms
video 1/1 (frame 191/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 165.9ms
video 1/1 (frame 192/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 158.6ms
video 1/1 (frame 193/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 156.9ms
video 1/1 (frame 194/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 156.7ms
video 1/1 (frame 195/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 179.1ms
video 1/1 (frame 196/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 1 truck, 171.6ms
video 1/1 (frame 197/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 214.1ms
video 1/1 (frame 198/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 227.4ms
video 1/1 (frame 199/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 247.7ms
video 1/1 (frame 200/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 257.7ms
video 1/1 (frame 201/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 179.7ms
video 1/1 (frame 202/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 177.1ms
video 1/1 (frame 203/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 194.4ms
video 1/1 (frame 204/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 198.2ms
video 1/1 (frame 205/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 11 cars, 1 motorcycle, 185.2ms
video 1/1 (frame 206/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 182.0ms
video 1/1 (frame 207/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 157.0ms
video 1/1 (frame 208/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 suitcase, 154.8ms
video 1/1 (frame 209/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 suitcase, 155.9ms
video 1/1 (frame 210/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 180.3ms
video 1/1 (frame 211/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 156.3ms
video 1/1 (frame 212/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 156.7ms
video 1/1 (frame 213/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 162.3ms
video 1/1 (frame 214/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 160.0ms
video 1/1 (frame 215/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 172.3ms
video 1/1 (frame 216/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 184.8ms
video 1/1 (frame 217/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 169.8ms
video 1/1 (frame 218/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 172.4ms
video 1/1 (frame 219/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 175.3ms
video 1/1 (frame 220/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 162.5ms
video 1/1 (frame 221/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 182.6ms
video 1/1 (frame 222/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 164.0ms
video 1/1 (frame 223/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 163.2ms
video 1/1 (frame 224/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 164.8ms
video 1/1 (frame 225/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 165.3ms
video 1/1 (frame 226/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 166.6ms
video 1/1 (frame 227/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 176.4ms
video 1/1 (frame 228/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 12 cars, 154.9ms
video 1/1 (frame 229/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 11 cars, 158.0ms
video 1/1 (frame 230/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 160.3ms
video 1/1 (frame 231/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 11 cars, 156.6ms
video 1/1 (frame 232/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 167.4ms
video 1/1 (frame 233/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 168.1ms
video 1/1 (frame 234/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 156.2ms
video 1/1 (frame 235/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 156.6ms
video 1/1 (frame 236/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 155.0ms
video 1/1 (frame 237/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 339.6ms
video 1/1 (frame 238/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 166.2ms
video 1/1 (frame 239/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 152.5ms
video 1/1 (frame 240/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 159.1ms
video 1/1 (frame 241/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 152.8ms
video 1/1 (frame 242/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 161.1ms
video 1/1 (frame 243/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 177.4ms
video 1/1 (frame 244/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 171.6ms
video 1/1 (frame 245/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 158.6ms
video 1/1 (frame 246/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 160.1ms
video 1/1 (frame 247/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 1 handbag, 156.6ms
video 1/1 (frame 248/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 1 handbag, 157.9ms
video 1/1 (frame 249/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 1 handbag, 173.2ms
video 1/1 (frame 250/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 173.7ms
video 1/1 (frame 251/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 159.9ms
video 1/1 (frame 252/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 156.5ms
video 1/1 (frame 253/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 154.8ms
video 1/1 (frame 254/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 165.7ms
video 1/1 (frame 255/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 207.5ms
video 1/1 (frame 256/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 235.3ms
video 1/1 (frame 257/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 241.8ms
video 1/1 (frame 258/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 truck, 258.0ms
video 1/1 (frame 259/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 176.6ms
video 1/1 (frame 260/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 handbag, 175.6ms
video 1/1 (frame 261/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 1 handbag, 186.8ms
video 1/1 (frame 262/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 175.4ms
video 1/1 (frame 263/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 backpack, 167.8ms
video 1/1 (frame 264/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 179.7ms
video 1/1 (frame 265/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 176.2ms
video 1/1 (frame 266/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 159.0ms
video 1/1 (frame 267/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 158.7ms
video 1/1 (frame 268/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 154.9ms
video 1/1 (frame 269/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 162.4ms
video 1/1 (frame 270/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 168.8ms
video 1/1 (frame 271/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 10 cars, 178.3ms
video 1/1 (frame 272/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 170.7ms
video 1/1 (frame 273/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 173.9ms
video 1/1 (frame 274/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 169.9ms
video 1/1 (frame 275/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 backpack, 181.1ms
video 1/1 (frame 276/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 backpack, 153.1ms
video 1/1 (frame 277/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 162.4ms
video 1/1 (frame 278/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 156.1ms
video 1/1 (frame 279/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 1 backpack, 156.2ms
video 1/1 (frame 280/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 truck, 1 backpack, 155.1ms
video 1/1 (frame 281/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 170.1ms
video 1/1 (frame 282/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 backpack, 1 handbag, 155.6ms
video 1/1 (frame 283/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 1 backpack, 1 handbag, 166.6ms
video 1/1 (frame 284/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 1 backpack, 1 handbag, 160.2ms
video 1/1 (frame 285/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 backpack, 155.6ms
video 1/1 (frame 286/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 13 cars, 1 handbag, 159.4ms
video 1/1 (frame 287/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 2 motorcycles, 1 handbag, 182.9ms
video 1/1 (frame 288/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 2 backpacks, 1 handbag, 165.5ms
video 1/1 (frame 289/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 2 backpacks, 168.9ms
video 1/1 (frame 290/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 2 backpacks, 159.6ms
video 1/1 (frame 291/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 2 backpacks, 165.2ms
video 1/1 (frame 292/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 174.6ms
video 1/1 (frame 293/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 169.5ms
video 1/1 (frame 294/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 161.1ms
video 1/1 (frame 295/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 motorcycles, 2 backpacks, 168.2ms
video 1/1 (frame 296/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 motorcycle, 2 backpacks, 159.3ms
video 1/1 (frame 297/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 2 motorcycles, 2 backpacks, 157.3ms
video 1/1 (frame 298/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 motorcycle, 2 backpacks, 177.8ms
video 1/1 (frame 299/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 154.4ms
video 1/1 (frame 300/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 162.3ms
video 1/1 (frame 301/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 158.7ms
video 1/1 (frame 302/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 1 backpack, 154.9ms
video 1/1 (frame 303/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 156.7ms
video 1/1 (frame 304/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 186.2ms
video 1/1 (frame 305/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 1 truck, 1 backpack, 168.7ms
video 1/1 (frame 306/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 truck, 1 backpack, 170.7ms
video 1/1 (frame 307/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 motorcycle, 1 truck, 1 backpack, 169.1ms
video 1/1 (frame 308/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 165.7ms
video 1/1 (frame 309/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 176.1ms
video 1/1 (frame 310/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 1 backpack, 160.6ms
video 1/1 (frame 311/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 207.9ms
video 1/1 (frame 312/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 1 boat, 2 backpacks, 231.7ms
video 1/1 (frame 313/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 1 truck, 182.7ms
video 1/1 (frame 314/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 219.3ms
video 1/1 (frame 315/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 231.7ms
video 1/1 (frame 316/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 umbrella, 223.0ms
video 1/1 (frame 317/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 204.9ms
video 1/1 (frame 318/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 174.3ms
video 1/1 (frame 319/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 173.4ms
video 1/1 (frame 320/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 175.1ms
video 1/1 (frame 321/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 174.6ms
video 1/1 (frame 322/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 175.9ms
video 1/1 (frame 323/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 175.9ms
video 1/1 (frame 324/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 175.7ms
video 1/1 (frame 325/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 165.5ms
video 1/1 (frame 326/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 154.0ms
video 1/1 (frame 327/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 11 cars, 1 motorcycle, 1 truck, 1 backpack, 158.0ms
video 1/1 (frame 328/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 1 truck, 1 backpack, 157.5ms
video 1/1 (frame 329/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 153.1ms
video 1/1 (frame 330/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 167.3ms
video 1/1 (frame 331/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 157.0ms
video 1/1 (frame 332/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 1 bicycle, 11 cars, 1 motorcycle, 2 trucks, 2 backpacks, 156.6ms
video 1/1 (frame 333/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 321.4ms
video 1/1 (frame 334/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 165.1ms
video 1/1 (frame 335/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 164.3ms
video 1/1 (frame 336/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 motorcycle, 2 trucks, 1 horse, 201.4ms
video 1/1 (frame 337/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 motorcycle, 2 trucks, 184.7ms
video 1/1 (frame 338/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 192.7ms
video 1/1 (frame 339/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 motorcycle, 1 truck, 158.4ms
video 1/1 (frame 340/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 184.2ms
video 1/1 (frame 341/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 173.5ms
video 1/1 (frame 342/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 157.2ms
video 1/1 (frame 343/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 2 trucks, 165.0ms
video 1/1 (frame 344/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 178.1ms
video 1/1 (frame 345/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 177.7ms
video 1/1 (frame 346/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 motorcycle, 1 truck, 166.8ms
video 1/1 (frame 347/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 158.4ms
video 1/1 (frame 348/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 153.8ms
video 1/1 (frame 349/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 156.0ms
video 1/1 (frame 350/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 167.5ms
video 1/1 (frame 351/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 178.7ms
video 1/1 (frame 352/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 157.1ms
video 1/1 (frame 353/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 154.5ms
video 1/1 (frame 354/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 151.9ms
video 1/1 (frame 355/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 10 cars, 1 motorcycle, 1 truck, 156.9ms
video 1/1 (frame 356/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 158.4ms
video 1/1 (frame 357/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 182.1ms
video 1/1 (frame 358/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 163.0ms
video 1/1 (frame 359/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 8 cars, 1 motorcycle, 1 truck, 155.8ms
video 1/1 (frame 360/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 10 cars, 1 motorcycle, 1 truck, 156.8ms
video 1/1 (frame 361/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 170.9ms
video 1/1 (frame 362/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 10 cars, 1 truck, 171.0ms
video 1/1 (frame 363/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 truck, 169.0ms
video 1/1 (frame 364/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 1 truck, 160.1ms
video 1/1 (frame 365/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 11 cars, 1 truck, 156.6ms
video 1/1 (frame 366/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 1 truck, 159.2ms
video 1/1 (frame 367/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 truck, 160.1ms
video 1/1 (frame 368/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 169.4ms
video 1/1 (frame 369/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 169.4ms
video 1/1 (frame 370/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 162.2ms
video 1/1 (frame 371/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 153.3ms
video 1/1 (frame 372/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 182.5ms
video 1/1 (frame 373/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 truck, 207.4ms
video 1/1 (frame 374/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 238.3ms
video 1/1 (frame 375/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 1 truck, 222.8ms
video 1/1 (frame 376/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 198.2ms
video 1/1 (frame 377/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 2 trucks, 1 backpack, 186.6ms
video 1/1 (frame 378/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 2 trucks, 1 backpack, 1 handbag, 185.0ms
video 1/1 (frame 379/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 2 trucks, 1 backpack, 172.0ms
video 1/1 (frame 380/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 1 handbag, 173.9ms
video 1/1 (frame 381/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 2 trucks, 172.0ms
video 1/1 (frame 382/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 trucks, 1 backpack, 156.7ms
video 1/1 (frame 383/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 159.3ms
video 1/1 (frame 384/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 176.5ms
video 1/1 (frame 385/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 3 cars, 2 trucks, 1 backpack, 158.2ms
video 1/1 (frame 386/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 1 truck, 1 backpack, 157.5ms
video 1/1 (frame 387/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 1 truck, 1 backpack, 168.9ms
video 1/1 (frame 388/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 1 truck, 1 backpack, 154.9ms
video 1/1 (frame 389/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 2 trucks, 1 backpack, 159.6ms
video 1/1 (frame 390/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 175.3ms
video 1/1 (frame 391/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 156.5ms
video 1/1 (frame 392/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 truck, 2 backpacks, 151.9ms
video 1/1 (frame 393/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 2 backpacks, 159.6ms
video 1/1 (frame 394/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 158.3ms
video 1/1 (frame 395/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 169.5ms
video 1/1 (frame 396/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 168.4ms
video 1/1 (frame 397/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 2 trucks, 3 backpacks, 156.9ms
video 1/1 (frame 398/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 trucks, 2 backpacks, 154.3ms
video 1/1 (frame 399/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 truck, 2 backpacks, 162.7ms
video 1/1 (frame 400/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 backpacks, 206.6ms
video 1/1 (frame 401/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 225.3ms
video 1/1 (frame 402/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 1 backpack, 223.8ms
video 1/1 (frame 403/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 225.3ms
video 1/1 (frame 404/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 339.7ms
video 1/1 (frame 405/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 226.2ms
video 1/1 (frame 406/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 178.2ms
video 1/1 (frame 407/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 172.1ms
video 1/1 (frame 408/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 175.9ms
video 1/1 (frame 409/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 1 truck, 1 backpack, 1 handbag, 186.2ms
video 1/1 (frame 410/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 182.7ms
video 1/1 (frame 411/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 169.0ms
video 1/1 (frame 412/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 177.1ms
video 1/1 (frame 413/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 185.5ms
video 1/1 (frame 414/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 171.2ms
video 1/1 (frame 415/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 226.1ms
video 1/1 (frame 416/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 206.2ms
video 1/1 (frame 417/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 2 backpacks, 181.9ms
video 1/1 (frame 418/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 1 suitcase, 156.2ms
video 1/1 (frame 419/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 truck, 3 backpacks, 151.9ms
video 1/1 (frame 420/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 3 backpacks, 180.0ms
video 1/1 (frame 421/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 171.2ms
video 1/1 (frame 422/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 2 backpacks, 156.2ms
video 1/1 (frame 423/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 155.7ms
video 1/1 (frame 424/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 2 trucks, 1 backpack, 164.6ms
video 1/1 (frame 425/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 2 trucks, 1 backpack, 158.2ms
video 1/1 (frame 426/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 176.8ms
video 1/1 (frame 427/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 158.7ms
video 1/1 (frame 428/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 164.5ms
video 1/1 (frame 429/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 2 trucks, 2 backpacks, 199.2ms
video 1/1 (frame 430/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 2 backpacks, 178.0ms
video 1/1 (frame 431/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 3 backpacks, 169.8ms
video 1/1 (frame 432/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 171.2ms
video 1/1 (frame 433/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 165.6ms
video 1/1 (frame 434/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 167.6ms
video 1/1 (frame 435/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 180.3ms
video 1/1 (frame 436/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 164.5ms
video 1/1 (frame 437/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 backpack, 179.7ms
video 1/1 (frame 438/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 164.5ms
video 1/1 (frame 439/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 backpack, 163.7ms
video 1/1 (frame 440/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 backpack, 168.1ms
video 1/1 (frame 441/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 2 backpacks, 168.8ms
video 1/1 (frame 442/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 176.7ms
video 1/1 (frame 443/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 backpack, 1 suitcase, 176.3ms
video 1/1 (frame 444/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 1 suitcase, 178.6ms
video 1/1 (frame 445/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 1 suitcase, 171.0ms
video 1/1 (frame 446/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 176.5ms
video 1/1 (frame 447/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 193.4ms
video 1/1 (frame 448/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 233.0ms
video 1/1 (frame 449/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 226.2ms
video 1/1 (frame 450/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 155.3ms
video 1/1 (frame 451/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 158.4ms
video 1/1 (frame 452/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 190.9ms
video 1/1 (frame 453/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 171.6ms
video 1/1 (frame 454/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 166.6ms
video 1/1 (frame 455/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 165.8ms
video 1/1 (frame 456/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 backpack, 185.5ms
video 1/1 (frame 457/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 208.4ms
video 1/1 (frame 458/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 223.0ms
video 1/1 (frame 459/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 228.5ms
video 1/1 (frame 460/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 195.1ms
video 1/1 (frame 461/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 182.6ms
video 1/1 (frame 462/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 177.1ms
video 1/1 (frame 463/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 169.4ms
video 1/1 (frame 464/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 1 backpack, 185.2ms
video 1/1 (frame 465/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 170.3ms
video 1/1 (frame 466/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 2 backpacks, 157.6ms
video 1/1 (frame 467/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 truck, 2 backpacks, 161.8ms
video 1/1 (frame 468/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 2 backpacks, 179.3ms
video 1/1 (frame 469/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 11 cars, 1 truck, 1 backpack, 162.3ms
video 1/1 (frame 470/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 truck, 1 backpack, 174.4ms
video 1/1 (frame 471/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 11 cars, 1 backpack, 160.1ms
video 1/1 (frame 472/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 12 cars, 1 backpack, 162.2ms
video 1/1 (frame 473/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 159.0ms
video 1/1 (frame 474/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 164.3ms
video 1/1 (frame 475/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 153.2ms
video 1/1 (frame 476/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 162.2ms
video 1/1 (frame 477/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 159.3ms
video 1/1 (frame 478/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 159.0ms
video 1/1 (frame 479/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 166.1ms
video 1/1 (frame 480/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 334.3ms
video 1/1 (frame 481/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 174.2ms
video 1/1 (frame 482/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 1 backpack, 167.0ms
video 1/1 (frame 483/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 165.2ms
video 1/1 (frame 484/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 175.3ms
video 1/1 (frame 485/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 162.0ms
video 1/1 (frame 486/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 161.6ms
video 1/1 (frame 487/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 164.5ms
video 1/1 (frame 488/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 161.2ms
video 1/1 (frame 489/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 159.4ms
video 1/1 (frame 490/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 184.2ms
video 1/1 (frame 491/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 157.6ms
video 1/1 (frame 492/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 159.4ms
video 1/1 (frame 493/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 159.4ms
video 1/1 (frame 494/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 158.5ms
video 1/1 (frame 495/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 backpacks, 158.1ms
video 1/1 (frame 496/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 172.0ms
video 1/1 (frame 497/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 187.1ms
video 1/1 (frame 498/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 166.6ms
video 1/1 (frame 499/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 165.2ms
video 1/1 (frame 500/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 162.3ms
video 1/1 (frame 501/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 183.6ms
video 1/1 (frame 502/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 169.3ms
video 1/1 (frame 503/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 backpack, 163.7ms
video 1/1 (frame 504/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 169.4ms
video 1/1 (frame 505/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 10 cars, 1 backpack, 164.5ms
video 1/1 (frame 506/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 161.2ms
video 1/1 (frame 507/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 backpack, 167.5ms
video 1/1 (frame 508/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 161.2ms
video 1/1 (frame 509/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 1 backpack, 157.5ms
video 1/1 (frame 510/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 1 backpack, 160.0ms
video 1/1 (frame 511/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 8 cars, 156.2ms
video 1/1 (frame 512/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 165.5ms
video 1/1 (frame 513/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 161.1ms
video 1/1 (frame 514/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 154.4ms
video 1/1 (frame 515/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 211.0ms
video 1/1 (frame 516/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 backpack, 213.2ms
video 1/1 (frame 517/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 backpack, 223.5ms
video 1/1 (frame 518/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 221.1ms
video 1/1 (frame 519/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 8 cars, 176.1ms
video 1/1 (frame 520/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 180.2ms
video 1/1 (frame 521/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 185.3ms
video 1/1 (frame 522/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 backpack, 213.0ms
video 1/1 (frame 523/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 178.2ms
video 1/1 (frame 524/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 2 backpacks, 175.2ms
video 1/1 (frame 525/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 163.3ms
video 1/1 (frame 526/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 156.4ms
video 1/1 (frame 527/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 156.4ms
video 1/1 (frame 528/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 170.6ms
video 1/1 (frame 529/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 158.9ms
video 1/1 (frame 530/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 155.1ms
video 1/1 (frame 531/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 161.1ms
video 1/1 (frame 532/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 158.6ms
video 1/1 (frame 533/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 21 persons, 8 cars, 1 truck, 1 backpack, 160.5ms
video 1/1 (frame 534/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 9 cars, 1 backpack, 183.5ms
video 1/1 (frame 535/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 9 cars, 1 truck, 1 backpack, 182.6ms
video 1/1 (frame 536/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 1 truck, 192.3ms
video 1/1 (frame 537/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 202.2ms
video 1/1 (frame 538/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 9 cars, 190.8ms
video 1/1 (frame 539/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 8 cars, 195.4ms
video 1/1 (frame 540/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 8 cars, 157.0ms
video 1/1 (frame 541/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 154.3ms
video 1/1 (frame 542/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 162.6ms
video 1/1 (frame 543/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 158.1ms
video 1/1 (frame 544/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 169.6ms
video 1/1 (frame 545/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 318.8ms
video 1/1 (frame 546/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 155.9ms
video 1/1 (frame 547/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 162.6ms
video 1/1 (frame 548/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 21 persons, 6 cars, 160.4ms
video 1/1 (frame 549/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 165.0ms
video 1/1 (frame 550/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 157.3ms
video 1/1 (frame 551/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 153.5ms
video 1/1 (frame 552/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 155.6ms
video 1/1 (frame 553/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 164.7ms
video 1/1 (frame 554/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 156.7ms
video 1/1 (frame 555/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 5 cars, 175.3ms
video 1/1 (frame 556/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 5 cars, 157.9ms
video 1/1 (frame 557/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 157.1ms
video 1/1 (frame 558/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 158.9ms
video 1/1 (frame 559/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 159.6ms
video 1/1 (frame 560/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 157.1ms
video 1/1 (frame 561/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 180.9ms
video 1/1 (frame 562/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 231.7ms
video 1/1 (frame 563/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 207.4ms
video 1/1 (frame 564/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 210.2ms
video 1/1 (frame 565/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 185.4ms
video 1/1 (frame 566/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 5 cars, 171.8ms
video 1/1 (frame 567/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 176.6ms
video 1/1 (frame 568/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 178.7ms
video 1/1 (frame 569/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 169.0ms
video 1/1 (frame 570/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 171.9ms
video 1/1 (frame 571/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 181.2ms
video 1/1 (frame 572/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 221.1ms
video 1/1 (frame 573/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 227.1ms
video 1/1 (frame 574/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 274.6ms
video 1/1 (frame 575/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 256.8ms
video 1/1 (frame 576/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 173.1ms
video 1/1 (frame 577/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 179.5ms
video 1/1 (frame 578/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 184.2ms
video 1/1 (frame 579/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 179.5ms
video 1/1 (frame 580/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 183.0ms
video 1/1 (frame 581/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 181.3ms
video 1/1 (frame 582/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 handbag, 169.6ms
video 1/1 (frame 583/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 165.9ms
video 1/1 (frame 584/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 163.4ms
video 1/1 (frame 585/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 208.4ms
video 1/1 (frame 586/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 176.5ms
video 1/1 (frame 587/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 178.4ms
video 1/1 (frame 588/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 158.9ms
video 1/1 (frame 589/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 156.4ms
video 1/1 (frame 590/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 156.7ms
video 1/1 (frame 591/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 170.9ms
video 1/1 (frame 592/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 handbag, 160.7ms
video 1/1 (frame 593/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 handbag, 153.1ms
video 1/1 (frame 594/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 1 handbag, 155.3ms
video 1/1 (frame 595/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 handbag, 155.9ms
video 1/1 (frame 596/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 165.1ms
video 1/1 (frame 597/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 172.3ms
video 1/1 (frame 598/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 8 cars, 167.7ms
video 1/1 (frame 599/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 326.1ms
video 1/1 (frame 600/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 handbag, 157.1ms
video 1/1 (frame 601/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 164.6ms
video 1/1 (frame 602/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 174.5ms
video 1/1 (frame 603/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 155.6ms
video 1/1 (frame 604/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 158.3ms
video 1/1 (frame 605/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 157.6ms
video 1/1 (frame 606/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 159.8ms
video 1/1 (frame 607/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 5 cars, 177.9ms
video 1/1 (frame 608/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 162.9ms
video 1/1 (frame 609/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 156.6ms
video 1/1 (frame 610/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 158.7ms
video 1/1 (frame 611/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 155.0ms
video 1/1 (frame 612/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 200.4ms
video 1/1 (frame 613/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 210.0ms
video 1/1 (frame 614/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 182.4ms
video 1/1 (frame 615/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 157.1ms
video 1/1 (frame 616/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 handbag, 163.4ms
video 1/1 (frame 617/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 handbag, 175.9ms
video 1/1 (frame 618/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 172.6ms
video 1/1 (frame 619/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 170.0ms
video 1/1 (frame 620/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 179.0ms
video 1/1 (frame 621/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 161.7ms
video 1/1 (frame 622/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 11 cars, 162.1ms
video 1/1 (frame 623/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 161.1ms
video 1/1 (frame 624/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 13 cars, 1 truck, 181.4ms
video 1/1 (frame 625/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 truck, 158.2ms
video 1/1 (frame 626/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 318.6ms
video 1/1 (frame 627/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 162.3ms
video 1/1 (frame 628/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 192.7ms
video 1/1 (frame 629/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 11 cars, 1 truck, 232.8ms
video 1/1 (frame 630/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 215.0ms
video 1/1 (frame 631/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 223.9ms
video 1/1 (frame 632/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 1 handbag, 194.4ms
video 1/1 (frame 633/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 178.3ms
video 1/1 (frame 634/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 182.1ms
video 1/1 (frame 635/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 9 cars, 177.2ms
video 1/1 (frame 636/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 178.9ms
video 1/1 (frame 637/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 189.5ms
video 1/1 (frame 638/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 backpack, 224.4ms
video 1/1 (frame 639/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 206.7ms
video 1/1 (frame 640/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 10 cars, 1 backpack, 159.1ms
video 1/1 (frame 641/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 158.4ms
video 1/1 (frame 642/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 1 backpack, 153.1ms
video 1/1 (frame 643/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 1 backpack, 1 handbag, 174.9ms
video 1/1 (frame 644/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 1 backpack, 177.1ms
video 1/1 (frame 645/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 1 handbag, 153.7ms
video 1/1 (frame 646/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 157.7ms
video 1/1 (frame 647/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 1 handbag, 155.6ms
video 1/1 (frame 648/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 handbag, 160.0ms
video 1/1 (frame 649/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 165.1ms
video 1/1 (frame 650/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 183.3ms
Speed: 4.7ms preprocess, 182.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0e058a7e-2785-449f-a868-d344a7a1b96b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">From</span> <span class="n">the</span> <span class="n">inference</span> <span class="n">log</span> <span class="p">,</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">seems</span> <span class="n">to</span> <span class="n">have</span> <span class="n">performed</span> <span class="n">well</span><span class="o">.</span>

<span class="n">Results</span> <span class="n">Overview</span><span class="p">:</span>
<span class="n">Detection</span> <span class="n">Summary</span><span class="p">:</span>

<span class="n">The</span> <span class="n">model</span> <span class="n">successfully</span> <span class="n">detected</span> <span class="n">persons</span><span class="p">,</span> <span class="n">cars</span><span class="p">,</span> <span class="n">trucks</span><span class="p">,</span> <span class="n">backpacks</span><span class="p">,</span> <span class="n">bottles</span><span class="p">,</span> <span class="ow">and</span> <span class="n">handbags</span> 
<span class="n">across</span> <span class="mi">650</span> <span class="n">frames</span><span class="o">.</span>
<span class="n">The</span> <span class="n">inference</span> <span class="n">speed</span> <span class="ow">is</span> <span class="n">consistent</span><span class="p">,</span> <span class="n">averaging</span> <span class="n">around</span> <span class="mi">182</span><span class="n">ms</span> <span class="n">per</span> <span class="n">frame</span><span class="p">,</span> <span class="n">which</span> <span class="ow">is</span> <span class="n">acceptable</span>
<span class="k">for</span> <span class="n">many</span> <span class="n">applications</span><span class="o">.</span>
    
<span class="c1"># Detection Details:</span>

<span class="n">The</span> <span class="n">number</span> <span class="n">of</span> <span class="n">persons</span> <span class="n">detected</span> <span class="n">increased</span> <span class="k">as</span> <span class="n">the</span> <span class="n">video</span> <span class="n">progressed</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="kn">from</span> <span class="mi">11</span> <span class="n">persons</span> <span class="ow">in</span>
<span class="n">frame</span> <span class="mi">1</span> <span class="n">to</span> <span class="mi">17</span> <span class="n">persons</span> <span class="ow">in</span> <span class="n">frame</span> <span class="mi">647</span><span class="p">)</span><span class="o">.</span> <span class="n">This</span> <span class="n">suggests</span> <span class="n">that</span> <span class="n">the</span> <span class="n">video</span> <span class="n">captures</span> <span class="n">varying</span> <span class="n">crowd</span> <span class="n">densities</span><span class="o">.</span>
<span class="n">Multiple</span> <span class="n">vehicles</span> <span class="p">(</span><span class="n">cars</span> <span class="ow">and</span> <span class="n">trucks</span><span class="p">)</span> <span class="n">were</span> <span class="n">consistently</span> <span class="n">detected</span> <span class="n">throughout</span> <span class="n">the</span> <span class="n">video</span><span class="o">.</span>
<span class="n">Smaller</span> <span class="n">objects</span> <span class="n">like</span> <span class="n">bottles</span> <span class="ow">and</span> <span class="n">handbags</span> <span class="n">were</span> <span class="n">detected</span> <span class="k">as</span> <span class="n">well</span><span class="p">,</span> <span class="n">indicating</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s</span>
<span class="n">ability</span> <span class="n">to</span> <span class="n">identify</span> <span class="n">smaller</span><span class="o">-</span><span class="n">scale</span> <span class="n">objects</span><span class="o">.</span>
    
<span class="c1"># Performance Indicators:</span>

<span class="n">Inference</span> <span class="n">Speed</span><span class="p">:</span>
<span class="n">Preprocessing</span><span class="p">:</span> <span class="o">~</span><span class="mi">5</span><span class="n">ms</span>
<span class="n">Inference</span><span class="p">:</span> <span class="o">~</span><span class="mi">183</span><span class="n">ms</span>
<span class="n">Postprocessing</span><span class="p">:</span> <span class="o">~</span><span class="mi">3</span><span class="n">ms</span>
<span class="n">These</span> <span class="n">times</span> <span class="n">are</span> <span class="n">acceptable</span><span class="p">,</span> <span class="n">but</span> <span class="n">inference</span> <span class="n">could</span> <span class="n">be</span> <span class="n">optimized</span> <span class="k">for</span> <span class="n">faster</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">detection</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="k">for</span> <span class="n">live</span> <span class="n">streaming</span><span class="p">)</span><span class="o">.</span>
<span class="n">Insights</span> <span class="ow">and</span> <span class="n">Suggestions</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Model</span> <span class="n">Performance</span>
<span class="n">Strengths</span><span class="p">:</span>

<span class="n">The</span> <span class="n">model</span> <span class="n">detected</span> <span class="n">objects</span> <span class="n">across</span> <span class="n">various</span> <span class="n">categories</span> <span class="n">accurately</span><span class="o">.</span>
<span class="n">Smaller</span> <span class="n">objects</span> <span class="n">like</span> <span class="n">bottles</span> <span class="ow">and</span> <span class="n">backpacks</span> <span class="n">were</span> <span class="n">detected</span><span class="p">,</span> <span class="n">showcasing</span> <span class="n">good</span> <span class="n">sensitivity</span><span class="o">.</span>
<span class="n">Detection</span> <span class="n">consistency</span> <span class="n">across</span> <span class="n">frames</span> <span class="n">indicates</span> <span class="n">robustness</span><span class="o">.</span>
<span class="n">Weaknesses</span><span class="o">/</span><span class="n">Challenges</span><span class="p">:</span>

<span class="n">Possible</span> <span class="n">false</span> <span class="n">positives</span> <span class="ow">or</span> <span class="n">false</span> <span class="n">negatives</span><span class="p">:</span>
<span class="kc">False</span> <span class="n">Positives</span><span class="p">:</span> <span class="n">Non</span><span class="o">-</span><span class="nb">object</span> <span class="n">areas</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">shadows</span><span class="p">,</span> <span class="n">patterns</span><span class="p">)</span> <span class="n">could</span> <span class="n">be</span> <span class="n">misclassified</span> <span class="k">as</span> <span class="n">objects</span><span class="o">.</span>
<span class="kc">False</span> <span class="n">Negatives</span><span class="p">:</span> <span class="n">Objects</span> <span class="ow">in</span> <span class="n">crowded</span> <span class="ow">or</span> <span class="n">overlapping</span> <span class="n">areas</span> <span class="n">might</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">detected</span><span class="o">.</span>
<span class="n">The</span> <span class="n">model</span> <span class="n">may</span> <span class="n">struggle</span> <span class="k">with</span> <span class="n">detecting</span> <span class="n">occluded</span> <span class="n">objects</span> 
<span class="p">(</span><span class="n">such</span> <span class="k">as</span><span class="p">,</span><span class="n">a</span> <span class="n">person</span> <span class="n">partially</span> <span class="n">hidden</span> <span class="n">behind</span> <span class="n">a</span> <span class="n">vehicle</span><span class="p">)</span><span class="o">.</span>

<span class="mf">2.</span> <span class="n">Suggestions</span> <span class="k">for</span> <span class="n">Improvement</span>

<span class="n">a</span><span class="o">.</span> <span class="n">Evaluate</span> <span class="n">Detection</span> <span class="n">Metrics</span>
<span class="n">Quantitative</span> <span class="n">metrics</span> <span class="n">like</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="ow">and</span> <span class="n">mAP</span> <span class="p">(</span><span class="n">mean</span> <span class="n">Average</span> <span class="n">Precision</span><span class="p">)</span> <span class="n">can</span> <span class="n">provide</span>
<span class="n">deeper</span> <span class="n">insights</span> <span class="n">into</span> <span class="n">performance</span><span class="o">.</span>
<span class="n">High</span> <span class="n">precision</span> <span class="n">means</span> <span class="n">fewer</span> <span class="n">false</span> <span class="n">positives</span><span class="p">,</span> <span class="ow">and</span> <span class="n">high</span> <span class="n">recall</span> <span class="n">means</span> <span class="n">fewer</span> <span class="n">false</span> <span class="n">negatives</span><span class="o">.</span>
<span class="n">b</span><span class="o">.</span> <span class="n">Improve</span> <span class="n">Detection</span> <span class="n">Accuracy</span>
<span class="n">Fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">the</span> <span class="n">Model</span><span class="p">:</span>
<span class="n">Use</span> <span class="n">labeled</span> <span class="n">data</span> <span class="n">similar</span> <span class="n">to</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">video</span> <span class="k">for</span> <span class="n">fine</span><span class="o">-</span><span class="n">tuning</span><span class="o">.</span>
<span class="n">Fine</span><span class="o">-</span><span class="n">tuning</span> <span class="n">can</span> <span class="n">improve</span> <span class="n">the</span> <span class="n">model</span><span class="s1">'s ability to detect smaller or partially occluded objects.</span>
<span class="n">Data</span> <span class="n">Augmentation</span><span class="p">:</span>
<span class="n">Apply</span> <span class="n">techniques</span> <span class="n">like</span> <span class="n">rotation</span><span class="p">,</span> <span class="n">scaling</span><span class="p">,</span> <span class="ow">or</span> <span class="n">cropping</span> <span class="n">to</span> <span class="n">create</span> <span class="n">variations</span> <span class="n">of</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">data</span><span class="o">.</span>
<span class="n">This</span> <span class="n">helps</span> <span class="n">the</span> <span class="n">model</span> <span class="n">generalize</span> <span class="n">better</span><span class="o">.</span>
<span class="n">c</span><span class="o">.</span> <span class="n">Optimize</span> <span class="n">Inference</span> <span class="n">Speed</span>
<span class="n">Use</span> <span class="n">a</span> <span class="n">lighter</span> <span class="n">YOLO</span> <span class="n">variant</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">YOLOv8n</span> <span class="ow">or</span> <span class="n">YOLOv5s</span><span class="p">)</span> <span class="k">for</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="o">.</span>
<span class="n">Optimize</span> <span class="n">hardware</span> <span class="n">utilization</span> <span class="n">by</span> <span class="n">leveraging</span> <span class="n">GPUs</span> <span class="ow">or</span> <span class="n">AI</span> <span class="n">accelerators</span><span class="o">.</span>
<span class="n">d</span><span class="o">.</span> <span class="n">Post</span><span class="o">-</span><span class="n">processing</span> <span class="n">Enhancements</span>
<span class="n">Apply</span> <span class="n">non</span><span class="o">-</span><span class="n">maximum</span> <span class="n">suppression</span> <span class="p">(</span><span class="n">NMS</span><span class="p">)</span> <span class="n">tuning</span> <span class="n">to</span> <span class="n">reduce</span> <span class="n">overlapping</span> <span class="n">detections</span><span class="o">.</span>
<span class="n">Filter</span> <span class="n">out</span> <span class="n">detections</span> <span class="k">with</span> <span class="n">low</span> <span class="n">confidence</span> <span class="n">scores</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">below</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span>
<span class="mf">3.</span> <span class="n">Advanced</span> <span class="n">Suggestions</span>
<span class="n">Context</span><span class="o">-</span><span class="n">Aware</span> <span class="n">Enhancements</span><span class="p">:</span>
<span class="n">Incorporate</span> <span class="n">temporal</span> <span class="n">consistency</span> <span class="n">checks</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">objects</span> <span class="n">are</span> <span class="n">tracked</span> <span class="n">accurately</span> 
<span class="n">across</span> <span class="n">frames</span><span class="o">.</span>

<span class="c1">#Integration with Tracking Algorithms:</span>
<span class="n">Combine</span> <span class="n">YOLO</span> <span class="k">with</span> <span class="nb">object</span><span class="o">-</span><span class="n">tracking</span> <span class="n">frameworks</span> <span class="n">like</span> <span class="n">DeepSORT</span> <span class="ow">or</span> <span class="n">SORT</span> <span class="n">to</span> <span class="n">handle</span> <span class="n">motion</span>
<span class="n">better</span> <span class="ow">and</span> <span class="n">maintain</span> <span class="nb">object</span> <span class="n">identities</span> <span class="n">across</span> <span class="n">frames</span><span class="o">.</span>

<span class="c1"># Deployment Considerations:</span>
<span class="n">If</span> <span class="n">deploying</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">resource</span><span class="o">-</span><span class="n">constrained</span> <span class="n">environment</span><span class="p">,</span> <span class="n">consider</span> <span class="n">model</span> <span class="n">quantization</span> <span class="ow">or</span>
<span class="n">pruning</span> <span class="k">for</span> <span class="n">faster</span> <span class="n">inference</span><span class="o">.</span>
    
<span class="c1"># Conclusion</span>
<span class="n">The</span> <span class="n">results</span> <span class="n">are</span> <span class="n">promising</span><span class="p">,</span> <span class="n">indicating</span> <span class="n">that</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">well</span><span class="o">-</span><span class="n">suited</span> <span class="k">for</span> <span class="n">general</span><span class="o">-</span><span class="n">purpose</span>
<span class="nb">object</span> <span class="n">detection</span><span class="o">.</span> <span class="n">However</span><span class="p">,</span> <span class="n">performance</span> <span class="n">can</span> <span class="n">be</span> <span class="n">improved</span> <span class="n">by</span> <span class="n">evaluating</span> <span class="n">the</span> <span class="n">detections</span> <span class="n">quantitatively</span><span class="p">,</span>
<span class="n">refining</span> <span class="n">the</span> <span class="n">model</span> <span class="k">with</span> <span class="n">domain</span><span class="o">-</span><span class="n">specific</span> <span class="n">data</span><span class="p">,</span> <span class="ow">and</span> <span class="n">optimizing</span> <span class="n">inference</span> <span class="n">speed</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=4514ded1-9218-42e5-8ea4-5a694eed8565">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Performing Object Detection and Storing Predictions</span>
<span class="n">Saving</span> <span class="n">Predictions</span> <span class="n">to</span> <span class="n">File</span> <span class="n">When</span> <span class="n">the</span> <span class="n">predictions</span> <span class="nb">list</span> <span class="ow">is</span> <span class="n">populated</span><span class="p">,</span> <span class="n">save</span> <span class="n">it</span> <span class="n">to</span> <span class="n">text</span> 
<span class="n">files</span> <span class="k">for</span> <span class="n">evaluation</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1ed83ad3-6b82-4800-8ad7-ef983e76b523">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Assuming you are using the YOLO model to predict objects on a video</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="c1"># Load your pre-trained YOLO model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">'yolov8n.pt'</span><span class="p">)</span>  <span class="c1"># Replace with your specific model</span>

<span class="c1"># Video file path</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\30952-383991415_small.mp4"</span>

<span class="c1"># Run predictions on the video</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">video_path</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Loop through results for each frame</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Create an empty list to store predictions</span>

<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># Save each frame's result</span>

<span class="c1"># Now 'predictions' contains detection results for all frames</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
video 1/1 (frame 1/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 258.2ms
video 1/1 (frame 2/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 228.9ms
video 1/1 (frame 3/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 1 backpack, 1 bottle, 201.4ms
video 1/1 (frame 4/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 206.0ms
video 1/1 (frame 5/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 212.8ms
video 1/1 (frame 6/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 truck, 1 bottle, 205.9ms
video 1/1 (frame 7/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 truck, 2 bottles, 215.0ms
video 1/1 (frame 8/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 3 cars, 1 truck, 2 bottles, 209.1ms
video 1/1 (frame 9/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 truck, 206.8ms
video 1/1 (frame 10/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 truck, 1 bottle, 213.0ms
video 1/1 (frame 11/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 truck, 208.3ms
video 1/1 (frame 12/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 truck, 2 bottles, 220.0ms
video 1/1 (frame 13/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 truck, 2 bottles, 211.8ms
video 1/1 (frame 14/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 truck, 3 bottles, 201.7ms
video 1/1 (frame 15/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 3 bottles, 203.0ms
video 1/1 (frame 16/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 1 bottle, 205.0ms
video 1/1 (frame 17/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 225.0ms
video 1/1 (frame 18/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 207.9ms
video 1/1 (frame 19/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 219.7ms
video 1/1 (frame 20/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 227.8ms
video 1/1 (frame 21/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 231.4ms
video 1/1 (frame 22/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 209.4ms
video 1/1 (frame 23/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 176.9ms
video 1/1 (frame 24/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 185.1ms
video 1/1 (frame 25/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 177.9ms
video 1/1 (frame 26/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 173.0ms
video 1/1 (frame 27/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 188.8ms
video 1/1 (frame 28/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 179.7ms
video 1/1 (frame 29/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 171.9ms
video 1/1 (frame 30/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 194.4ms
video 1/1 (frame 31/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 204.4ms
video 1/1 (frame 32/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 bus, 212.7ms
video 1/1 (frame 33/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 174.2ms
video 1/1 (frame 34/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 170.1ms
video 1/1 (frame 35/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 199.2ms
video 1/1 (frame 36/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 193.2ms
video 1/1 (frame 37/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 1 truck, 215.6ms
video 1/1 (frame 38/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 213.2ms
video 1/1 (frame 39/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 202.1ms
video 1/1 (frame 40/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 206.2ms
video 1/1 (frame 41/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 208.0ms
video 1/1 (frame 42/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 233.0ms
video 1/1 (frame 43/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 228.6ms
video 1/1 (frame 44/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 1 truck, 222.1ms
video 1/1 (frame 45/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 444.4ms
video 1/1 (frame 46/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 192.5ms
video 1/1 (frame 47/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 2 cars, 1 bus, 204.3ms
video 1/1 (frame 48/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 182.9ms
video 1/1 (frame 49/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 192.2ms
video 1/1 (frame 50/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 193.2ms
video 1/1 (frame 51/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 196.2ms
video 1/1 (frame 52/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 208.1ms
video 1/1 (frame 53/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 192.9ms
video 1/1 (frame 54/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 195.0ms
video 1/1 (frame 55/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 211.2ms
video 1/1 (frame 56/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 206.4ms
video 1/1 (frame 57/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 227.0ms
video 1/1 (frame 58/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 1 car, 1 bus, 210.8ms
video 1/1 (frame 59/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 216.1ms
video 1/1 (frame 60/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 215.0ms
video 1/1 (frame 61/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 1 car, 1 bus, 204.1ms
video 1/1 (frame 62/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 215.0ms
video 1/1 (frame 63/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 truck, 196.3ms
video 1/1 (frame 64/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 212.2ms
video 1/1 (frame 65/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 201.9ms
video 1/1 (frame 66/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 207.4ms
video 1/1 (frame 67/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 230.7ms
video 1/1 (frame 68/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 198.8ms
video 1/1 (frame 69/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 191.6ms
video 1/1 (frame 70/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 190.5ms
video 1/1 (frame 71/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 176.4ms
video 1/1 (frame 72/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 1 car, 1 bus, 179.6ms
video 1/1 (frame 73/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 197.1ms
video 1/1 (frame 74/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 bus, 196.6ms
video 1/1 (frame 75/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 bus, 208.3ms
video 1/1 (frame 76/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 191.8ms
video 1/1 (frame 77/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 210.0ms
video 1/1 (frame 78/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 1 truck, 211.1ms
video 1/1 (frame 79/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 196.2ms
video 1/1 (frame 80/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 183.6ms
video 1/1 (frame 81/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 1 truck, 193.1ms
video 1/1 (frame 82/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 210.0ms
video 1/1 (frame 83/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 1 truck, 198.2ms
video 1/1 (frame 84/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 2 cars, 1 truck, 1 handbag, 206.3ms
video 1/1 (frame 85/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 212.6ms
video 1/1 (frame 86/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 truck, 206.4ms
video 1/1 (frame 87/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 2 cars, 1 bus, 214.0ms
video 1/1 (frame 88/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 202.1ms
video 1/1 (frame 89/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 201.6ms
video 1/1 (frame 90/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 205.4ms
video 1/1 (frame 91/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 206.2ms
video 1/1 (frame 92/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 230.9ms
video 1/1 (frame 93/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 230.4ms
video 1/1 (frame 94/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 228.3ms
video 1/1 (frame 95/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 1 handbag, 225.4ms
video 1/1 (frame 96/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 bus, 198.0ms
video 1/1 (frame 97/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 4 cars, 1 bus, 188.1ms
video 1/1 (frame 98/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 203.0ms
video 1/1 (frame 99/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 191.3ms
video 1/1 (frame 100/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 185.5ms
video 1/1 (frame 101/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 190.8ms
video 1/1 (frame 102/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 handbag, 179.6ms
video 1/1 (frame 103/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 203.0ms
video 1/1 (frame 104/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 185.6ms
video 1/1 (frame 105/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 191.1ms
video 1/1 (frame 106/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 2 handbags, 202.2ms
video 1/1 (frame 107/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 201.8ms
video 1/1 (frame 108/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 212.7ms
video 1/1 (frame 109/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 198.4ms
video 1/1 (frame 110/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 193.2ms
video 1/1 (frame 111/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 183.6ms
video 1/1 (frame 112/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 191.7ms
video 1/1 (frame 113/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 193.6ms
video 1/1 (frame 114/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 200.4ms
video 1/1 (frame 115/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 202.7ms
video 1/1 (frame 116/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 177.9ms
video 1/1 (frame 117/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 185.7ms
video 1/1 (frame 118/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 176.2ms
video 1/1 (frame 119/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 185.2ms
video 1/1 (frame 120/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 198.9ms
video 1/1 (frame 121/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 1 backpack, 188.7ms
video 1/1 (frame 122/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 bus, 1 backpack, 202.5ms
video 1/1 (frame 123/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 1 backpack, 196.4ms
video 1/1 (frame 124/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 2 motorcycles, 1 bus, 1 backpack, 209.0ms
video 1/1 (frame 125/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 219.8ms
video 1/1 (frame 126/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 208.9ms
video 1/1 (frame 127/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 226.4ms
video 1/1 (frame 128/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 1 backpack, 221.7ms
video 1/1 (frame 129/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 1 backpack, 223.1ms
video 1/1 (frame 130/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 191.4ms
video 1/1 (frame 131/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 198.5ms
video 1/1 (frame 132/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 2 motorcycles, 1 bus, 1 backpack, 180.0ms
video 1/1 (frame 133/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 bus, 1 backpack, 183.5ms
video 1/1 (frame 134/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 bus, 202.9ms
video 1/1 (frame 135/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 backpack, 197.6ms
video 1/1 (frame 136/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 backpack, 193.9ms
video 1/1 (frame 137/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 1 backpack, 197.6ms
video 1/1 (frame 138/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 195.9ms
video 1/1 (frame 139/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 197.8ms
video 1/1 (frame 140/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 195.7ms
video 1/1 (frame 141/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 192.6ms
video 1/1 (frame 142/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 1 truck, 180.3ms
video 1/1 (frame 143/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 2 trucks, 203.8ms
video 1/1 (frame 144/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 214.4ms
video 1/1 (frame 145/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 2 trucks, 223.4ms
video 1/1 (frame 146/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 1 backpack, 215.2ms
video 1/1 (frame 147/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 1 truck, 189.4ms
video 1/1 (frame 148/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 186.0ms
video 1/1 (frame 149/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 181.1ms
video 1/1 (frame 150/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 1 backpack, 173.4ms
video 1/1 (frame 151/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 1 backpack, 170.1ms
video 1/1 (frame 152/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 169.2ms
video 1/1 (frame 153/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 186.3ms
video 1/1 (frame 154/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 190.7ms
video 1/1 (frame 155/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 182.1ms
video 1/1 (frame 156/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 206.2ms
video 1/1 (frame 157/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 188.2ms
video 1/1 (frame 158/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 backpack, 195.6ms
video 1/1 (frame 159/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 197.2ms
video 1/1 (frame 160/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 bus, 184.8ms
video 1/1 (frame 161/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 191.9ms
video 1/1 (frame 162/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 201.7ms
video 1/1 (frame 163/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 198.3ms
video 1/1 (frame 164/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 1 motorcycle, 1 bus, 164.4ms
video 1/1 (frame 165/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 196.4ms
video 1/1 (frame 166/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 8 cars, 1 bus, 1 truck, 201.0ms
video 1/1 (frame 167/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 9 cars, 1 bus, 1 truck, 186.8ms
video 1/1 (frame 168/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 bus, 1 truck, 201.9ms
video 1/1 (frame 169/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 204.6ms
video 1/1 (frame 170/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 193.8ms
video 1/1 (frame 171/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 bus, 194.0ms
video 1/1 (frame 172/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 bus, 1 truck, 217.7ms
video 1/1 (frame 173/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 212.4ms
video 1/1 (frame 174/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 191.5ms
video 1/1 (frame 175/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 188.2ms
video 1/1 (frame 176/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 195.2ms
video 1/1 (frame 177/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 truck, 209.6ms
video 1/1 (frame 178/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 202.1ms
video 1/1 (frame 179/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 354.2ms
video 1/1 (frame 180/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 184.5ms
video 1/1 (frame 181/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 190.7ms
video 1/1 (frame 182/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 183.3ms
video 1/1 (frame 183/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 202.6ms
video 1/1 (frame 184/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 207.1ms
video 1/1 (frame 185/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 212.9ms
video 1/1 (frame 186/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 motorcycle, 204.7ms
video 1/1 (frame 187/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 truck, 190.5ms
video 1/1 (frame 188/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 193.0ms
video 1/1 (frame 189/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 199.1ms
video 1/1 (frame 190/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 truck, 199.8ms
video 1/1 (frame 191/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 213.5ms
video 1/1 (frame 192/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 187.8ms
video 1/1 (frame 193/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 194.7ms
video 1/1 (frame 194/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 205.3ms
video 1/1 (frame 195/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 258.6ms
video 1/1 (frame 196/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 1 truck, 234.2ms
video 1/1 (frame 197/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 299.0ms
video 1/1 (frame 198/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 284.7ms
video 1/1 (frame 199/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 204.3ms
video 1/1 (frame 200/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 232.5ms
video 1/1 (frame 201/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 285.9ms
video 1/1 (frame 202/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 251.2ms
video 1/1 (frame 203/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 195.5ms
video 1/1 (frame 204/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 219.8ms
video 1/1 (frame 205/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 11 cars, 1 motorcycle, 231.9ms
video 1/1 (frame 206/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 213.4ms
video 1/1 (frame 207/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 198.4ms
video 1/1 (frame 208/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 suitcase, 255.3ms
video 1/1 (frame 209/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 suitcase, 231.3ms
video 1/1 (frame 210/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 208.0ms
video 1/1 (frame 211/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 192.6ms
video 1/1 (frame 212/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 223.6ms
video 1/1 (frame 213/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 239.6ms
video 1/1 (frame 214/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 242.0ms
video 1/1 (frame 215/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 191.7ms
video 1/1 (frame 216/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 250.0ms
video 1/1 (frame 217/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 215.0ms
video 1/1 (frame 218/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 216.0ms
video 1/1 (frame 219/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 191.7ms
video 1/1 (frame 220/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 215.2ms
video 1/1 (frame 221/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 195.0ms
video 1/1 (frame 222/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 220.1ms
video 1/1 (frame 223/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 199.3ms
video 1/1 (frame 224/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 194.6ms
video 1/1 (frame 225/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 201.0ms
video 1/1 (frame 226/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 186.1ms
video 1/1 (frame 227/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 191.7ms
video 1/1 (frame 228/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 12 cars, 225.1ms
video 1/1 (frame 229/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 11 cars, 212.6ms
video 1/1 (frame 230/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 220.9ms
video 1/1 (frame 231/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 11 cars, 203.8ms
video 1/1 (frame 232/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 212.4ms
video 1/1 (frame 233/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 256.9ms
video 1/1 (frame 234/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 258.9ms
video 1/1 (frame 235/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 282.0ms
video 1/1 (frame 236/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 207.7ms
video 1/1 (frame 237/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 183.3ms
video 1/1 (frame 238/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 187.2ms
video 1/1 (frame 239/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 182.1ms
video 1/1 (frame 240/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 214.0ms
video 1/1 (frame 241/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 211.9ms
video 1/1 (frame 242/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 222.0ms
video 1/1 (frame 243/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 241.6ms
video 1/1 (frame 244/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 260.1ms
video 1/1 (frame 245/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 235.9ms
video 1/1 (frame 246/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 218.5ms
video 1/1 (frame 247/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 1 handbag, 199.0ms
video 1/1 (frame 248/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 1 handbag, 219.8ms
video 1/1 (frame 249/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 1 handbag, 462.4ms
video 1/1 (frame 250/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 261.7ms
video 1/1 (frame 251/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 235.5ms
video 1/1 (frame 252/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 256.3ms
video 1/1 (frame 253/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 216.0ms
video 1/1 (frame 254/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 311.5ms
video 1/1 (frame 255/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 245.9ms
video 1/1 (frame 256/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 285.7ms
video 1/1 (frame 257/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 323.8ms
video 1/1 (frame 258/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 truck, 297.7ms
video 1/1 (frame 259/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 251.0ms
video 1/1 (frame 260/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 handbag, 199.1ms
video 1/1 (frame 261/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 1 handbag, 236.6ms
video 1/1 (frame 262/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 222.1ms
video 1/1 (frame 263/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 backpack, 197.4ms
video 1/1 (frame 264/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 250.3ms
video 1/1 (frame 265/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 190.9ms
video 1/1 (frame 266/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 225.4ms
video 1/1 (frame 267/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 214.9ms
video 1/1 (frame 268/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 257.0ms
video 1/1 (frame 269/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 210.0ms
video 1/1 (frame 270/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 218.6ms
video 1/1 (frame 271/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 10 cars, 216.2ms
video 1/1 (frame 272/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 214.6ms
video 1/1 (frame 273/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 202.8ms
video 1/1 (frame 274/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 200.5ms
video 1/1 (frame 275/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 backpack, 212.9ms
video 1/1 (frame 276/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 backpack, 221.7ms
video 1/1 (frame 277/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 223.5ms
video 1/1 (frame 278/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 220.1ms
video 1/1 (frame 279/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 1 backpack, 219.0ms
video 1/1 (frame 280/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 truck, 1 backpack, 212.1ms
video 1/1 (frame 281/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 190.0ms
video 1/1 (frame 282/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 backpack, 1 handbag, 187.9ms
video 1/1 (frame 283/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 1 backpack, 1 handbag, 206.3ms
video 1/1 (frame 284/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 1 backpack, 1 handbag, 210.2ms
video 1/1 (frame 285/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 backpack, 222.0ms
video 1/1 (frame 286/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 13 cars, 1 handbag, 253.1ms
video 1/1 (frame 287/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 2 motorcycles, 1 handbag, 225.1ms
video 1/1 (frame 288/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 2 backpacks, 1 handbag, 215.7ms
video 1/1 (frame 289/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 2 backpacks, 227.3ms
video 1/1 (frame 290/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 2 backpacks, 198.0ms
video 1/1 (frame 291/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 2 backpacks, 198.3ms
video 1/1 (frame 292/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 208.5ms
video 1/1 (frame 293/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 223.5ms
video 1/1 (frame 294/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 224.5ms
video 1/1 (frame 295/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 motorcycles, 2 backpacks, 269.8ms
video 1/1 (frame 296/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 motorcycle, 2 backpacks, 233.0ms
video 1/1 (frame 297/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 2 motorcycles, 2 backpacks, 223.3ms
video 1/1 (frame 298/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 motorcycle, 2 backpacks, 245.0ms
video 1/1 (frame 299/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 226.9ms
video 1/1 (frame 300/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 220.0ms
video 1/1 (frame 301/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 214.2ms
video 1/1 (frame 302/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 1 backpack, 206.9ms
video 1/1 (frame 303/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 186.3ms
video 1/1 (frame 304/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 205.4ms
video 1/1 (frame 305/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 1 truck, 1 backpack, 415.3ms
video 1/1 (frame 306/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 truck, 1 backpack, 216.8ms
video 1/1 (frame 307/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 motorcycle, 1 truck, 1 backpack, 218.3ms
video 1/1 (frame 308/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 216.0ms
video 1/1 (frame 309/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 190.9ms
video 1/1 (frame 310/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 1 backpack, 207.3ms
video 1/1 (frame 311/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 220.6ms
video 1/1 (frame 312/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 1 boat, 2 backpacks, 236.6ms
video 1/1 (frame 313/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 1 truck, 207.0ms
video 1/1 (frame 314/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 219.5ms
video 1/1 (frame 315/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 219.2ms
video 1/1 (frame 316/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 umbrella, 206.9ms
video 1/1 (frame 317/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 195.1ms
video 1/1 (frame 318/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 206.2ms
video 1/1 (frame 319/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 217.2ms
video 1/1 (frame 320/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 221.0ms
video 1/1 (frame 321/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 219.7ms
video 1/1 (frame 322/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 208.2ms
video 1/1 (frame 323/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 222.9ms
video 1/1 (frame 324/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 192.3ms
video 1/1 (frame 325/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 194.8ms
video 1/1 (frame 326/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 200.7ms
video 1/1 (frame 327/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 11 cars, 1 motorcycle, 1 truck, 1 backpack, 201.9ms
video 1/1 (frame 328/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 1 truck, 1 backpack, 210.6ms
video 1/1 (frame 329/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 193.4ms
video 1/1 (frame 330/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 188.2ms
video 1/1 (frame 331/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 193.9ms
video 1/1 (frame 332/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 1 bicycle, 11 cars, 1 motorcycle, 2 trucks, 2 backpacks, 199.9ms
video 1/1 (frame 333/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 198.9ms
video 1/1 (frame 334/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 221.4ms
video 1/1 (frame 335/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 212.3ms
video 1/1 (frame 336/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 motorcycle, 2 trucks, 1 horse, 205.2ms
video 1/1 (frame 337/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 motorcycle, 2 trucks, 223.7ms
video 1/1 (frame 338/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 223.8ms
video 1/1 (frame 339/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 motorcycle, 1 truck, 211.6ms
video 1/1 (frame 340/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 201.4ms
video 1/1 (frame 341/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 196.8ms
video 1/1 (frame 342/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 191.0ms
video 1/1 (frame 343/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 2 trucks, 217.7ms
video 1/1 (frame 344/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 227.6ms
video 1/1 (frame 345/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 238.6ms
video 1/1 (frame 346/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 motorcycle, 1 truck, 201.5ms
video 1/1 (frame 347/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 190.7ms
video 1/1 (frame 348/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 215.3ms
video 1/1 (frame 349/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 224.5ms
video 1/1 (frame 350/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 213.6ms
video 1/1 (frame 351/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 206.1ms
video 1/1 (frame 352/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 213.7ms
video 1/1 (frame 353/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 206.6ms
video 1/1 (frame 354/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 204.5ms
video 1/1 (frame 355/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 10 cars, 1 motorcycle, 1 truck, 199.4ms
video 1/1 (frame 356/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 204.4ms
video 1/1 (frame 357/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 214.5ms
video 1/1 (frame 358/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 215.2ms
video 1/1 (frame 359/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 8 cars, 1 motorcycle, 1 truck, 209.3ms
video 1/1 (frame 360/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 10 cars, 1 motorcycle, 1 truck, 203.3ms
video 1/1 (frame 361/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 219.8ms
video 1/1 (frame 362/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 10 cars, 1 truck, 207.1ms
video 1/1 (frame 363/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 truck, 219.2ms
video 1/1 (frame 364/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 1 truck, 195.7ms
video 1/1 (frame 365/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 11 cars, 1 truck, 203.3ms
video 1/1 (frame 366/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 1 truck, 211.4ms
video 1/1 (frame 367/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 truck, 195.5ms
video 1/1 (frame 368/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 205.3ms
video 1/1 (frame 369/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 221.4ms
video 1/1 (frame 370/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 209.7ms
video 1/1 (frame 371/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 205.4ms
video 1/1 (frame 372/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 227.5ms
video 1/1 (frame 373/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 truck, 206.0ms
video 1/1 (frame 374/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 200.5ms
video 1/1 (frame 375/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 1 truck, 203.3ms
video 1/1 (frame 376/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 209.5ms
video 1/1 (frame 377/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 2 trucks, 1 backpack, 208.8ms
video 1/1 (frame 378/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 2 trucks, 1 backpack, 1 handbag, 219.0ms
video 1/1 (frame 379/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 2 trucks, 1 backpack, 212.6ms
video 1/1 (frame 380/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 1 handbag, 191.7ms
video 1/1 (frame 381/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 2 trucks, 193.7ms
video 1/1 (frame 382/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 trucks, 1 backpack, 182.7ms
video 1/1 (frame 383/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 201.0ms
video 1/1 (frame 384/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 188.3ms
video 1/1 (frame 385/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 3 cars, 2 trucks, 1 backpack, 188.9ms
video 1/1 (frame 386/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 1 truck, 1 backpack, 191.7ms
video 1/1 (frame 387/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 1 truck, 1 backpack, 194.7ms
video 1/1 (frame 388/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 1 truck, 1 backpack, 209.1ms
video 1/1 (frame 389/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 2 trucks, 1 backpack, 200.9ms
video 1/1 (frame 390/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 203.5ms
video 1/1 (frame 391/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 216.1ms
video 1/1 (frame 392/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 truck, 2 backpacks, 247.5ms
video 1/1 (frame 393/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 2 backpacks, 243.4ms
video 1/1 (frame 394/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 228.7ms
video 1/1 (frame 395/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 227.3ms
video 1/1 (frame 396/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 185.2ms
video 1/1 (frame 397/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 2 trucks, 3 backpacks, 289.5ms
video 1/1 (frame 398/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 trucks, 2 backpacks, 207.9ms
video 1/1 (frame 399/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 truck, 2 backpacks, 201.3ms
video 1/1 (frame 400/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 backpacks, 190.1ms
video 1/1 (frame 401/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 184.3ms
video 1/1 (frame 402/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 1 backpack, 188.6ms
video 1/1 (frame 403/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 190.4ms
video 1/1 (frame 404/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 202.8ms
video 1/1 (frame 405/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 206.5ms
video 1/1 (frame 406/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 214.3ms
video 1/1 (frame 407/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 190.3ms
video 1/1 (frame 408/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 212.2ms
video 1/1 (frame 409/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 1 truck, 1 backpack, 1 handbag, 200.9ms
video 1/1 (frame 410/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 204.3ms
video 1/1 (frame 411/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 201.2ms
video 1/1 (frame 412/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 187.5ms
video 1/1 (frame 413/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 191.6ms
video 1/1 (frame 414/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 187.9ms
video 1/1 (frame 415/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 198.1ms
video 1/1 (frame 416/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 206.1ms
video 1/1 (frame 417/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 2 backpacks, 199.9ms
video 1/1 (frame 418/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 1 suitcase, 212.3ms
video 1/1 (frame 419/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 truck, 3 backpacks, 179.6ms
video 1/1 (frame 420/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 3 backpacks, 178.8ms
video 1/1 (frame 421/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 198.7ms
video 1/1 (frame 422/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 2 backpacks, 191.7ms
video 1/1 (frame 423/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 202.8ms
video 1/1 (frame 424/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 2 trucks, 1 backpack, 190.3ms
video 1/1 (frame 425/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 2 trucks, 1 backpack, 193.1ms
video 1/1 (frame 426/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 194.1ms
video 1/1 (frame 427/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 193.7ms
video 1/1 (frame 428/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 196.2ms
video 1/1 (frame 429/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 2 trucks, 2 backpacks, 208.9ms
video 1/1 (frame 430/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 2 backpacks, 202.8ms
video 1/1 (frame 431/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 3 backpacks, 195.7ms
video 1/1 (frame 432/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 202.2ms
video 1/1 (frame 433/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 215.5ms
video 1/1 (frame 434/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 214.8ms
video 1/1 (frame 435/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 219.5ms
video 1/1 (frame 436/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 205.8ms
video 1/1 (frame 437/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 backpack, 229.1ms
video 1/1 (frame 438/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 212.0ms
video 1/1 (frame 439/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 backpack, 201.9ms
video 1/1 (frame 440/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 backpack, 197.7ms
video 1/1 (frame 441/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 2 backpacks, 204.1ms
video 1/1 (frame 442/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 227.3ms
video 1/1 (frame 443/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 backpack, 1 suitcase, 233.3ms
video 1/1 (frame 444/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 1 suitcase, 237.2ms
video 1/1 (frame 445/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 1 suitcase, 233.5ms
video 1/1 (frame 446/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 196.0ms
video 1/1 (frame 447/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 195.0ms
video 1/1 (frame 448/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 185.8ms
video 1/1 (frame 449/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 190.9ms
video 1/1 (frame 450/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 194.0ms
video 1/1 (frame 451/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 198.7ms
video 1/1 (frame 452/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 192.3ms
video 1/1 (frame 453/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 191.4ms
video 1/1 (frame 454/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 185.3ms
video 1/1 (frame 455/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 203.3ms
video 1/1 (frame 456/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 backpack, 193.4ms
video 1/1 (frame 457/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 192.5ms
video 1/1 (frame 458/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 186.3ms
video 1/1 (frame 459/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 192.4ms
video 1/1 (frame 460/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 196.4ms
video 1/1 (frame 461/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 195.6ms
video 1/1 (frame 462/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 202.9ms
video 1/1 (frame 463/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 198.8ms
video 1/1 (frame 464/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 1 backpack, 192.6ms
video 1/1 (frame 465/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 194.3ms
video 1/1 (frame 466/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 2 backpacks, 188.0ms
video 1/1 (frame 467/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 truck, 2 backpacks, 194.0ms
video 1/1 (frame 468/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 2 backpacks, 200.6ms
video 1/1 (frame 469/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 11 cars, 1 truck, 1 backpack, 201.5ms
video 1/1 (frame 470/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 truck, 1 backpack, 376.2ms
video 1/1 (frame 471/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 11 cars, 1 backpack, 215.9ms
video 1/1 (frame 472/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 12 cars, 1 backpack, 206.1ms
video 1/1 (frame 473/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 205.9ms
video 1/1 (frame 474/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 206.3ms
video 1/1 (frame 475/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 216.7ms
video 1/1 (frame 476/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 195.7ms
video 1/1 (frame 477/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 190.2ms
video 1/1 (frame 478/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 200.4ms
video 1/1 (frame 479/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 205.5ms
video 1/1 (frame 480/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 217.4ms
video 1/1 (frame 481/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 232.2ms
video 1/1 (frame 482/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 1 backpack, 198.5ms
video 1/1 (frame 483/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 208.3ms
video 1/1 (frame 484/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 207.2ms
video 1/1 (frame 485/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 200.6ms
video 1/1 (frame 486/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 212.8ms
video 1/1 (frame 487/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 222.7ms
video 1/1 (frame 488/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 222.2ms
video 1/1 (frame 489/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 220.9ms
video 1/1 (frame 490/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 224.3ms
video 1/1 (frame 491/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 236.6ms
video 1/1 (frame 492/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 270.7ms
video 1/1 (frame 493/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 243.9ms
video 1/1 (frame 494/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 237.8ms
video 1/1 (frame 495/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 backpacks, 198.2ms
video 1/1 (frame 496/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 195.2ms
video 1/1 (frame 497/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 196.8ms
video 1/1 (frame 498/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 194.5ms
video 1/1 (frame 499/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 202.5ms
video 1/1 (frame 500/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 192.3ms
video 1/1 (frame 501/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 210.0ms
video 1/1 (frame 502/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 199.9ms
video 1/1 (frame 503/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 backpack, 188.3ms
video 1/1 (frame 504/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 201.2ms
video 1/1 (frame 505/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 10 cars, 1 backpack, 214.4ms
video 1/1 (frame 506/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 210.2ms
video 1/1 (frame 507/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 backpack, 220.0ms
video 1/1 (frame 508/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 211.1ms
video 1/1 (frame 509/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 1 backpack, 190.9ms
video 1/1 (frame 510/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 1 backpack, 201.7ms
video 1/1 (frame 511/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 8 cars, 191.0ms
video 1/1 (frame 512/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 192.5ms
video 1/1 (frame 513/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 191.3ms
video 1/1 (frame 514/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 192.4ms
video 1/1 (frame 515/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 216.3ms
video 1/1 (frame 516/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 backpack, 195.0ms
video 1/1 (frame 517/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 backpack, 197.6ms
video 1/1 (frame 518/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 203.0ms
video 1/1 (frame 519/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 8 cars, 185.9ms
video 1/1 (frame 520/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 194.5ms
video 1/1 (frame 521/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 188.6ms
video 1/1 (frame 522/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 backpack, 182.5ms
video 1/1 (frame 523/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 193.2ms
video 1/1 (frame 524/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 2 backpacks, 209.2ms
video 1/1 (frame 525/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 197.2ms
video 1/1 (frame 526/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 192.8ms
video 1/1 (frame 527/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 219.5ms
video 1/1 (frame 528/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 223.5ms
video 1/1 (frame 529/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 221.6ms
video 1/1 (frame 530/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 191.6ms
video 1/1 (frame 531/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 185.5ms
video 1/1 (frame 532/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 194.0ms
video 1/1 (frame 533/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 21 persons, 8 cars, 1 truck, 1 backpack, 189.5ms
video 1/1 (frame 534/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 9 cars, 1 backpack, 195.1ms
video 1/1 (frame 535/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 9 cars, 1 truck, 1 backpack, 186.8ms
video 1/1 (frame 536/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 1 truck, 199.6ms
video 1/1 (frame 537/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 202.1ms
video 1/1 (frame 538/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 9 cars, 197.3ms
video 1/1 (frame 539/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 8 cars, 193.4ms
video 1/1 (frame 540/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 8 cars, 198.7ms
video 1/1 (frame 541/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 191.6ms
video 1/1 (frame 542/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 209.7ms
video 1/1 (frame 543/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 219.3ms
video 1/1 (frame 544/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 226.8ms
video 1/1 (frame 545/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 239.3ms
video 1/1 (frame 546/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 199.9ms
video 1/1 (frame 547/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 192.2ms
video 1/1 (frame 548/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 21 persons, 6 cars, 194.5ms
video 1/1 (frame 549/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 208.9ms
video 1/1 (frame 550/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 258.2ms
video 1/1 (frame 551/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 234.3ms
video 1/1 (frame 552/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 266.3ms
video 1/1 (frame 553/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 242.5ms
video 1/1 (frame 554/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 212.2ms
video 1/1 (frame 555/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 5 cars, 186.9ms
video 1/1 (frame 556/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 5 cars, 201.1ms
video 1/1 (frame 557/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 187.0ms
video 1/1 (frame 558/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 189.6ms
video 1/1 (frame 559/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 200.0ms
video 1/1 (frame 560/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 192.7ms
video 1/1 (frame 561/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 201.0ms
video 1/1 (frame 562/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 200.9ms
video 1/1 (frame 563/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 215.3ms
video 1/1 (frame 564/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 215.9ms
video 1/1 (frame 565/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 216.0ms
video 1/1 (frame 566/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 5 cars, 192.1ms
video 1/1 (frame 567/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 199.4ms
video 1/1 (frame 568/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 202.0ms
video 1/1 (frame 569/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 191.4ms
video 1/1 (frame 570/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 198.7ms
video 1/1 (frame 571/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 203.3ms
video 1/1 (frame 572/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 196.5ms
video 1/1 (frame 573/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 211.3ms
video 1/1 (frame 574/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 202.7ms
video 1/1 (frame 575/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 204.2ms
video 1/1 (frame 576/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 198.3ms
video 1/1 (frame 577/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 207.9ms
video 1/1 (frame 578/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 356.4ms
video 1/1 (frame 579/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 218.5ms
video 1/1 (frame 580/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 204.3ms
video 1/1 (frame 581/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 193.8ms
video 1/1 (frame 582/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 handbag, 204.1ms
video 1/1 (frame 583/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 213.0ms
video 1/1 (frame 584/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 200.3ms
video 1/1 (frame 585/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 191.8ms
video 1/1 (frame 586/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 191.3ms
video 1/1 (frame 587/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 200.2ms
video 1/1 (frame 588/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 193.8ms
video 1/1 (frame 589/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 196.0ms
video 1/1 (frame 590/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 193.7ms
video 1/1 (frame 591/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 224.2ms
video 1/1 (frame 592/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 handbag, 225.1ms
video 1/1 (frame 593/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 handbag, 239.9ms
video 1/1 (frame 594/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 1 handbag, 221.4ms
video 1/1 (frame 595/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 handbag, 192.1ms
video 1/1 (frame 596/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 214.1ms
video 1/1 (frame 597/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 221.3ms
video 1/1 (frame 598/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 8 cars, 213.9ms
video 1/1 (frame 599/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 217.0ms
video 1/1 (frame 600/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 handbag, 208.9ms
video 1/1 (frame 601/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 212.0ms
video 1/1 (frame 602/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 216.4ms
video 1/1 (frame 603/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 203.1ms
video 1/1 (frame 604/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 210.5ms
video 1/1 (frame 605/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 193.6ms
video 1/1 (frame 606/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 223.0ms
video 1/1 (frame 607/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 5 cars, 217.3ms
video 1/1 (frame 608/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 180.5ms
video 1/1 (frame 609/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 209.3ms
video 1/1 (frame 610/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 212.7ms
video 1/1 (frame 611/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 201.4ms
video 1/1 (frame 612/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 203.6ms
video 1/1 (frame 613/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 205.1ms
video 1/1 (frame 614/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 207.1ms
video 1/1 (frame 615/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 187.4ms
video 1/1 (frame 616/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 handbag, 227.6ms
video 1/1 (frame 617/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 handbag, 209.3ms
video 1/1 (frame 618/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 194.0ms
video 1/1 (frame 619/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 217.8ms
video 1/1 (frame 620/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 214.8ms
video 1/1 (frame 621/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 197.1ms
video 1/1 (frame 622/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 11 cars, 203.1ms
video 1/1 (frame 623/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 197.2ms
video 1/1 (frame 624/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 13 cars, 1 truck, 200.1ms
video 1/1 (frame 625/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 truck, 227.4ms
video 1/1 (frame 626/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 199.5ms
video 1/1 (frame 627/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 190.1ms
video 1/1 (frame 628/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 201.1ms
video 1/1 (frame 629/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 11 cars, 1 truck, 199.4ms
video 1/1 (frame 630/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 209.1ms
video 1/1 (frame 631/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 196.7ms
video 1/1 (frame 632/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 1 handbag, 204.7ms
video 1/1 (frame 633/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 206.2ms
video 1/1 (frame 634/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 200.3ms
video 1/1 (frame 635/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 9 cars, 205.9ms
video 1/1 (frame 636/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 200.0ms
video 1/1 (frame 637/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 191.6ms
video 1/1 (frame 638/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 backpack, 211.9ms
video 1/1 (frame 639/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 207.4ms
video 1/1 (frame 640/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 10 cars, 1 backpack, 229.8ms
video 1/1 (frame 641/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 236.2ms
video 1/1 (frame 642/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 1 backpack, 239.4ms
video 1/1 (frame 643/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 1 backpack, 1 handbag, 251.0ms
video 1/1 (frame 644/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 1 backpack, 198.8ms
video 1/1 (frame 645/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 1 handbag, 190.4ms
video 1/1 (frame 646/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 200.3ms
video 1/1 (frame 647/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 1 handbag, 194.6ms
video 1/1 (frame 648/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 handbag, 196.2ms
video 1/1 (frame 649/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 207.6ms
video 1/1 (frame 650/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 188.5ms
Speed: 4.8ms preprocess, 209.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=4faeb227-2705-49ec-9f8d-bb87ff8cf403">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Output shows that the YOLO model has successfully processed each frame of the video, detecting </span>
<span class="n">various</span> <span class="n">objects</span> <span class="p">(</span><span class="n">persons</span><span class="p">,</span> <span class="n">cars</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">)</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">frame</span><span class="o">.</span>

<span class="c1"># Interpretation:</span>

<span class="c1"># Detection Speed and Latency:</span>

<span class="n">Preprocessing</span> <span class="n">time</span><span class="p">:</span> <span class="mf">4.8</span> <span class="n">ms</span>
<span class="n">Inference</span> <span class="n">time</span><span class="p">:</span> <span class="mf">209.3</span> <span class="n">ms</span>
<span class="n">Postprocessing</span> <span class="n">time</span><span class="p">:</span> 
<span class="mf">3.3</span> <span class="n">ms</span> <span class="n">These</span> <span class="n">timings</span> <span class="n">indicate</span> <span class="n">that</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">processing</span> <span class="n">each</span> <span class="n">frame</span> <span class="n">efficiently</span><span class="p">,</span> <span class="n">especially</span> <span class="n">the</span> 
<span class="n">inference</span> <span class="n">time</span><span class="o">.</span> <span class="n">For</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="ow">or</span> <span class="n">near</span> <span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">applications</span><span class="p">,</span> <span class="n">this</span> <span class="n">speed</span> <span class="ow">is</span> <span class="n">typically</span> <span class="n">adequate</span><span class="o">.</span>

<span class="c1"># Detected Objects:</span>

<span class="n">In</span> <span class="n">frame</span> <span class="mi">649</span><span class="p">,</span> <span class="n">the</span> <span class="n">model</span> <span class="n">detected</span><span class="p">:</span> <span class="mi">15</span> <span class="n">persons</span> <span class="mi">6</span> <span class="n">cars</span>
<span class="n">In</span> <span class="n">frame</span> <span class="mi">650</span><span class="p">,</span> <span class="n">the</span> <span class="n">model</span> <span class="n">detected</span><span class="p">:</span> <span class="mi">15</span> <span class="n">persons</span> <span class="mi">5</span> <span class="n">cars</span> <span class="n">These</span> <span class="n">detections</span> <span class="n">align</span> <span class="k">with</span> <span class="n">the</span> <span class="n">expectations</span> <span class="k">for</span>
<span class="n">a</span> <span class="n">typical</span> <span class="n">video</span> <span class="n">scene</span> <span class="k">with</span> <span class="n">multiple</span> <span class="n">persons</span> <span class="ow">and</span> <span class="n">vehicles</span><span class="o">.</span> <span class="n">The</span> <span class="n">slight</span> <span class="n">variation</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> 
<span class="n">detected</span> <span class="n">cars</span> <span class="n">could</span> <span class="n">be</span> <span class="n">due</span> <span class="n">to</span> <span class="n">the</span> <span class="n">movement</span> <span class="n">of</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">scene</span><span class="o">.</span>
    
<span class="c1"># Resolution: </span>
<span class="n">The</span> <span class="nb">input</span> <span class="n">video</span> <span class="n">has</span> <span class="n">a</span> <span class="n">resolution</span> <span class="n">of</span> <span class="mi">384</span><span class="n">x640</span><span class="p">,</span> <span class="n">which</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">reasonable</span> <span class="n">balance</span> <span class="n">between</span> <span class="n">accuracy</span> <span class="ow">and</span> <span class="n">speed</span><span class="o">.</span> 
<span class="n">It</span> <span class="n">allows</span> <span class="k">for</span> <span class="n">efficient</span> <span class="nb">object</span> <span class="n">detection</span> <span class="k">while</span> <span class="n">maintaining</span> <span class="n">sufficient</span> <span class="n">detail</span> <span class="k">for</span> <span class="n">the</span> <span class="n">model</span> <span class="n">to</span> <span class="n">perform</span> <span class="n">well</span><span class="o">.</span>

<span class="c1"># Model Output: The output from YOLO shows the detected objects with the following types:Persons,Cars</span>
<span class="n">The</span> <span class="n">numbers</span> <span class="n">may</span> <span class="n">vary</span> <span class="n">depending</span> <span class="n">on</span> <span class="n">the</span> <span class="n">content</span> <span class="n">of</span> <span class="n">each</span> <span class="n">frame</span><span class="p">,</span> <span class="ow">and</span> <span class="n">YOLO</span><span class="s1">'s ability to detect different types</span>
<span class="n">of</span> <span class="n">objects</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">scene</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=aec94647-2222-4511-a7df-c12da022a410">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Storage and Saving Predictions:</span>
<span class="n">The</span> <span class="n">predictions</span> <span class="n">gathered</span> <span class="n">across</span> <span class="n">the</span> <span class="n">frames</span> <span class="n">need</span> <span class="n">to</span> <span class="n">be</span> <span class="n">properly</span> <span class="n">stored</span> <span class="k">for</span> <span class="n">future</span> <span class="n">analysis</span> <span class="ow">or</span> <span class="n">evaluation</span>
<span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">mAP</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">)</span><span class="o">.</span><span class="n">There</span> <span class="n">are</span> <span class="n">a</span> <span class="n">few</span> <span class="n">methods</span> <span class="n">to</span> <span class="n">handle</span> <span class="n">this</span><span class="p">:</span>
<span class="c1"># Storage of Predictions:</span>
<span class="n">The</span> <span class="n">results</span> <span class="k">for</span> <span class="n">each</span> <span class="n">frame</span> <span class="n">are</span> <span class="n">stored</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">variable</span> <span class="n">predictions</span><span class="o">.</span>
<span class="c1"># Ensure Model Inference is Performed Correctly: Make sure the YOLO model is invoked correctly and its </span>
<span class="n">output</span> <span class="ow">is</span> <span class="n">assigned</span> <span class="n">to</span> <span class="n">a</span> <span class="n">variable</span> <span class="n">named</span> <span class="n">results</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=dcb26a52-2ea1-4c80-934b-90048d6c51ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="c1"># Load the YOLO model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">'yolov8n.pt'</span><span class="p">)</span>  <span class="c1"># Load a pre-trained YOLO model (e.g., YOLOv8n)</span>

<span class="c1"># Path to the video file</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\30952-383991415_small.mp4"</span>

<span class="c1"># Perform inference on the video</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>

WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory
errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.

Example:
    results = model(source=..., stream=True)  # generator of Results objects
    for r in results:
        boxes = r.boxes  # Boxes object for bbox outputs
        masks = r.masks  # Masks object for segment masks outputs
        probs = r.probs  # Class probabilities for classification outputs

video 1/1 (frame 1/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 272.1ms
video 1/1 (frame 2/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 2 cars, 1 truck, 1 backpack, 2 bottles, 238.6ms
video 1/1 (frame 3/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 1 backpack, 1 bottle, 227.4ms
video 1/1 (frame 4/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 214.6ms
video 1/1 (frame 5/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 truck, 2 bottles, 228.2ms
video 1/1 (frame 6/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 truck, 1 bottle, 203.3ms
video 1/1 (frame 7/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 truck, 2 bottles, 197.8ms
video 1/1 (frame 8/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 3 cars, 1 truck, 2 bottles, 398.9ms
video 1/1 (frame 9/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 truck, 304.5ms
video 1/1 (frame 10/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 truck, 1 bottle, 297.8ms
video 1/1 (frame 11/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 truck, 398.5ms
video 1/1 (frame 12/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 truck, 2 bottles, 283.5ms
video 1/1 (frame 13/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 truck, 2 bottles, 237.3ms
video 1/1 (frame 14/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 truck, 3 bottles, 246.7ms
video 1/1 (frame 15/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 3 bottles, 213.1ms
video 1/1 (frame 16/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 truck, 1 backpack, 1 bottle, 193.4ms
video 1/1 (frame 17/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 195.6ms
video 1/1 (frame 18/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 242.2ms
video 1/1 (frame 19/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 bottle, 246.3ms
video 1/1 (frame 20/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 239.0ms
video 1/1 (frame 21/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 201.7ms
video 1/1 (frame 22/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 222.5ms
video 1/1 (frame 23/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 228.1ms
video 1/1 (frame 24/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 265.3ms
video 1/1 (frame 25/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 231.4ms
video 1/1 (frame 26/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 203.5ms
video 1/1 (frame 27/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 182.9ms
video 1/1 (frame 28/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 198.0ms
video 1/1 (frame 29/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 203.9ms
video 1/1 (frame 30/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 207.4ms
video 1/1 (frame 31/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 220.3ms
video 1/1 (frame 32/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 bus, 209.5ms
video 1/1 (frame 33/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 185.8ms
video 1/1 (frame 34/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 176.2ms
video 1/1 (frame 35/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 5 cars, 1 bus, 199.1ms
video 1/1 (frame 36/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 211.1ms
video 1/1 (frame 37/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 1 truck, 189.2ms
video 1/1 (frame 38/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 193.9ms
video 1/1 (frame 39/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 179.4ms
video 1/1 (frame 40/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 169.1ms
video 1/1 (frame 41/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 198.4ms
video 1/1 (frame 42/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 190.7ms
video 1/1 (frame 43/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 188.5ms
video 1/1 (frame 44/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 1 truck, 197.9ms
video 1/1 (frame 45/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 206.3ms
video 1/1 (frame 46/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 211.9ms
video 1/1 (frame 47/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 2 cars, 1 bus, 235.5ms
video 1/1 (frame 48/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 230.1ms
video 1/1 (frame 49/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 214.5ms
video 1/1 (frame 50/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 259.6ms
video 1/1 (frame 51/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 215.6ms
video 1/1 (frame 52/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 199.2ms
video 1/1 (frame 53/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 193.0ms
video 1/1 (frame 54/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 201.5ms
video 1/1 (frame 55/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 183.6ms
video 1/1 (frame 56/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 189.6ms
video 1/1 (frame 57/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 177.4ms
video 1/1 (frame 58/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 1 car, 1 bus, 177.4ms
video 1/1 (frame 59/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 1 truck, 208.7ms
video 1/1 (frame 60/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 220.2ms
video 1/1 (frame 61/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 1 car, 1 bus, 233.4ms
video 1/1 (frame 62/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 240.3ms
video 1/1 (frame 63/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 truck, 279.9ms
video 1/1 (frame 64/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 197.5ms
video 1/1 (frame 65/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 1 truck, 208.0ms
video 1/1 (frame 66/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 196.1ms
video 1/1 (frame 67/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 197.5ms
video 1/1 (frame 68/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 2 cars, 1 bus, 209.0ms
video 1/1 (frame 69/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 2 cars, 1 bus, 200.5ms
video 1/1 (frame 70/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 251.4ms
video 1/1 (frame 71/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 2 cars, 1 bus, 444.3ms
video 1/1 (frame 72/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 1 car, 1 bus, 215.6ms
video 1/1 (frame 73/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 209.9ms
video 1/1 (frame 74/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 bus, 196.0ms
video 1/1 (frame 75/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 bus, 220.1ms
video 1/1 (frame 76/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 262.8ms
video 1/1 (frame 77/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 253.3ms
video 1/1 (frame 78/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 1 truck, 199.9ms
video 1/1 (frame 79/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 bus, 207.3ms
video 1/1 (frame 80/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 198.1ms
video 1/1 (frame 81/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 bus, 1 truck, 222.6ms
video 1/1 (frame 82/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 182.6ms
video 1/1 (frame 83/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 1 truck, 185.1ms
video 1/1 (frame 84/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 2 cars, 1 truck, 1 handbag, 201.8ms
video 1/1 (frame 85/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 1 car, 1 bus, 1 truck, 228.8ms
video 1/1 (frame 86/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 1 car, 1 truck, 252.3ms
video 1/1 (frame 87/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 2 cars, 1 bus, 194.3ms
video 1/1 (frame 88/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 214.0ms
video 1/1 (frame 89/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 257.2ms
video 1/1 (frame 90/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 254.5ms
video 1/1 (frame 91/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 202.2ms
video 1/1 (frame 92/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 198.4ms
video 1/1 (frame 93/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 203.3ms
video 1/1 (frame 94/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 217.3ms
video 1/1 (frame 95/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 1 handbag, 243.7ms
video 1/1 (frame 96/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 bus, 280.2ms
video 1/1 (frame 97/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 4 cars, 1 bus, 370.6ms
video 1/1 (frame 98/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 234.3ms
video 1/1 (frame 99/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 172.3ms
video 1/1 (frame 100/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 bus, 1 handbag, 164.5ms
video 1/1 (frame 101/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 182.1ms
video 1/1 (frame 102/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 handbag, 194.4ms
video 1/1 (frame 103/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 218.3ms
video 1/1 (frame 104/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 227.1ms
video 1/1 (frame 105/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 221.2ms
video 1/1 (frame 106/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 2 handbags, 190.8ms
video 1/1 (frame 107/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 211.1ms
video 1/1 (frame 108/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 208.9ms
video 1/1 (frame 109/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 handbag, 212.9ms
video 1/1 (frame 110/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 228.8ms
video 1/1 (frame 111/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 230.7ms
video 1/1 (frame 112/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 230.7ms
video 1/1 (frame 113/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 195.6ms
video 1/1 (frame 114/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 191.0ms
video 1/1 (frame 115/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 207.9ms
video 1/1 (frame 116/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 4 cars, 1 bus, 222.8ms
video 1/1 (frame 117/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 200.1ms
video 1/1 (frame 118/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 218.8ms
video 1/1 (frame 119/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 234.1ms
video 1/1 (frame 120/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 backpack, 225.9ms
video 1/1 (frame 121/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 bus, 1 backpack, 225.1ms
video 1/1 (frame 122/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 3 cars, 1 bus, 1 backpack, 192.3ms
video 1/1 (frame 123/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 1 backpack, 234.3ms
video 1/1 (frame 124/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 2 motorcycles, 1 bus, 1 backpack, 232.0ms
video 1/1 (frame 125/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 240.3ms
video 1/1 (frame 126/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 219.8ms
video 1/1 (frame 127/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 214.1ms
video 1/1 (frame 128/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 1 backpack, 220.8ms
video 1/1 (frame 129/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 5 cars, 1 bus, 1 backpack, 241.9ms
video 1/1 (frame 130/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 motorcycle, 1 bus, 1 backpack, 348.4ms
video 1/1 (frame 131/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 backpack, 331.5ms
video 1/1 (frame 132/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 2 motorcycles, 1 bus, 1 backpack, 248.7ms
video 1/1 (frame 133/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 bus, 1 backpack, 215.2ms
video 1/1 (frame 134/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 4 cars, 1 bus, 223.1ms
video 1/1 (frame 135/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 backpack, 219.4ms
video 1/1 (frame 136/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 backpack, 238.5ms
video 1/1 (frame 137/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 1 backpack, 224.6ms
video 1/1 (frame 138/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 258.4ms
video 1/1 (frame 139/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 3 cars, 1 bus, 2 trucks, 243.7ms
video 1/1 (frame 140/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 3 cars, 1 bus, 1 truck, 272.8ms
video 1/1 (frame 141/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 3 cars, 1 bus, 1 truck, 244.5ms
video 1/1 (frame 142/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 1 truck, 222.7ms
video 1/1 (frame 143/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 2 trucks, 174.1ms
video 1/1 (frame 144/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 173.4ms
video 1/1 (frame 145/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 4 cars, 1 bus, 2 trucks, 168.4ms
video 1/1 (frame 146/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 2 trucks, 1 backpack, 149.7ms
video 1/1 (frame 147/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 bus, 1 truck, 168.7ms
video 1/1 (frame 148/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 171.2ms
video 1/1 (frame 149/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 4 cars, 1 bus, 1 truck, 1 backpack, 175.6ms
video 1/1 (frame 150/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 1 backpack, 197.6ms
video 1/1 (frame 151/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 1 backpack, 192.4ms
video 1/1 (frame 152/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 5 cars, 1 bus, 1 truck, 214.9ms
video 1/1 (frame 153/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 205.8ms
video 1/1 (frame 154/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 220.3ms
video 1/1 (frame 155/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 218.7ms
video 1/1 (frame 156/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 217.6ms
video 1/1 (frame 157/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 bus, 217.9ms
video 1/1 (frame 158/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 backpack, 207.8ms
video 1/1 (frame 159/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 bus, 1 truck, 215.6ms
video 1/1 (frame 160/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 bus, 188.7ms
video 1/1 (frame 161/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 227.7ms
video 1/1 (frame 162/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 208.6ms
video 1/1 (frame 163/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 1 truck, 210.2ms
video 1/1 (frame 164/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 1 motorcycle, 1 bus, 195.1ms
video 1/1 (frame 165/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 bus, 190.7ms
video 1/1 (frame 166/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 8 cars, 1 bus, 1 truck, 221.7ms
video 1/1 (frame 167/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 9 cars, 1 bus, 1 truck, 195.8ms
video 1/1 (frame 168/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 bus, 1 truck, 230.0ms
video 1/1 (frame 169/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 bus, 1 truck, 209.9ms
video 1/1 (frame 170/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 230.8ms
video 1/1 (frame 171/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 bus, 260.3ms
video 1/1 (frame 172/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 bus, 1 truck, 282.8ms
video 1/1 (frame 173/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 bus, 1 truck, 279.0ms
video 1/1 (frame 174/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 268.4ms
video 1/1 (frame 175/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 bus, 288.3ms
video 1/1 (frame 176/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 426.1ms
video 1/1 (frame 177/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 truck, 190.5ms
video 1/1 (frame 178/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 184.1ms
video 1/1 (frame 179/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 217.9ms
video 1/1 (frame 180/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 191.6ms
video 1/1 (frame 181/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 225.5ms
video 1/1 (frame 182/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 246.0ms
video 1/1 (frame 183/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 233.6ms
video 1/1 (frame 184/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 212.4ms
video 1/1 (frame 185/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 218.0ms
video 1/1 (frame 186/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 motorcycle, 295.6ms
video 1/1 (frame 187/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 truck, 256.4ms
video 1/1 (frame 188/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 265.4ms
video 1/1 (frame 189/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 274.2ms
video 1/1 (frame 190/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 truck, 225.9ms
video 1/1 (frame 191/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 226.0ms
video 1/1 (frame 192/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 229.3ms
video 1/1 (frame 193/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 225.8ms
video 1/1 (frame 194/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 207.4ms
video 1/1 (frame 195/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 221.1ms
video 1/1 (frame 196/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 1 truck, 221.4ms
video 1/1 (frame 197/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 220.9ms
video 1/1 (frame 198/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 189.1ms
video 1/1 (frame 199/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 198.6ms
video 1/1 (frame 200/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 218.0ms
video 1/1 (frame 201/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 213.1ms
video 1/1 (frame 202/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 218.3ms
video 1/1 (frame 203/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 214.0ms
video 1/1 (frame 204/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 224.9ms
video 1/1 (frame 205/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 11 cars, 1 motorcycle, 216.3ms
video 1/1 (frame 206/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 238.4ms
video 1/1 (frame 207/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 225.3ms
video 1/1 (frame 208/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 suitcase, 211.3ms
video 1/1 (frame 209/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 suitcase, 211.8ms
video 1/1 (frame 210/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 218.4ms
video 1/1 (frame 211/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 209.1ms
video 1/1 (frame 212/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 189.3ms
video 1/1 (frame 213/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 202.3ms
video 1/1 (frame 214/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 205.4ms
video 1/1 (frame 215/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 214.3ms
video 1/1 (frame 216/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 234.3ms
video 1/1 (frame 217/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 226.8ms
video 1/1 (frame 218/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 187.8ms
video 1/1 (frame 219/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 199.5ms
video 1/1 (frame 220/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 195.3ms
video 1/1 (frame 221/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 209.0ms
video 1/1 (frame 222/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 217.6ms
video 1/1 (frame 223/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 221.1ms
video 1/1 (frame 224/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 206.2ms
video 1/1 (frame 225/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 183.4ms
video 1/1 (frame 226/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 192.9ms
video 1/1 (frame 227/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 210.3ms
video 1/1 (frame 228/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 12 cars, 233.9ms
video 1/1 (frame 229/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 11 cars, 203.1ms
video 1/1 (frame 230/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 214.4ms
video 1/1 (frame 231/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 11 cars, 226.6ms
video 1/1 (frame 232/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 214.7ms
video 1/1 (frame 233/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 208.0ms
video 1/1 (frame 234/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 255.3ms
video 1/1 (frame 235/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 238.6ms
video 1/1 (frame 236/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 259.6ms
video 1/1 (frame 237/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 208.0ms
video 1/1 (frame 238/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 220.5ms
video 1/1 (frame 239/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 298.4ms
video 1/1 (frame 240/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 229.2ms
video 1/1 (frame 241/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 249.8ms
video 1/1 (frame 242/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 230.0ms
video 1/1 (frame 243/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 246.0ms
video 1/1 (frame 244/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 222.3ms
video 1/1 (frame 245/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 272.2ms
video 1/1 (frame 246/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 226.9ms
video 1/1 (frame 247/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 1 handbag, 211.6ms
video 1/1 (frame 248/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 1 handbag, 217.7ms
video 1/1 (frame 249/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 1 handbag, 224.2ms
video 1/1 (frame 250/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 235.1ms
video 1/1 (frame 251/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 222.6ms
video 1/1 (frame 252/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 220.7ms
video 1/1 (frame 253/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 201.5ms
video 1/1 (frame 254/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 223.0ms
video 1/1 (frame 255/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 212.6ms
video 1/1 (frame 256/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 220.2ms
video 1/1 (frame 257/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 224.7ms
video 1/1 (frame 258/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 1 truck, 396.5ms
video 1/1 (frame 259/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 220.0ms
video 1/1 (frame 260/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 handbag, 208.0ms
video 1/1 (frame 261/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 1 handbag, 230.3ms
video 1/1 (frame 262/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 234.8ms
video 1/1 (frame 263/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 backpack, 234.1ms
video 1/1 (frame 264/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 backpack, 227.1ms
video 1/1 (frame 265/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 216.3ms
video 1/1 (frame 266/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 211.5ms
video 1/1 (frame 267/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 223.0ms
video 1/1 (frame 268/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 232.9ms
video 1/1 (frame 269/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 237.9ms
video 1/1 (frame 270/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 211.6ms
video 1/1 (frame 271/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 10 cars, 210.2ms
video 1/1 (frame 272/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 233.2ms
video 1/1 (frame 273/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 212.5ms
video 1/1 (frame 274/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 216.1ms
video 1/1 (frame 275/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 backpack, 215.8ms
video 1/1 (frame 276/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 backpack, 205.0ms
video 1/1 (frame 277/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 201.1ms
video 1/1 (frame 278/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 251.8ms
video 1/1 (frame 279/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 1 backpack, 233.5ms
video 1/1 (frame 280/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 truck, 1 backpack, 248.5ms
video 1/1 (frame 281/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 228.9ms
video 1/1 (frame 282/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 backpack, 1 handbag, 246.6ms
video 1/1 (frame 283/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 9 cars, 1 backpack, 1 handbag, 247.0ms
video 1/1 (frame 284/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 1 backpack, 1 handbag, 221.6ms
video 1/1 (frame 285/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 backpack, 209.2ms
video 1/1 (frame 286/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 13 cars, 1 handbag, 235.8ms
video 1/1 (frame 287/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 2 motorcycles, 1 handbag, 239.9ms
video 1/1 (frame 288/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 2 backpacks, 1 handbag, 247.4ms
video 1/1 (frame 289/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 2 backpacks, 211.5ms
video 1/1 (frame 290/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 2 backpacks, 229.0ms
video 1/1 (frame 291/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 2 backpacks, 215.1ms
video 1/1 (frame 292/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 211.1ms
video 1/1 (frame 293/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 214.0ms
video 1/1 (frame 294/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 220.7ms
video 1/1 (frame 295/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 motorcycles, 2 backpacks, 219.8ms
video 1/1 (frame 296/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 motorcycle, 2 backpacks, 212.7ms
video 1/1 (frame 297/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 2 motorcycles, 2 backpacks, 224.2ms
video 1/1 (frame 298/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 motorcycle, 2 backpacks, 216.8ms
video 1/1 (frame 299/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 221.8ms
video 1/1 (frame 300/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 233.4ms
video 1/1 (frame 301/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 1 handbag, 232.4ms
video 1/1 (frame 302/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 1 backpack, 230.3ms
video 1/1 (frame 303/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 222.2ms
video 1/1 (frame 304/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 208.2ms
video 1/1 (frame 305/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 10 cars, 1 truck, 1 backpack, 208.9ms
video 1/1 (frame 306/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 truck, 1 backpack, 214.4ms
video 1/1 (frame 307/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 10 cars, 1 motorcycle, 1 truck, 1 backpack, 233.8ms
video 1/1 (frame 308/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 239.9ms
video 1/1 (frame 309/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 216.2ms
video 1/1 (frame 310/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 1 backpack, 221.6ms
video 1/1 (frame 311/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 209.8ms
video 1/1 (frame 312/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 1 boat, 2 backpacks, 210.3ms
video 1/1 (frame 313/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 1 truck, 219.2ms
video 1/1 (frame 314/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 206.8ms
video 1/1 (frame 315/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 204.7ms
video 1/1 (frame 316/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 umbrella, 199.5ms
video 1/1 (frame 317/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 210.1ms
video 1/1 (frame 318/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 224.6ms
video 1/1 (frame 319/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 229.9ms
video 1/1 (frame 320/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 232.1ms
video 1/1 (frame 321/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 227.1ms
video 1/1 (frame 322/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 231.5ms
video 1/1 (frame 323/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 motorcycle, 1 truck, 1 boat, 1 backpack, 218.9ms
video 1/1 (frame 324/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 motorcycle, 1 truck, 2 backpacks, 231.6ms
video 1/1 (frame 325/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 449.3ms
video 1/1 (frame 326/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 261.7ms
video 1/1 (frame 327/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 11 cars, 1 motorcycle, 1 truck, 1 backpack, 243.2ms
video 1/1 (frame 328/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 1 truck, 1 backpack, 235.5ms
video 1/1 (frame 329/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 219.5ms
video 1/1 (frame 330/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 214.7ms
video 1/1 (frame 331/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 motorcycle, 2 trucks, 1 backpack, 219.1ms
video 1/1 (frame 332/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 1 bicycle, 11 cars, 1 motorcycle, 2 trucks, 2 backpacks, 202.0ms
video 1/1 (frame 333/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 227.7ms
video 1/1 (frame 334/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 motorcycle, 2 trucks, 1 backpack, 199.0ms
video 1/1 (frame 335/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 220.6ms
video 1/1 (frame 336/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 motorcycle, 2 trucks, 1 horse, 205.8ms
video 1/1 (frame 337/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 motorcycle, 2 trucks, 213.3ms
video 1/1 (frame 338/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 234.1ms
video 1/1 (frame 339/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 motorcycle, 1 truck, 221.3ms
video 1/1 (frame 340/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 225.0ms
video 1/1 (frame 341/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 229.1ms
video 1/1 (frame 342/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 210.4ms
video 1/1 (frame 343/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 2 trucks, 217.3ms
video 1/1 (frame 344/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 197.9ms
video 1/1 (frame 345/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 206.3ms
video 1/1 (frame 346/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 motorcycle, 1 truck, 219.5ms
video 1/1 (frame 347/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 219.9ms
video 1/1 (frame 348/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 222.6ms
video 1/1 (frame 349/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 motorcycle, 1 truck, 214.7ms
video 1/1 (frame 350/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 217.0ms
video 1/1 (frame 351/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 226.5ms
video 1/1 (frame 352/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 motorcycle, 1 truck, 229.7ms
video 1/1 (frame 353/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 motorcycle, 1 truck, 221.5ms
video 1/1 (frame 354/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 206.5ms
video 1/1 (frame 355/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 8 persons, 10 cars, 1 motorcycle, 1 truck, 238.3ms
video 1/1 (frame 356/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 229.1ms
video 1/1 (frame 357/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 9 cars, 1 motorcycle, 1 truck, 277.6ms
video 1/1 (frame 358/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 motorcycle, 1 truck, 213.1ms
video 1/1 (frame 359/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 8 cars, 1 motorcycle, 1 truck, 225.9ms
video 1/1 (frame 360/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 10 cars, 1 motorcycle, 1 truck, 260.4ms
video 1/1 (frame 361/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 11 cars, 1 motorcycle, 1 truck, 248.8ms
video 1/1 (frame 362/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 9 persons, 10 cars, 1 truck, 239.3ms
video 1/1 (frame 363/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 7 cars, 1 truck, 252.7ms
video 1/1 (frame 364/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 1 truck, 298.6ms
video 1/1 (frame 365/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 11 cars, 1 truck, 282.9ms
video 1/1 (frame 366/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 10 cars, 1 truck, 385.1ms
video 1/1 (frame 367/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 9 cars, 1 truck, 347.2ms
video 1/1 (frame 368/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 280.5ms
video 1/1 (frame 369/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 349.1ms
video 1/1 (frame 370/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 287.9ms
video 1/1 (frame 371/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 254.4ms
video 1/1 (frame 372/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 416.3ms
video 1/1 (frame 373/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 6 cars, 1 truck, 235.2ms
video 1/1 (frame 374/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 219.7ms
video 1/1 (frame 375/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 9 cars, 1 truck, 199.7ms
video 1/1 (frame 376/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 222.7ms
video 1/1 (frame 377/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 2 trucks, 1 backpack, 205.7ms
video 1/1 (frame 378/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 2 trucks, 1 backpack, 1 handbag, 197.8ms
video 1/1 (frame 379/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 2 trucks, 1 backpack, 187.7ms
video 1/1 (frame 380/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 1 handbag, 187.0ms
video 1/1 (frame 381/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 2 trucks, 189.5ms
video 1/1 (frame 382/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 trucks, 1 backpack, 206.3ms
video 1/1 (frame 383/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 216.8ms
video 1/1 (frame 384/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 2 trucks, 1 backpack, 210.7ms
video 1/1 (frame 385/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 3 cars, 2 trucks, 1 backpack, 226.3ms
video 1/1 (frame 386/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 4 cars, 1 truck, 1 backpack, 233.1ms
video 1/1 (frame 387/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 1 truck, 1 backpack, 203.8ms
video 1/1 (frame 388/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 1 truck, 1 backpack, 206.4ms
video 1/1 (frame 389/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 2 trucks, 1 backpack, 204.4ms
video 1/1 (frame 390/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 1 backpack, 225.7ms
video 1/1 (frame 391/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 trucks, 1 backpack, 203.6ms
video 1/1 (frame 392/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 5 cars, 1 truck, 2 backpacks, 194.3ms
video 1/1 (frame 393/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 5 cars, 1 truck, 2 backpacks, 223.4ms
video 1/1 (frame 394/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 229.9ms
video 1/1 (frame 395/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 230.7ms
video 1/1 (frame 396/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 2 backpacks, 230.1ms
video 1/1 (frame 397/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 2 trucks, 3 backpacks, 224.5ms
video 1/1 (frame 398/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 trucks, 2 backpacks, 218.9ms
video 1/1 (frame 399/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 truck, 2 backpacks, 194.5ms
video 1/1 (frame 400/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 2 backpacks, 257.6ms
video 1/1 (frame 401/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 218.5ms
video 1/1 (frame 402/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 1 truck, 1 backpack, 208.9ms
video 1/1 (frame 403/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 10 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 219.5ms
video 1/1 (frame 404/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 225.4ms
video 1/1 (frame 405/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 231.2ms
video 1/1 (frame 406/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 207.0ms
video 1/1 (frame 407/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 226.0ms
video 1/1 (frame 408/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 226.8ms
video 1/1 (frame 409/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 1 truck, 1 backpack, 1 handbag, 217.5ms
video 1/1 (frame 410/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 246.9ms
video 1/1 (frame 411/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 229.4ms
video 1/1 (frame 412/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 truck, 2 backpacks, 1 handbag, 220.4ms
video 1/1 (frame 413/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 233.2ms
video 1/1 (frame 414/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 1 truck, 1 backpack, 1 handbag, 233.2ms
video 1/1 (frame 415/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 truck, 2 backpacks, 1 handbag, 262.8ms
video 1/1 (frame 416/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 2 backpacks, 252.2ms
video 1/1 (frame 417/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 2 backpacks, 260.5ms
video 1/1 (frame 418/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 1 suitcase, 232.9ms
video 1/1 (frame 419/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 truck, 3 backpacks, 212.5ms
video 1/1 (frame 420/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 3 backpacks, 225.3ms
video 1/1 (frame 421/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 truck, 3 backpacks, 233.1ms
video 1/1 (frame 422/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 truck, 2 backpacks, 415.0ms
video 1/1 (frame 423/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 219.9ms
video 1/1 (frame 424/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 2 trucks, 1 backpack, 230.4ms
video 1/1 (frame 425/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 2 trucks, 1 backpack, 208.4ms
video 1/1 (frame 426/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 truck, 1 backpack, 212.7ms
video 1/1 (frame 427/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 truck, 1 backpack, 201.4ms
video 1/1 (frame 428/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 7 cars, 2 trucks, 1 backpack, 207.7ms
video 1/1 (frame 429/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 11 persons, 8 cars, 2 trucks, 2 backpacks, 216.8ms
video 1/1 (frame 430/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 truck, 2 backpacks, 210.0ms
video 1/1 (frame 431/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 truck, 3 backpacks, 209.3ms
video 1/1 (frame 432/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 1 handbag, 231.7ms
video 1/1 (frame 433/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 245.1ms
video 1/1 (frame 434/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 1 backpack, 224.6ms
video 1/1 (frame 435/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 7 cars, 1 motorcycle, 196.0ms
video 1/1 (frame 436/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 215.6ms
video 1/1 (frame 437/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 1 motorcycle, 1 backpack, 239.0ms
video 1/1 (frame 438/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 truck, 1 backpack, 240.3ms
video 1/1 (frame 439/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 motorcycle, 1 backpack, 230.6ms
video 1/1 (frame 440/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 motorcycle, 1 backpack, 223.8ms
video 1/1 (frame 441/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 7 cars, 2 backpacks, 216.3ms
video 1/1 (frame 442/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 truck, 1 backpack, 215.3ms
video 1/1 (frame 443/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 motorcycle, 1 backpack, 1 suitcase, 212.5ms
video 1/1 (frame 444/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 1 suitcase, 243.9ms
video 1/1 (frame 445/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 1 suitcase, 225.7ms
video 1/1 (frame 446/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 223.5ms
video 1/1 (frame 447/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 225.4ms
video 1/1 (frame 448/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 233.5ms
video 1/1 (frame 449/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 228.2ms
video 1/1 (frame 450/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 219.8ms
video 1/1 (frame 451/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 215.1ms
video 1/1 (frame 452/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 212.6ms
video 1/1 (frame 453/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 230.6ms
video 1/1 (frame 454/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 223.9ms
video 1/1 (frame 455/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 1 backpack, 217.3ms
video 1/1 (frame 456/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 12 persons, 8 cars, 1 backpack, 215.7ms
video 1/1 (frame 457/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 1 truck, 2 backpacks, 212.0ms
video 1/1 (frame 458/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 212.5ms
video 1/1 (frame 459/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 218.6ms
video 1/1 (frame 460/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 237.8ms
video 1/1 (frame 461/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 2 backpacks, 233.1ms
video 1/1 (frame 462/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 truck, 2 backpacks, 262.1ms
video 1/1 (frame 463/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 truck, 2 backpacks, 212.8ms
video 1/1 (frame 464/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 1 backpack, 200.8ms
video 1/1 (frame 465/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 truck, 1 backpack, 195.4ms
video 1/1 (frame 466/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 2 backpacks, 213.9ms
video 1/1 (frame 467/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 truck, 2 backpacks, 209.2ms
video 1/1 (frame 468/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 2 backpacks, 383.4ms
video 1/1 (frame 469/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 11 cars, 1 truck, 1 backpack, 190.9ms
video 1/1 (frame 470/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 truck, 1 backpack, 221.0ms
video 1/1 (frame 471/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 11 cars, 1 backpack, 239.7ms
video 1/1 (frame 472/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 12 cars, 1 backpack, 219.2ms
video 1/1 (frame 473/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 228.1ms
video 1/1 (frame 474/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 252.6ms
video 1/1 (frame 475/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 backpack, 247.7ms
video 1/1 (frame 476/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 229.8ms
video 1/1 (frame 477/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 208.8ms
video 1/1 (frame 478/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 230.1ms
video 1/1 (frame 479/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 219.3ms
video 1/1 (frame 480/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 205.3ms
video 1/1 (frame 481/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 201.8ms
video 1/1 (frame 482/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 1 backpack, 225.5ms
video 1/1 (frame 483/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 241.5ms
video 1/1 (frame 484/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 224.2ms
video 1/1 (frame 485/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 200.3ms
video 1/1 (frame 486/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 222.0ms
video 1/1 (frame 487/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 222.6ms
video 1/1 (frame 488/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 220.0ms
video 1/1 (frame 489/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 239.3ms
video 1/1 (frame 490/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 220.1ms
video 1/1 (frame 491/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 229.5ms
video 1/1 (frame 492/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 backpack, 225.1ms
video 1/1 (frame 493/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 227.7ms
video 1/1 (frame 494/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 226.8ms
video 1/1 (frame 495/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 6 cars, 2 backpacks, 226.7ms
video 1/1 (frame 496/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 1 backpack, 219.9ms
video 1/1 (frame 497/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 209.9ms
video 1/1 (frame 498/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 208.4ms
video 1/1 (frame 499/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 205.6ms
video 1/1 (frame 500/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 207.5ms
video 1/1 (frame 501/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 216.9ms
video 1/1 (frame 502/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 220.5ms
video 1/1 (frame 503/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 backpack, 214.9ms
video 1/1 (frame 504/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 225.0ms
video 1/1 (frame 505/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 10 cars, 1 backpack, 256.1ms
video 1/1 (frame 506/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 242.1ms
video 1/1 (frame 507/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 backpack, 239.2ms
video 1/1 (frame 508/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 251.4ms
video 1/1 (frame 509/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 1 backpack, 227.3ms
video 1/1 (frame 510/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 1 backpack, 220.0ms
video 1/1 (frame 511/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 8 cars, 453.2ms
video 1/1 (frame 512/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 212.4ms
video 1/1 (frame 513/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 238.9ms
video 1/1 (frame 514/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 198.9ms
video 1/1 (frame 515/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 209.2ms
video 1/1 (frame 516/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 backpack, 256.1ms
video 1/1 (frame 517/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 backpack, 250.3ms
video 1/1 (frame 518/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 232.8ms
video 1/1 (frame 519/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 8 cars, 231.1ms
video 1/1 (frame 520/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 211.0ms
video 1/1 (frame 521/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 13 persons, 8 cars, 217.4ms
video 1/1 (frame 522/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 9 cars, 1 backpack, 202.6ms
video 1/1 (frame 523/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 8 cars, 1 backpack, 226.0ms
video 1/1 (frame 524/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 2 backpacks, 234.8ms
video 1/1 (frame 525/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 243.3ms
video 1/1 (frame 526/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 224.3ms
video 1/1 (frame 527/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 212.6ms
video 1/1 (frame 528/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 224.0ms
video 1/1 (frame 529/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 222.2ms
video 1/1 (frame 530/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 221.9ms
video 1/1 (frame 531/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 215.5ms
video 1/1 (frame 532/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 229.3ms
video 1/1 (frame 533/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 21 persons, 8 cars, 1 truck, 1 backpack, 224.0ms
video 1/1 (frame 534/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 9 cars, 1 backpack, 211.7ms
video 1/1 (frame 535/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 9 cars, 1 truck, 1 backpack, 211.2ms
video 1/1 (frame 536/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 1 truck, 226.1ms
video 1/1 (frame 537/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 234.7ms
video 1/1 (frame 538/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 9 cars, 236.4ms
video 1/1 (frame 539/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 20 persons, 8 cars, 226.6ms
video 1/1 (frame 540/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 8 cars, 223.0ms
video 1/1 (frame 541/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 239.7ms
video 1/1 (frame 542/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 225.0ms
video 1/1 (frame 543/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 220.4ms
video 1/1 (frame 544/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 213.3ms
video 1/1 (frame 545/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 218.3ms
video 1/1 (frame 546/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 219.5ms
video 1/1 (frame 547/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 211.5ms
video 1/1 (frame 548/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 21 persons, 6 cars, 235.7ms
video 1/1 (frame 549/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 229.7ms
video 1/1 (frame 550/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 482.8ms
video 1/1 (frame 551/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 265.8ms
video 1/1 (frame 552/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 247.1ms
video 1/1 (frame 553/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 229.0ms
video 1/1 (frame 554/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 191.4ms
video 1/1 (frame 555/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 5 cars, 217.0ms
video 1/1 (frame 556/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 5 cars, 210.8ms
video 1/1 (frame 557/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 7 cars, 215.0ms
video 1/1 (frame 558/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 213.3ms
video 1/1 (frame 559/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 197.6ms
video 1/1 (frame 560/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 212.6ms
video 1/1 (frame 561/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 234.6ms
video 1/1 (frame 562/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 212.9ms
video 1/1 (frame 563/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 6 cars, 209.2ms
video 1/1 (frame 564/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 208.1ms
video 1/1 (frame 565/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 221.3ms
video 1/1 (frame 566/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 5 cars, 206.5ms
video 1/1 (frame 567/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 213.5ms
video 1/1 (frame 568/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 5 cars, 205.7ms
video 1/1 (frame 569/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 197.3ms
video 1/1 (frame 570/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 214.9ms
video 1/1 (frame 571/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 248.6ms
video 1/1 (frame 572/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 5 cars, 226.5ms
video 1/1 (frame 573/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 246.6ms
video 1/1 (frame 574/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 229.3ms
video 1/1 (frame 575/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 230.6ms
video 1/1 (frame 576/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 216.6ms
video 1/1 (frame 577/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 218.3ms
video 1/1 (frame 578/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 222.0ms
video 1/1 (frame 579/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 205.3ms
video 1/1 (frame 580/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 215.2ms
video 1/1 (frame 581/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 207.2ms
video 1/1 (frame 582/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 1 handbag, 230.7ms
video 1/1 (frame 583/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 227.6ms
video 1/1 (frame 584/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 235.9ms
video 1/1 (frame 585/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 237.4ms
video 1/1 (frame 586/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 230.5ms
video 1/1 (frame 587/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 230.5ms
video 1/1 (frame 588/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 234.5ms
video 1/1 (frame 589/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 399.4ms
video 1/1 (frame 590/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 254.3ms
video 1/1 (frame 591/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 229.7ms
video 1/1 (frame 592/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 handbag, 216.3ms
video 1/1 (frame 593/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 1 handbag, 207.9ms
video 1/1 (frame 594/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 1 handbag, 225.8ms
video 1/1 (frame 595/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 handbag, 231.1ms
video 1/1 (frame 596/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 253.5ms
video 1/1 (frame 597/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 252.4ms
video 1/1 (frame 598/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 18 persons, 8 cars, 241.2ms
video 1/1 (frame 599/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 8 cars, 215.0ms
video 1/1 (frame 600/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 handbag, 221.1ms
video 1/1 (frame 601/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 6 cars, 216.6ms
video 1/1 (frame 602/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 225.2ms
video 1/1 (frame 603/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 230.2ms
video 1/1 (frame 604/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 227.9ms
video 1/1 (frame 605/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 235.8ms
video 1/1 (frame 606/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 226.4ms
video 1/1 (frame 607/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 5 cars, 229.8ms
video 1/1 (frame 608/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 227.8ms
video 1/1 (frame 609/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 7 cars, 213.9ms
video 1/1 (frame 610/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 212.9ms
video 1/1 (frame 611/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 260.9ms
video 1/1 (frame 612/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 210.5ms
video 1/1 (frame 613/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 267.6ms
video 1/1 (frame 614/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 274.7ms
video 1/1 (frame 615/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 301.2ms
video 1/1 (frame 616/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 7 cars, 1 handbag, 255.9ms
video 1/1 (frame 617/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 6 cars, 1 handbag, 209.0ms
video 1/1 (frame 618/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 6 cars, 219.1ms
video 1/1 (frame 619/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 238.2ms
video 1/1 (frame 620/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 232.6ms
video 1/1 (frame 621/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 221.9ms
video 1/1 (frame 622/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 11 cars, 207.5ms
video 1/1 (frame 623/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 229.0ms
video 1/1 (frame 624/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 13 cars, 1 truck, 232.3ms
video 1/1 (frame 625/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 truck, 389.5ms
video 1/1 (frame 626/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 14 persons, 10 cars, 1 truck, 239.2ms
video 1/1 (frame 627/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 216.8ms
video 1/1 (frame 628/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 10 cars, 1 truck, 217.5ms
video 1/1 (frame 629/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 11 cars, 1 truck, 216.1ms
video 1/1 (frame 630/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 230.5ms
video 1/1 (frame 631/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 truck, 215.8ms
video 1/1 (frame 632/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 1 handbag, 232.1ms
video 1/1 (frame 633/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 truck, 231.1ms
video 1/1 (frame 634/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 truck, 238.5ms
video 1/1 (frame 635/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 9 cars, 243.2ms
video 1/1 (frame 636/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 234.0ms
video 1/1 (frame 637/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 8 cars, 1 backpack, 212.3ms
video 1/1 (frame 638/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 9 cars, 1 backpack, 237.0ms
video 1/1 (frame 639/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 8 cars, 1 backpack, 240.2ms
video 1/1 (frame 640/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 10 cars, 1 backpack, 268.2ms
video 1/1 (frame 641/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 236.5ms
video 1/1 (frame 642/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 7 cars, 1 backpack, 224.4ms
video 1/1 (frame 643/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 1 backpack, 1 handbag, 205.8ms
video 1/1 (frame 644/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 19 persons, 10 cars, 1 backpack, 238.8ms
video 1/1 (frame 645/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 9 cars, 1 backpack, 1 handbag, 228.4ms
video 1/1 (frame 646/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 7 cars, 1 backpack, 216.8ms
video 1/1 (frame 647/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 17 persons, 9 cars, 1 backpack, 1 handbag, 212.9ms
video 1/1 (frame 648/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 16 persons, 7 cars, 1 handbag, 235.1ms
video 1/1 (frame 649/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 6 cars, 225.4ms
video 1/1 (frame 650/650) C:\Users\krna5\Downloads\30952-383991415_small.mp4: 384x640 15 persons, 5 cars, 243.1ms
Speed: 5.1ms preprocess, 227.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f6e7e9aa-0a82-42c0-b69c-eb0b5af7a21c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">The</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">processed</span> <span class="n">the</span> <span class="n">video</span><span class="p">,</span> <span class="n">frame</span> <span class="n">by</span> <span class="n">frame</span><span class="p">,</span> <span class="ow">and</span> <span class="n">detected</span> <span class="n">objects</span> <span class="n">like</span> <span class="n">persons</span> <span class="ow">and</span> <span class="n">cars</span><span class="o">.</span>

<span class="n">Summary</span> <span class="n">of</span> <span class="n">Results</span><span class="p">:</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">Frames</span> <span class="n">Processed</span><span class="p">:</span> <span class="mi">650</span>
<span class="n">Detections</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">Last</span> <span class="n">Frames</span><span class="p">:</span>
<span class="n">Frame</span> <span class="mi">649</span><span class="p">:</span> <span class="mi">15</span> <span class="n">persons</span><span class="p">,</span> <span class="mi">6</span> <span class="n">cars</span><span class="o">.</span>
<span class="n">Frame</span> <span class="mi">650</span><span class="p">:</span> <span class="mi">15</span> <span class="n">persons</span><span class="p">,</span> <span class="mi">5</span> <span class="n">cars</span><span class="o">.</span>
<span class="n">Performance</span> <span class="n">Metrics</span><span class="p">:</span>
<span class="n">Preprocessing</span> <span class="n">Time</span><span class="p">:</span> <span class="o">~</span><span class="mf">5.1</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">frame</span><span class="o">.</span>
<span class="n">Inference</span> <span class="n">Time</span><span class="p">:</span> <span class="o">~</span><span class="mf">227.4</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">frame</span><span class="o">.</span>
<span class="n">Postprocessing</span> <span class="n">Time</span><span class="p">:</span> <span class="o">~</span><span class="mf">3.5</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">frame</span><span class="o">.</span>
<span class="n">Image</span> <span class="n">Resolution</span><span class="p">:</span> <span class="mi">384</span><span class="n">x640</span><span class="o">.</span>

<span class="c1"># Performance Analysis:</span>

<span class="c1"># Object Detection Accuracy:</span>

<span class="n">The</span> <span class="n">model</span> <span class="n">detected</span> <span class="mi">15</span> <span class="n">persons</span> <span class="n">consistently</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">last</span> <span class="n">frames</span><span class="o">.</span> <span class="n">This</span> <span class="n">suggests</span> <span class="n">good</span> <span class="n">recall</span> <span class="k">for</span> <span class="n">the</span>
<span class="s2">"person"</span> <span class="n">class</span><span class="o">.</span>
<span class="n">The</span> <span class="n">detection</span> <span class="n">of</span> <span class="n">cars</span> <span class="n">varies</span> <span class="n">slightly</span> <span class="p">(</span><span class="mi">6</span> <span class="ow">in</span> <span class="n">frame</span> <span class="mi">649</span> <span class="ow">and</span> <span class="mi">5</span> <span class="ow">in</span> <span class="n">frame</span> <span class="mi">650</span><span class="p">)</span><span class="o">.</span> <span class="n">This</span> <span class="n">could</span> <span class="n">be</span> <span class="n">due</span> <span class="n">to</span>
<span class="n">occlusions</span><span class="p">,</span> <span class="n">movement</span><span class="p">,</span> <span class="ow">or</span> <span class="n">other</span> <span class="n">factors</span> <span class="n">affecting</span> <span class="n">visibility</span> <span class="ow">in</span> <span class="n">those</span> <span class="n">frames</span><span class="o">.</span>
                                                                                        
<span class="c1"># Speed of Inference:</span>

<span class="n">The</span> <span class="n">total</span> <span class="n">time</span> <span class="k">for</span> <span class="n">inference</span> <span class="p">(</span><span class="o">~</span><span class="mi">227</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">frame</span><span class="p">)</span> <span class="n">indicates</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">operating</span> <span class="n">efficiently</span> <span class="k">for</span>
<span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="ow">or</span> <span class="n">near</span><span class="o">-</span><span class="n">real</span><span class="o">-</span><span class="n">time</span> <span class="n">scenarios</span><span class="p">,</span> <span class="n">though</span> <span class="n">optimization</span> <span class="n">might</span> <span class="n">be</span> <span class="n">required</span> <span class="k">for</span> <span class="n">high</span> <span class="n">frame</span><span class="o">-</span><span class="n">rate</span> <span class="n">videos</span><span class="o">.</span>
                                                                   
<span class="c1"># Model's Object Identification:</span>

<span class="n">It</span> <span class="n">identified</span> <span class="n">objects</span> <span class="n">such</span> <span class="k">as</span> <span class="n">persons</span> <span class="ow">and</span> <span class="n">cars</span><span class="p">,</span> <span class="n">which</span> <span class="n">are</span> <span class="n">expected</span> <span class="n">classes</span> <span class="k">for</span> <span class="n">urban</span> <span class="ow">or</span> <span class="n">public</span> <span class="n">scenes</span><span class="o">.</span>
                                              
<span class="c1"># Dataset Challenges:</span>

<span class="n">Variations</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">cars</span> <span class="n">between</span> <span class="n">frames</span> <span class="n">indicate</span> <span class="n">some</span> <span class="n">inconsistencies</span> <span class="ow">in</span> <span class="n">detection</span><span class="p">,</span> 
<span class="n">which</span> <span class="n">might</span> <span class="n">be</span> <span class="n">due</span> <span class="n">to</span><span class="p">:</span>
                                              
<span class="n">Lighting</span> <span class="n">Conditions</span><span class="p">:</span> <span class="n">Shadows</span> <span class="ow">or</span> <span class="n">bright</span> <span class="n">spots</span> <span class="n">may</span> <span class="n">affect</span> <span class="nb">object</span> <span class="n">visibility</span><span class="o">.</span>
<span class="n">Crowding</span> <span class="ow">or</span> <span class="n">Occlusion</span><span class="p">:</span> <span class="n">Overlapping</span> <span class="n">objects</span> <span class="n">may</span> <span class="n">confuse</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span>
<span class="n">Motion</span> <span class="n">Blur</span><span class="p">:</span> <span class="n">Fast</span><span class="o">-</span><span class="n">moving</span> <span class="n">objects</span> <span class="n">can</span> <span class="n">sometimes</span> <span class="n">be</span> <span class="n">missed</span><span class="o">.</span>
<span class="n">Suggestions</span> <span class="k">for</span> <span class="n">Improvement</span><span class="p">:</span>
<span class="n">Optimize</span> <span class="n">Model</span> <span class="k">for</span> <span class="n">Specific</span> <span class="n">Needs</span><span class="p">:</span>

<span class="n">If</span> <span class="n">the</span> <span class="n">focus</span> <span class="ow">is</span> <span class="n">on</span> <span class="n">a</span> <span class="n">specific</span> <span class="k">class</span> <span class="err">(</span><span class="nc">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">cars</span><span class="p">),</span> <span class="n">consider</span> <span class="n">fine</span><span class="o">-</span><span class="n">tuning</span> <span class="n">the</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">on</span> <span class="n">a</span> <span class="n">dataset</span> 
<span class="k">with</span> <span class="n">more</span> <span class="n">diverse</span> <span class="ow">and</span> <span class="n">annotated</span> <span class="n">examples</span> <span class="n">of</span> <span class="n">that</span> <span class="n">class</span><span class="o">.</span>
                                              
<span class="c1"># Postprocessing Enhancements:</span>

<span class="n">Apply</span> <span class="n">tracking</span> <span class="n">algorithms</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">SORT</span><span class="p">,</span> <span class="n">DeepSORT</span><span class="p">)</span> <span class="n">to</span> <span class="n">maintain</span> <span class="nb">object</span> <span class="n">continuity</span> <span class="n">across</span> <span class="n">frames</span><span class="o">.</span> 
<span class="n">This</span> <span class="n">reduces</span> <span class="n">false</span> <span class="n">negatives</span> <span class="ow">or</span> <span class="n">fluctuations</span> <span class="ow">in</span> <span class="nb">object</span> <span class="n">counts</span> <span class="n">between</span> <span class="n">frames</span><span class="o">.</span>
                                              
<span class="c1"># Quantitative Evaluation:</span>

<span class="n">Evaluate</span> <span class="n">Precision</span><span class="p">,</span> <span class="n">Recall</span><span class="p">,</span> <span class="ow">and</span> <span class="n">mAP</span> <span class="k">for</span> <span class="n">the</span> <span class="n">detected</span> <span class="n">classes</span> <span class="n">using</span> <span class="n">ground</span> <span class="n">truth</span> <span class="n">annotations</span> <span class="n">to</span>
<span class="n">understand</span> <span class="n">the</span> <span class="n">model</span><span class="err"></span><span class="n">s</span> <span class="n">performance</span> <span class="n">more</span> <span class="n">rigorously</span><span class="o">.</span>
                                              
<span class="c1"># Hardware Optimization:</span>

<span class="n">Use</span> <span class="n">a</span> <span class="n">GPU</span> <span class="n">to</span> <span class="n">reduce</span> <span class="n">inference</span> <span class="n">time</span> <span class="ow">and</span> <span class="n">handle</span> <span class="n">higher</span><span class="o">-</span><span class="n">resolution</span> <span class="n">videos</span><span class="o">.</span>
    
<span class="c1"># Robustness to Scene Variations:</span>

<span class="n">Augment</span> <span class="n">the</span> <span class="n">training</span> <span class="n">dataset</span> <span class="k">with</span> <span class="n">examples</span> <span class="n">of</span> <span class="n">crowded</span> <span class="n">scenes</span><span class="p">,</span> <span class="n">varying</span> <span class="n">lighting</span><span class="p">,</span> <span class="ow">and</span> <span class="n">motion</span> <span class="n">blur</span>
<span class="n">to</span> <span class="n">improve</span> <span class="n">model</span> <span class="n">generalization</span><span class="o">.</span>
    
<span class="c1"># Visual Feedback:</span>

<span class="n">Inspect</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="n">to</span> <span class="n">visually</span> <span class="n">verify</span> <span class="n">whether</span> <span class="n">objects</span> <span class="n">like</span> <span class="n">persons</span> <span class="ow">and</span> <span class="n">cars</span> <span class="n">are</span> <span class="n">correctly</span>
<span class="n">localized</span> <span class="ow">and</span> <span class="n">labeled</span><span class="o">.</span>
<span class="c1"># Next Steps:</span>
<span class="n">Visualize</span> <span class="n">Results</span><span class="p">:</span> <span class="n">Confirm</span> <span class="n">the</span> <span class="n">detections</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">annotated</span> <span class="n">video</span> <span class="n">to</span> <span class="n">identify</span> <span class="nb">any</span> <span class="n">false</span> <span class="n">positives</span> 
<span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="n">non</span><span class="o">-</span><span class="n">objects</span> <span class="n">labeled</span> <span class="k">as</span> <span class="n">objects</span><span class="p">)</span> <span class="ow">or</span> <span class="n">false</span> <span class="n">negatives</span> <span class="p">(</span><span class="n">like</span><span class="p">,</span> <span class="n">missed</span> <span class="n">objects</span><span class="p">)</span><span class="o">.</span>

<span class="n">Quantitative</span> <span class="n">Analysis</span><span class="p">:</span>
<span class="n">Implement</span> <span class="n">metrics</span> <span class="n">like</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="ow">and</span> <span class="n">mAP</span> <span class="n">to</span> <span class="n">quantitatively</span> <span class="n">evaluate</span> <span class="n">detection</span> <span class="n">performance</span><span class="o">.</span>

<span class="n">Enhance</span> <span class="n">Model</span><span class="p">:</span> 
<span class="n">If</span> <span class="n">the</span> <span class="n">model</span> <span class="n">underperforms</span> <span class="ow">in</span> <span class="n">specific</span> <span class="n">areas</span> <span class="p">(</span><span class="n">such</span> <span class="k">as</span><span class="p">,</span> <span class="n">detecting</span> <span class="n">small</span> <span class="n">objects</span> <span class="ow">or</span> <span class="n">under</span> <span class="n">occlusion</span><span class="p">),</span> 
<span class="n">consider</span> <span class="n">training</span> <span class="k">with</span> <span class="n">additional</span> <span class="n">relevant</span> <span class="n">data</span> <span class="ow">or</span> <span class="n">using</span> <span class="n">a</span> <span class="n">larger</span> <span class="n">YOLO</span> <span class="n">model</span> <span class="n">like</span> <span class="n">YOLOv8</span><span class="o">-</span><span class="n">L</span> <span class="ow">or</span> <span class="n">YOLOv8</span><span class="o">-</span><span class="n">X</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=a953642c-b797-45ff-b864-8c8b20459d5f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Fine-tune the Model:</span>
<span class="n">Train</span> <span class="n">the</span> <span class="n">model</span> <span class="n">on</span> <span class="n">your</span> <span class="n">dataset</span> <span class="n">using</span> <span class="n">a</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">YOLOv8</span> <span class="n">checkpoint</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=cf60b88d-ca67-4777-bba2-1e7ccdaea8bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">yaml_content</span> <span class="o">=</span> <span class="s2">"""</span>
<span class="s2"># Root directory containing all dataset splits</span>
<span class="s2">path: C:/Users/krna5/Downloads/processed_frames</span>

<span class="s2"># Subfolders for each dataset split</span>
<span class="s2">train: train</span>
<span class="s2">val: val</span>
<span class="s2">test: test</span>

<span class="s2"># Class names (modify based on your dataset)</span>
<span class="s2">names:</span>
<span class="s2">  0: person</span>
<span class="s2">  1: car</span>
<span class="s2">  2: truck</span>
<span class="s2">  3: backpack</span>
<span class="s2">  4: bottle</span>
<span class="s2">"""</span>

<span class="c1"># Save YAML content to a file</span>
<span class="n">yaml_file_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\dataset.yaml"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">yaml_file_path</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">yaml_content</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"YAML file saved at: </span><span class="si">{</span><span class="n">yaml_file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>YAML file saved at: C:\Users\krna5\Downloads\dataset.yaml
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=5c3fe1e0-9e37-4579-b8a0-fd9490407da2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Run YOLO Training Pass the YAML file path when initializing the YOLO model or calling the training function.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f3f175ba-7108-4814-88ed-c41855055260">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="c1"># Path to the dataset YAML file</span>
<span class="n">dataset_yaml</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\dataset.yaml"</span>

<span class="c1"># Load a pretrained YOLOv8 model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">'yolov8n.pt'</span><span class="p">)</span>  <span class="c1"># 'yolov8n.pt' is a small model, suitable for CPU</span>

<span class="c1"># Train the model with reduced parameters for CPU training</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">dataset_yaml</span><span class="p">,</span>       <span class="c1"># Path to the dataset.yaml</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>                <span class="c1"># Use fewer epochs to start with</span>
    <span class="n">batch</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>                 <span class="c1"># Smaller batch size to reduce memory load</span>
    <span class="n">imgsz</span><span class="o">=</span><span class="mi">320</span><span class="p">,</span>               <span class="c1"># Smaller image size for faster processing on CPU</span>
    <span class="n">workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>               <span class="c1"># Reduce the number of workers to avoid overloading the CPU</span>
    <span class="n">save</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>              <span class="c1"># Don't save intermediate results to conserve memory</span>
    <span class="n">amp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                <span class="c1"># Enable Automatic Mixed Precision to reduce memory usage</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>             <span class="c1"># Enable verbose mode for better progress tracking</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>New https://pypi.org/project/ultralytics/8.3.48 available  Update with 'pip install -U ultralytics'
Ultralytics 8.3.33  Python-3.12.7 torch-2.5.1+cpu CPU (13th Gen Intel Core(TM) i7-1355U)
<span class="ansi-blue-intense-fg ansi-bold">engine\trainer: </span>task=detect, mode=train, model=yolov8n.pt, data=C:\Users\krna5\Downloads\dataset.yaml, epochs=3, time=None, patience=100, batch=4, imgsz=320, save=False, save_period=-1, cache=False, device=None, workers=2, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\detect\train7
Overriding model.yaml nc=80 with nc=5

                   from  n    params  module                                       arguments                     
  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 
  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                
  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             
  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                
  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             
  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               
  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           
  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              
  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           
  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          
 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          
 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  
 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                
 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 
 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              
 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 
 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           
Model summary: 225 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs

Transferred 319/355 items from pretrained weights
<span class="ansi-blue-intense-fg ansi-bold">TensorBoard: </span>Start with 'tensorboard --logdir runs\detect\train7', view at http://localhost:6006/
Freezing layer 'model.22.dfl.conv.weight'
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre><span class="ansi-blue-intense-fg ansi-bold">train: </span>Scanning C:\Users\krna5\Downloads\processed_frames\train.cache... 0 images, 455 backgrounds, 0 corrupt: 100%|</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>WARNING  No labels found in C:\Users\krna5\Downloads\processed_frames\train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-blue-intense-fg ansi-bold">albumentations: </span>Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre><span class="ansi-blue-intense-fg ansi-bold">val: </span>Scanning C:\Users\krna5\Downloads\processed_frames\val.cache... 0 images, 130 backgrounds, 0 corrupt: 100%|</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>WARNING  No labels found in C:\Users\krna5\Downloads\processed_frames\val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Plotting labels to runs\detect\train7\labels.jpg... 
zero-size array to reduction operation maximum which has no identity
<span class="ansi-blue-intense-fg ansi-bold">optimizer:</span> 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... 
<span class="ansi-blue-intense-fg ansi-bold">optimizer:</span> AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)
<span class="ansi-blue-intense-fg ansi-bold">TensorBoard: </span>model graph visualization added 
Image sizes 320 train, 320 val
Using 0 dataloader workers
Logging results to <span class="ansi-bold">runs\detect\train7</span>
Starting training for 3 epochs...

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>  0%|          | 0/114 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=3118a144-2a71-4cc4-95d8-c11193402522">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Evaluate Model Using mAP and IoU</span>
<span class="c1">#  mAP (Mean Average Precision)</span>
<span class="n">mAP</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">widely</span> <span class="n">used</span> <span class="n">metric</span> <span class="k">for</span> <span class="n">evaluating</span> <span class="nb">object</span> <span class="n">detection</span> <span class="n">models</span><span class="o">.</span> <span class="n">YOLOv8</span> <span class="n">provides</span> <span class="n">built</span><span class="o">-</span><span class="ow">in</span> 
<span class="n">functionality</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">mAP</span> <span class="n">during</span> <span class="n">validation</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=24a0fa00-cb44-42d3-8607-942bcaa47029">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="c1"># Load the trained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolov8n.pt"</span><span class="p">)</span> 
<span class="n">dataset_yaml</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"C:\Users\krna5\Downloads\dataset.yaml"</span>
<span class="c1"># Evaluate the model</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dataset_yaml</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"val"</span><span class="p">)</span>  <span class="c1"># Using the validation dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>  <span class="c1"># Outputs evaluation metrics, including mAP and IoU</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Ultralytics 8.3.33  Python-3.12.7 torch-2.5.1+cpu CPU (13th Gen Intel Core(TM) i7-1355U)
YOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre><span class="ansi-blue-intense-fg ansi-bold">val: </span>Scanning C:\Users\krna5\Downloads\processed_frames\val.cache... 0 images, 130 backgrounds, 0 corrupt: 100%|</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>WARNING  No labels found in C:\Users\krna5\Downloads\processed_frames\val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 9/9 [00:21&lt;0</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>                   all        130          0          0          0          0          0
WARNING  no labels found in detect set, can not compute metrics without labels
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
